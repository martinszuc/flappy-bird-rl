{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuZQ28L7aul2"
      },
      "source": [
        "### MPDA - Flappy Bird Reinforcement Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4sWOZ6Ui-Xw"
      },
      "source": [
        "### PROGRESS:\n",
        "[Date] [Name]: [What was done]\n",
        "\n",
        "[30.11.] [mszuc]: [Team work templates, Define tasks for project]\n",
        "\n",
        "[1.12.] [xsocha02]: [Added random baseline agent + video/GIF]\n",
        "\n",
        "\n",
        "### BLOCKERS:\n",
        "None yet\n",
        "\n",
        "### DECISIONS:\n",
        "- Using flappy-bird-gymnasium package for environment\n",
        "- Starting with simple feature space (12 values)\n",
        "- PyTorch for neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ngo3x4cpa1xX",
        "outputId": "2807bf81-ab60-4afb-e682-1f01636db49b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flappy-bird-gymnasium\n",
            "  Downloading flappy_bird_gymnasium-0.4.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (2.37.2)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (from flappy-bird-gymnasium) (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from flappy-bird-gymnasium) (2.0.2)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.12/dist-packages (from flappy-bird-gymnasium) (2.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->flappy-bird-gymnasium) (3.1.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium->flappy-bird-gymnasium) (0.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading flappy_bird_gymnasium-0.4.0-py3-none-any.whl (37.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: flappy-bird-gymnasium\n",
            "Successfully installed flappy-bird-gymnasium-0.4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:294: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:367: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  rotation_lines = [l for l in lines if 'rotate          :' in l and re.search('\\d+$', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:370: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  match = re.search('\\d+$', rotation_line)\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 0: SETUP & INSTALLATION\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "!pip install flappy-bird-gymnasium torch torchvision matplotlib imageio\n",
        "\n",
        "import gymnasium as gym\n",
        "import flappy_bird_gymnasium\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import random\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import imageio\n",
        "from moviepy.video.io.VideoFileClip import VideoFileClip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkKS4C_L-qUU",
        "outputId": "3fd49761-ff8d-40bb-d502-302d2388138f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Environment loaded!\n",
            "Observation space: Box(0.0, 1.0, (180,), float64)\n",
            "Action space: Discrete(2)\n"
          ]
        }
      ],
      "source": [
        "# Test environment loads\n",
        "env = gym.make('FlappyBird-v0', render_mode='rgb_array', use_lidar=True)\n",
        "print(f\"✅ Environment loaded!\")\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "print(f\"Action space: {env.action_space}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "collapsed": true,
        "id": "zNg5Rd4p9Kla",
        "outputId": "7ca2ad4c-2735-4398-f887-ab3d794ae9fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-d6b10766-8638-4847-9366-26d3c87c3c7c\" class=\"ndarray_repr\"><pre>ndarray (512, 288, 3) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,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\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-d6b10766-8638-4847-9366-26d3c87c3c7c button').onclick = (e) => {\n",
              "        document.querySelector('#id-d6b10766-8638-4847-9366-26d3c87c3c7c').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-d6b10766-8638-4847-9366-26d3c87c3c7c button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Render test\n",
        "env.reset()\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "collapsed": true,
        "id": "FL-jlTqCUPUl",
        "outputId": "cf92e94d-3247-49c0-9da8-570493fa8d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 84,  56,  71],\n",
              "        [ 84,  56,  71]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 84,  56,  71],\n",
              "        [ 84,  56,  71]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 84,  56,  71],\n",
              "        [ 84,  56,  71]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-2a89bf51-fc0e-4bbf-9ad7-21c60ea175c2\" class=\"ndarray_repr\"><pre>ndarray (512, 288, 3) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,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\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 84,  56,  71],\n",
              "        [ 84,  56,  71]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 84,  56,  71],\n",
              "        [ 84,  56,  71]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 84,  56,  71],\n",
              "        [ 84,  56,  71]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-2a89bf51-fc0e-4bbf-9ad7-21c60ea175c2 button').onclick = (e) => {\n",
              "        document.querySelector('#id-2a89bf51-fc0e-4bbf-9ad7-21c60ea175c2').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-2a89bf51-fc0e-4bbf-9ad7-21c60ea175c2 button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "action = 0\n",
        "env.step(action)\n",
        "\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "collapsed": true,
        "id": "qmbVKtMpUSZM",
        "outputId": "7036a968-f3db-4202-ddbb-e88dd6dc2796"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [132, 169,  68],\n",
              "        [148, 183,  81],\n",
              "        [148, 183,  81]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [132, 169,  68],\n",
              "        [148, 183,  81],\n",
              "        [148, 183,  81]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [132, 170,  69],\n",
              "        [148, 183,  81],\n",
              "        [148, 183,  81]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-16d0fb22-53f1-4341-9b69-11a90af94a72\" class=\"ndarray_repr\"><pre>ndarray (512, 288, 3) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,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\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [132, 169,  68],\n",
              "        [148, 183,  81],\n",
              "        [148, 183,  81]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [132, 169,  68],\n",
              "        [148, 183,  81],\n",
              "        [148, 183,  81]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [132, 170,  69],\n",
              "        [148, 183,  81],\n",
              "        [148, 183,  81]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-16d0fb22-53f1-4341-9b69-11a90af94a72 button').onclick = (e) => {\n",
              "        document.querySelector('#id-16d0fb22-53f1-4341-9b69-11a90af94a72').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-16d0fb22-53f1-4341-9b69-11a90af94a72 button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "action = 1\n",
        "env.step(action)\n",
        "\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bEXl_IsAa2Jw"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dVfjqgGjaP4P"
      },
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 1: HYPERPARAMETERS\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "GAMMA = 0.99\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_END = 0.01\n",
        "EPSILON_DECAY = 0.995\n",
        "BATCH_SIZE = 64\n",
        "MEMORY_SIZE = 10000\n",
        "TARGET_UPDATE = 100\n",
        "NUM_EPISODES = 5000\n",
        "MODEL_PATH = 'flappy_bird_dqn_v1.pth' # Must match the save path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKw9LqZ9bJM6",
        "outputId": "cd406447-9331-4a46-f1da-5530824a37db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation shape: (180,)\n",
            "Info: {'score': 0}\n",
            "Last 10 values: [0.87077653 0.86325566 0.85355176 0.84422975 0.83749654 0.83099074\n",
            " 0.82471771 0.81868282 0.81289137 0.80734859]\n",
            "After action: [0.92584822 0.91578435 0.90606844 0.8967118  0.88772578 0.88123633\n",
            " 0.87496653 0.86892114 0.86310487 0.85752239]\n",
            "After action: [0.97139963 0.96137938 0.95169132 0.9423457  0.93335279 0.92684578\n",
            " 0.91849489 0.91248114 0.90668591 0.90111343]\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 2: ENVIRONMENT EXPLORATION\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "def explore_environment():\n",
        "    \"\"\"\n",
        "    Understand state/action space of Flappy Bird.\n",
        "    Prints shapes, runs random episode.\n",
        "    \"\"\"\n",
        "    # TODO: Implement exploration\n",
        "    observation, info = env.reset()\n",
        "    print(f\"Observation shape: {observation.shape}\")\n",
        "    print(f\"Info: {info}\")\n",
        "\n",
        "    print(f\"Last 10 values: {observation[-10:]}\")\n",
        "\n",
        "    # jump\n",
        "    action = 1\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    # bird jumped up => lower values increased\n",
        "    print(f\"After action: {observation[-10:]}\")\n",
        "\n",
        "    # don't jump\n",
        "    action = 0\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    # still rising\n",
        "    print(f\"After action: {observation[-10:]}\")\n",
        "    action = 0\n",
        "\n",
        "env = gym.make('FlappyBird-v0', render_mode='rgb_array', use_lidar=True)\n",
        "explore_environment()\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sp1Iss8rbNNd",
        "outputId": "eb6e50c6-06a1-4e99-97b4-3b91ce62e46a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average score: -7.619999999999999\n",
            "Average steps: 50.0\n",
            "Videos saved in /content/random_agent_video/\n",
            "MoviePy - Building file  random_agent.gif\n",
            "MoviePy - - Generating GIF frames.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - - File ready: random_agent.gif.\n",
            "GIF saved in /content/random_agent.gif\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 3: RANDOM BASELINE AGENT\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "def random_agent_baseline(env, num_episodes=10):\n",
        "    \"\"\"\n",
        "    Random agent for baseline performance.\n",
        "\n",
        "    Returns:\n",
        "        avg_score: Average score over episodes\n",
        "        avg_steps: Average survival time\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    steps = []\n",
        "    for episode in range(num_episodes):\n",
        "        observation, info = env.reset()\n",
        "        done = False\n",
        "        score = 0\n",
        "        steps_taken = 0\n",
        "        while not done:\n",
        "            action = env.action_space.sample()\n",
        "            observation, reward, terminated, truncated, info = env.step(action)\n",
        "            score += reward\n",
        "            done = terminated or truncated\n",
        "            steps_taken += 1\n",
        "        scores.append(score)\n",
        "        steps.append(steps_taken)\n",
        "\n",
        "    avg_score = np.mean(scores)\n",
        "    avg_steps = np.mean(steps)\n",
        "\n",
        "    return avg_score, avg_steps\n",
        "\n",
        "env = gym.make('FlappyBird-v0', render_mode='rgb_array', use_lidar=True)\n",
        "env = RecordVideo(env, 'random_agent_video', episode_trigger=lambda x: True)\n",
        "avg_score, avg_steps = random_agent_baseline(env)\n",
        "env.close()\n",
        "\n",
        "print(f\"Average score: {avg_score}\")\n",
        "print(f\"Average steps: {avg_steps}\")\n",
        "print(\"Videos saved in /content/random_agent_video/\")\n",
        "\n",
        "# convert to gif\n",
        "video = VideoFileClip(\"random_agent_video/rl-video-episode-0.mp4\")\n",
        "video.write_gif(\"random_agent.gif\", fps=30, program='ffmpeg')\n",
        "print(\"GIF saved in /content/random_agent.gif\")\n",
        "\n",
        "# When randomly choosing actions, bird almost always flies up to the top of the screen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3hgeel4bNPf",
        "outputId": "4a46ccd9-efa4-4c6a-f70a-8ab0296fe32f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State dim: 180\n",
            "Action dim: 2\n",
            "Q-values: tensor([[-0.0400,  0.0230]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 4: DQN NETWORK\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        \"\"\"\n",
        "        Deep Q-Network.\n",
        "\n",
        "        Args:\n",
        "            state_dim: Dimension of state space (12 for features, 180 for LIDAR)\n",
        "            action_dim: Number of actions (2: nothing/flap)\n",
        "        \"\"\"\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        # 2–3 fully connected layers with ReLU\n",
        "        hidden1 = 128\n",
        "        hidden2 = 128\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden1, hidden2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden2, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: State tensor [batch_size, state_dim]\n",
        "\n",
        "        Returns:\n",
        "            Q-values: [batch_size, action_dim]\n",
        "        \"\"\"\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Model Save / Load\n",
        "# -----------------------------------------------------------pabad\n",
        "\n",
        "def save_dqn(model, path):\n",
        "    \"\"\"\n",
        "    Save DQN model weights to file.\n",
        "    \"\"\"\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "\n",
        "def load_dqn(path, state_dim, action_dim, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Load DQN model weights from file.\n",
        "\n",
        "    Returns:\n",
        "        model: DQN instance with loaded weights (in eval mode)\n",
        "    \"\"\"\n",
        "    model = DQN(state_dim, action_dim).to(device)\n",
        "    state_dict = torch.load(path, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "#test\n",
        "env = gym.make('FlappyBird-v0', use_lidar=True)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "model = DQN(state_dim, action_dim)\n",
        "test_state = torch.randn(1, state_dim)   # fake input\n",
        "q_values = model(test_state)\n",
        "\n",
        "print(\"State dim:\", state_dim)\n",
        "print(\"Action dim:\", action_dim)\n",
        "print(\"Q-values:\", q_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhL0sLWlbNRb",
        "outputId": "d096ad8a-b5a7-40e0-fba3-0254363c4d09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buffer size: 2\n",
            "Sampled states: [[4 5 6]\n",
            " [1 2 3]]\n",
            "Sampled actions: [0 1]\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 5: EXPERIENCE REPLAY BUFFER\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        \"\"\"\n",
        "        Experience replay buffer.\n",
        "\n",
        "        Args:\n",
        "            capacity: Maximum number of experiences to store\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "        self.buffer = deque(maxlen=capacity)   # auto-removes old items\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Store one experience into the buffer.\"\"\"\n",
        "        experience = (state, action, reward, next_state, done)\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample a random batch of experiences.\"\"\"\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "\n",
        "        # Unzip the batch\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        # Convert to NumPy arrays (PyTorch will convert later)\n",
        "        return (\n",
        "            np.array(states),\n",
        "            np.array(actions),\n",
        "            np.array(rewards, dtype=np.float32),\n",
        "            np.array(next_states),\n",
        "            np.array(dones, dtype=np.float32),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return current size of the buffer.\"\"\"\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "#test\n",
        "buf = ReplayBuffer(100)\n",
        "\n",
        "buf.push([1,2,3], 1, 0.5, [1,2,4], False)\n",
        "buf.push([4,5,6], 0, 1.0, [4,5,7], True)\n",
        "\n",
        "print(\"Buffer size:\", len(buf))\n",
        "batch = buf.sample(2)\n",
        "\n",
        "print(\"Sampled states:\", batch[0])\n",
        "print(\"Sampled actions:\", batch[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyWpBiJzbVv4",
        "outputId": "7c69dad2-0b66-49ab-d03d-9aef206700f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled action: 0\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 6: DQN AGENT\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, device=None):\n",
        "        \"\"\"\n",
        "        DQN Agent with target network.\n",
        "\n",
        "        Args:\n",
        "            state_dim: State space dimension\n",
        "            action_dim: Action space dimension\n",
        "        \"\"\"\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "        # Device (CPU / GPU)\n",
        "        if device is None:\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.device = device\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.gamma = 0.99           # discount factor\n",
        "        self.batch_size = 64\n",
        "        self.learning_rate = 1e-3\n",
        "        self.replay_capacity = 50_000\n",
        "        self.min_buffer_size = 1_000  # start training only after this many transitions\n",
        "\n",
        "        # Networks\n",
        "        self.policy_net = DQN(state_dim, action_dim).to(self.device)\n",
        "        self.target_net = DQN(state_dim, action_dim).to(self.device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()  # target net in eval mode\n",
        "\n",
        "        # Optimizer & loss\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "        # Replay buffer\n",
        "        self.replay_buffer = ReplayBuffer(self.replay_capacity)\n",
        "\n",
        "    def select_action(self, state, epsilon):\n",
        "        \"\"\"\n",
        "        Epsilon-greedy action selection.\n",
        "\n",
        "        Args:\n",
        "            state: Current state (np.array shape [state_dim])\n",
        "            epsilon: Exploration probability\n",
        "\n",
        "        Returns:\n",
        "            action: 0 or 1\n",
        "        \"\"\"\n",
        "        # Explore\n",
        "        if random.random() < epsilon:\n",
        "            return random.randrange(self.action_dim)\n",
        "\n",
        "        # Exploit\n",
        "        state_tensor = torch.tensor(\n",
        "            state, dtype=torch.float32, device=self.device\n",
        "        ).unsqueeze(0)  # [1, state_dim]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_values = self.policy_net(state_tensor)  # [1, action_dim]\n",
        "            action = q_values.argmax(dim=1).item()\n",
        "\n",
        "        return action\n",
        "\n",
        "    def train_step(self):\n",
        "        \"\"\"\n",
        "        Sample batch and update network.\n",
        "\n",
        "        Returns:\n",
        "            loss_value: float or None (if not enough data)\n",
        "        \"\"\"\n",
        "        # Don't train until buffer has enough samples\n",
        "        if len(self.replay_buffer) < max(self.batch_size, self.min_buffer_size):\n",
        "            return None\n",
        "\n",
        "        # Sample batch from replay buffer\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "        # Convert to tensors\n",
        "        states      = torch.tensor(states, dtype=torch.float32, device=self.device)\n",
        "        actions     = torch.tensor(actions, dtype=torch.long,   device=self.device).unsqueeze(1)   # [B,1]\n",
        "        rewards     = torch.tensor(rewards, dtype=torch.float32, device=self.device).unsqueeze(1) # [B,1]\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float32, device=self.device)\n",
        "        dones       = torch.tensor(dones, dtype=torch.float32,  device=self.device).unsqueeze(1)  # [B,1]\n",
        "\n",
        "        # Current Q-values from policy network for taken actions\n",
        "        q_values = self.policy_net(states).gather(1, actions)  # [B,1]\n",
        "\n",
        "        # Target Q-values using target network\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_net(next_states).max(dim=1, keepdim=True)[0]  # [B,1]\n",
        "            target_q_values = rewards + self.gamma * (1.0 - dones) * next_q_values\n",
        "\n",
        "        # Compute loss\n",
        "        loss = self.loss_fn(q_values, target_q_values)\n",
        "\n",
        "        # Backpropagation\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # (Optional) gradient clipping:\n",
        "        # torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"Copy weights from policy to target network.\"\"\"\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "env = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "agent = DQNAgent(state_dim, action_dim)\n",
        "\n",
        "state, _ = env.reset()\n",
        "action = agent.select_action(state, epsilon=0.5)\n",
        "print(\"Sampled action:\", action)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "e2waCE8JgvU9"
      },
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 8: HELPER FUNCTIONS\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "def plot_training_results(rewards, losses):\n",
        "    \"\"\"Plot reward curves and training losses.\"\"\"\n",
        "    # TODO: Implement plotting\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(rewards)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.title('Training Rewards')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Losses')\n",
        "    plt.show()\n",
        "\n",
        "def save_model(agent, filepath):\n",
        "    \"\"\"Save model weights (agent's primary Q-Network: policy_net).\"\"\"\n",
        "    torch.save(agent.policy_net.state_dict(), filepath)\n",
        "    print(f\"✅ Model saved to {filepath}\")\n",
        "\n",
        "def load_model(agent, filepath):\n",
        "    \"\"\"Load model weights into agent's primary Q-Network: policy_net.\"\"\"\n",
        "    state_dict = torch.load(filepath, map_location=agent.device)\n",
        "    agent.policy_net.load_state_dict(state_dict)\n",
        "    agent.policy_net.eval() # Set network to evaluation mode after loading\n",
        "    print(f\"✅ Model loaded from {filepath}\")\n",
        "    return agent\n",
        "\n",
        "def record_video(env, agent, filename):\n",
        "    \"\"\"Record agent gameplay video.\"\"\"\n",
        "    # Set a seed for consistent visualization\n",
        "    SEED = 42\n",
        "\n",
        "    # 1. Wrap the environment for recording.\n",
        "    video_env = RecordVideo(env, filename, episode_trigger=lambda x: True, name_prefix='eval_run')\n",
        "\n",
        "    # 2. Run an episode\n",
        "    state, _ = video_env.reset(seed=SEED)\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    step = 0\n",
        "\n",
        "    while not done:\n",
        "        # *** FIX: Using agent.select_action and epsilon=0.0 ***\n",
        "        # Action selection uses the trained agent's greedy policy\n",
        "        action = agent.select_action(state, epsilon=0.0)\n",
        "\n",
        "        next_state, reward, terminated, truncated, _ = video_env.step(action)\n",
        "        done = terminated or truncated\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        step += 1\n",
        "\n",
        "    video_env.close()\n",
        "    print(f\"✅ Video recorded in the '{filename}' directory. Total Reward: {total_reward:.2f}, Steps: {step}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MFs3iP3vQ9gm",
        "outputId": "ee79c505-f132-49e8-c6da-22aa1287f011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/5000 | Reward: -7.50 | Avg loss: 0.0000 | Epsilon: 0.778\n",
            "Episode 2/5000 | Reward: -8.70 | Avg loss: 0.0000 | Epsilon: 0.606\n",
            "Episode 3/5000 | Reward: -8.70 | Avg loss: 0.0000 | Epsilon: 0.471\n",
            "Episode 4/5000 | Reward: -8.70 | Avg loss: 0.0000 | Epsilon: 0.367\n",
            "Episode 5/5000 | Reward: -8.70 | Avg loss: 0.0000 | Epsilon: 0.286\n",
            "Episode 6/5000 | Reward: -9.30 | Avg loss: 0.0000 | Epsilon: 0.222\n",
            "Episode 7/5000 | Reward: -9.30 | Avg loss: 0.0000 | Epsilon: 0.173\n",
            "Episode 8/5000 | Reward: -9.30 | Avg loss: 0.0000 | Epsilon: 0.135\n",
            "Episode 9/5000 | Reward: -9.30 | Avg loss: 0.0000 | Epsilon: 0.105\n",
            "Episode 10/5000 | Reward: -9.30 | Avg loss: 0.0000 | Epsilon: 0.082\n",
            "Episode 11/5000 | Reward: -9.30 | Avg loss: 0.0000 | Epsilon: 0.063\n",
            "Episode 12/5000 | Reward: -9.30 | Avg loss: 0.0000 | Epsilon: 0.049\n",
            "Episode 13/5000 | Reward: -9.30 | Avg loss: 0.0000 | Epsilon: 0.038\n",
            "Episode 14/5000 | Reward: -9.30 | Avg loss: 0.0000 | Epsilon: 0.030\n",
            "Episode 15/5000 | Reward: -9.30 | Avg loss: 0.0000 | Epsilon: 0.023\n",
            "Episode 16/5000 | Reward: -9.30 | Avg loss: 0.0000 | Epsilon: 0.018\n",
            "Episode 17/5000 | Reward: -9.30 | Avg loss: 0.0000 | Epsilon: 0.014\n",
            "Episode 18/5000 | Reward: -9.30 | Avg loss: 0.0000 | Epsilon: 0.011\n",
            "Episode 19/5000 | Reward: -9.30 | Avg loss: 0.0000 | Epsilon: 0.010\n",
            "Episode 20/5000 | Reward: -9.30 | Avg loss: 0.1411 | Epsilon: 0.010\n",
            "Episode 21/5000 | Reward: -6.30 | Avg loss: 0.0383 | Epsilon: 0.010\n",
            "Episode 22/5000 | Reward: 2.10 | Avg loss: 0.0105 | Epsilon: 0.010\n",
            "Episode 23/5000 | Reward: -3.90 | Avg loss: 0.0192 | Epsilon: 0.010\n",
            "Episode 24/5000 | Reward: -3.40 | Avg loss: 0.0157 | Epsilon: 0.010\n",
            "Episode 25/5000 | Reward: -0.60 | Avg loss: 0.0168 | Epsilon: 0.010\n",
            "Episode 26/5000 | Reward: -0.90 | Avg loss: 0.0215 | Epsilon: 0.010\n",
            "Episode 27/5000 | Reward: -0.30 | Avg loss: 0.0208 | Epsilon: 0.010\n",
            "Episode 28/5000 | Reward: -2.30 | Avg loss: 0.0197 | Epsilon: 0.010\n",
            "Episode 29/5000 | Reward: 0.90 | Avg loss: 0.0177 | Epsilon: 0.010\n",
            "Episode 30/5000 | Reward: 0.30 | Avg loss: 0.0275 | Epsilon: 0.010\n",
            "Episode 31/5000 | Reward: -1.60 | Avg loss: 0.0241 | Epsilon: 0.010\n",
            "Episode 32/5000 | Reward: -0.90 | Avg loss: 0.0346 | Epsilon: 0.010\n",
            "Episode 33/5000 | Reward: 0.50 | Avg loss: 0.0390 | Epsilon: 0.010\n",
            "Episode 34/5000 | Reward: -0.90 | Avg loss: 0.0476 | Epsilon: 0.010\n",
            "Episode 35/5000 | Reward: -1.30 | Avg loss: 0.0538 | Epsilon: 0.010\n",
            "Episode 36/5000 | Reward: -0.90 | Avg loss: 0.0559 | Epsilon: 0.010\n",
            "Episode 37/5000 | Reward: -0.90 | Avg loss: 0.0572 | Epsilon: 0.010\n",
            "Episode 38/5000 | Reward: -2.20 | Avg loss: 0.0584 | Epsilon: 0.010\n",
            "Episode 39/5000 | Reward: -0.90 | Avg loss: 0.0493 | Epsilon: 0.010\n",
            "Episode 40/5000 | Reward: 0.10 | Avg loss: 0.0480 | Epsilon: 0.010\n",
            "Episode 41/5000 | Reward: 0.30 | Avg loss: 0.0428 | Epsilon: 0.010\n",
            "Episode 42/5000 | Reward: 2.80 | Avg loss: 0.0661 | Epsilon: 0.010\n",
            "Episode 43/5000 | Reward: 2.10 | Avg loss: 0.0605 | Epsilon: 0.010\n",
            "Episode 44/5000 | Reward: -0.40 | Avg loss: 0.0741 | Epsilon: 0.010\n",
            "Episode 45/5000 | Reward: -0.90 | Avg loss: 0.0819 | Epsilon: 0.010\n",
            "Episode 46/5000 | Reward: 3.80 | Avg loss: 0.0597 | Epsilon: 0.010\n",
            "Episode 47/5000 | Reward: -0.90 | Avg loss: 0.0917 | Epsilon: 0.010\n",
            "Episode 48/5000 | Reward: 2.80 | Avg loss: 0.0784 | Epsilon: 0.010\n",
            "Episode 49/5000 | Reward: -0.30 | Avg loss: 0.1109 | Epsilon: 0.010\n",
            "Episode 50/5000 | Reward: -0.90 | Avg loss: 0.1071 | Epsilon: 0.010\n",
            "Episode 51/5000 | Reward: 2.20 | Avg loss: 0.0833 | Epsilon: 0.010\n",
            "Episode 52/5000 | Reward: -1.60 | Avg loss: 0.0790 | Epsilon: 0.010\n",
            "Episode 53/5000 | Reward: 2.70 | Avg loss: 0.0847 | Epsilon: 0.010\n",
            "Episode 54/5000 | Reward: 1.50 | Avg loss: 0.0823 | Epsilon: 0.010\n",
            "Episode 55/5000 | Reward: -1.10 | Avg loss: 0.0761 | Epsilon: 0.010\n",
            "Episode 56/5000 | Reward: 2.20 | Avg loss: 0.1069 | Epsilon: 0.010\n",
            "Episode 57/5000 | Reward: -1.50 | Avg loss: 0.0930 | Epsilon: 0.010\n",
            "Episode 58/5000 | Reward: -0.90 | Avg loss: 0.0812 | Epsilon: 0.010\n",
            "Episode 59/5000 | Reward: 0.90 | Avg loss: 0.0824 | Epsilon: 0.010\n",
            "Episode 60/5000 | Reward: -0.40 | Avg loss: 0.0761 | Epsilon: 0.010\n",
            "Episode 61/5000 | Reward: -0.70 | Avg loss: 0.0623 | Epsilon: 0.010\n",
            "Episode 62/5000 | Reward: 2.20 | Avg loss: 0.0770 | Epsilon: 0.010\n",
            "Episode 63/5000 | Reward: 1.10 | Avg loss: 0.0776 | Epsilon: 0.010\n",
            "Episode 64/5000 | Reward: 2.00 | Avg loss: 0.0986 | Epsilon: 0.010\n",
            "Episode 65/5000 | Reward: 1.60 | Avg loss: 0.0697 | Epsilon: 0.010\n",
            "Episode 66/5000 | Reward: 0.20 | Avg loss: 0.0699 | Epsilon: 0.010\n",
            "Episode 67/5000 | Reward: 2.40 | Avg loss: 0.0727 | Epsilon: 0.010\n",
            "Episode 68/5000 | Reward: -0.90 | Avg loss: 0.0668 | Epsilon: 0.010\n",
            "Episode 69/5000 | Reward: 3.10 | Avg loss: 0.0809 | Epsilon: 0.010\n",
            "Episode 70/5000 | Reward: -0.90 | Avg loss: 0.0891 | Epsilon: 0.010\n",
            "Episode 71/5000 | Reward: 1.20 | Avg loss: 0.0823 | Epsilon: 0.010\n",
            "Episode 72/5000 | Reward: 2.60 | Avg loss: 0.0862 | Epsilon: 0.010\n",
            "Episode 73/5000 | Reward: 2.10 | Avg loss: 0.0850 | Epsilon: 0.010\n",
            "Episode 74/5000 | Reward: 2.70 | Avg loss: 0.0874 | Epsilon: 0.010\n",
            "Episode 75/5000 | Reward: -0.90 | Avg loss: 0.0796 | Epsilon: 0.010\n",
            "Episode 76/5000 | Reward: -1.30 | Avg loss: 0.0964 | Epsilon: 0.010\n",
            "Episode 77/5000 | Reward: 2.10 | Avg loss: 0.1002 | Epsilon: 0.010\n",
            "Episode 78/5000 | Reward: 1.30 | Avg loss: 0.0839 | Epsilon: 0.010\n",
            "Episode 79/5000 | Reward: 0.40 | Avg loss: 0.0822 | Epsilon: 0.010\n",
            "Episode 80/5000 | Reward: 1.90 | Avg loss: 0.0915 | Epsilon: 0.010\n",
            "Episode 81/5000 | Reward: -0.90 | Avg loss: 0.0920 | Epsilon: 0.010\n",
            "Episode 82/5000 | Reward: 0.90 | Avg loss: 0.1033 | Epsilon: 0.010\n",
            "Episode 83/5000 | Reward: -0.90 | Avg loss: 0.0823 | Epsilon: 0.010\n",
            "Episode 84/5000 | Reward: -3.60 | Avg loss: 0.0828 | Epsilon: 0.010\n",
            "Episode 85/5000 | Reward: 2.60 | Avg loss: 0.0689 | Epsilon: 0.010\n",
            "Episode 86/5000 | Reward: 0.30 | Avg loss: 0.0725 | Epsilon: 0.010\n",
            "Episode 87/5000 | Reward: 0.10 | Avg loss: 0.0748 | Epsilon: 0.010\n",
            "Episode 88/5000 | Reward: -0.90 | Avg loss: 0.0801 | Epsilon: 0.010\n",
            "Episode 89/5000 | Reward: 2.50 | Avg loss: 0.0876 | Epsilon: 0.010\n",
            "Episode 90/5000 | Reward: -0.90 | Avg loss: 0.0644 | Epsilon: 0.010\n",
            "Episode 91/5000 | Reward: 2.50 | Avg loss: 0.0624 | Epsilon: 0.010\n",
            "Episode 92/5000 | Reward: -0.90 | Avg loss: 0.0632 | Epsilon: 0.010\n",
            "Episode 93/5000 | Reward: 3.00 | Avg loss: 0.0674 | Epsilon: 0.010\n",
            "Episode 94/5000 | Reward: 1.80 | Avg loss: 0.0700 | Epsilon: 0.010\n",
            "Episode 95/5000 | Reward: -1.60 | Avg loss: 0.0702 | Epsilon: 0.010\n",
            "Episode 96/5000 | Reward: 1.20 | Avg loss: 0.0901 | Epsilon: 0.010\n",
            "Episode 97/5000 | Reward: 1.40 | Avg loss: 0.0734 | Epsilon: 0.010\n",
            "Episode 98/5000 | Reward: 1.20 | Avg loss: 0.0657 | Epsilon: 0.010\n",
            "Episode 99/5000 | Reward: -0.90 | Avg loss: 0.0658 | Epsilon: 0.010\n",
            "Episode 100/5000 | Reward: -0.90 | Avg loss: 0.0552 | Epsilon: 0.010\n",
            "Episode 101/5000 | Reward: -0.90 | Avg loss: 0.0599 | Epsilon: 0.010\n",
            "Episode 102/5000 | Reward: -0.90 | Avg loss: 0.0626 | Epsilon: 0.010\n",
            "Episode 103/5000 | Reward: 1.90 | Avg loss: 0.0700 | Epsilon: 0.010\n",
            "Episode 104/5000 | Reward: 2.90 | Avg loss: 0.0653 | Epsilon: 0.010\n",
            "Episode 105/5000 | Reward: 4.60 | Avg loss: 0.0620 | Epsilon: 0.010\n",
            "Episode 106/5000 | Reward: 0.30 | Avg loss: 0.0724 | Epsilon: 0.010\n",
            "Episode 107/5000 | Reward: -0.90 | Avg loss: 0.0641 | Epsilon: 0.010\n",
            "Episode 108/5000 | Reward: 0.50 | Avg loss: 0.0776 | Epsilon: 0.010\n",
            "Episode 109/5000 | Reward: -0.90 | Avg loss: 0.0755 | Epsilon: 0.010\n",
            "Episode 110/5000 | Reward: 2.60 | Avg loss: 0.0730 | Epsilon: 0.010\n",
            "Episode 111/5000 | Reward: 2.40 | Avg loss: 0.0683 | Epsilon: 0.010\n",
            "Episode 112/5000 | Reward: -3.10 | Avg loss: 0.0826 | Epsilon: 0.010\n",
            "Episode 113/5000 | Reward: 0.30 | Avg loss: 0.0800 | Epsilon: 0.010\n",
            "Episode 114/5000 | Reward: 3.00 | Avg loss: 0.0755 | Epsilon: 0.010\n",
            "Episode 115/5000 | Reward: 1.70 | Avg loss: 0.0678 | Epsilon: 0.010\n",
            "Episode 116/5000 | Reward: -0.20 | Avg loss: 0.0787 | Epsilon: 0.010\n",
            "Episode 117/5000 | Reward: -3.30 | Avg loss: 0.0650 | Epsilon: 0.010\n",
            "Episode 118/5000 | Reward: 2.50 | Avg loss: 0.0686 | Epsilon: 0.010\n",
            "Episode 119/5000 | Reward: 1.10 | Avg loss: 0.0626 | Epsilon: 0.010\n",
            "Episode 120/5000 | Reward: 2.70 | Avg loss: 0.0513 | Epsilon: 0.010\n",
            "Episode 121/5000 | Reward: 2.10 | Avg loss: 0.0512 | Epsilon: 0.010\n",
            "Episode 122/5000 | Reward: 3.00 | Avg loss: 0.0599 | Epsilon: 0.010\n",
            "Episode 123/5000 | Reward: 1.50 | Avg loss: 0.0608 | Epsilon: 0.010\n",
            "Episode 124/5000 | Reward: -0.10 | Avg loss: 0.0642 | Epsilon: 0.010\n",
            "Episode 125/5000 | Reward: 2.70 | Avg loss: 0.0567 | Epsilon: 0.010\n",
            "Episode 126/5000 | Reward: 2.80 | Avg loss: 0.0503 | Epsilon: 0.010\n",
            "Episode 127/5000 | Reward: 2.50 | Avg loss: 0.0514 | Epsilon: 0.010\n",
            "Episode 128/5000 | Reward: -0.30 | Avg loss: 0.0460 | Epsilon: 0.010\n",
            "Episode 129/5000 | Reward: 3.70 | Avg loss: 0.0548 | Epsilon: 0.010\n",
            "Episode 130/5000 | Reward: -0.90 | Avg loss: 0.0509 | Epsilon: 0.010\n",
            "Episode 131/5000 | Reward: 3.00 | Avg loss: 0.0564 | Epsilon: 0.010\n",
            "Episode 132/5000 | Reward: 2.60 | Avg loss: 0.0687 | Epsilon: 0.010\n",
            "Episode 133/5000 | Reward: 2.50 | Avg loss: 0.0747 | Epsilon: 0.010\n",
            "Episode 134/5000 | Reward: 1.50 | Avg loss: 0.0495 | Epsilon: 0.010\n",
            "Episode 135/5000 | Reward: -0.90 | Avg loss: 0.0694 | Epsilon: 0.010\n",
            "Episode 136/5000 | Reward: 0.30 | Avg loss: 0.0622 | Epsilon: 0.010\n",
            "Episode 137/5000 | Reward: 2.50 | Avg loss: 0.0641 | Epsilon: 0.010\n",
            "Episode 138/5000 | Reward: 1.90 | Avg loss: 0.0559 | Epsilon: 0.010\n",
            "Episode 139/5000 | Reward: 3.00 | Avg loss: 0.0716 | Epsilon: 0.010\n",
            "Episode 140/5000 | Reward: 0.90 | Avg loss: 0.0633 | Epsilon: 0.010\n",
            "Episode 141/5000 | Reward: -0.90 | Avg loss: 0.0650 | Epsilon: 0.010\n",
            "Episode 142/5000 | Reward: 2.60 | Avg loss: 0.0468 | Epsilon: 0.010\n",
            "Episode 143/5000 | Reward: -0.90 | Avg loss: 0.0484 | Epsilon: 0.010\n",
            "Episode 144/5000 | Reward: 2.60 | Avg loss: 0.0489 | Epsilon: 0.010\n",
            "Episode 145/5000 | Reward: 2.10 | Avg loss: 0.0422 | Epsilon: 0.010\n",
            "Episode 146/5000 | Reward: 2.10 | Avg loss: 0.0454 | Epsilon: 0.010\n",
            "Episode 147/5000 | Reward: 2.90 | Avg loss: 0.0427 | Epsilon: 0.010\n",
            "Episode 148/5000 | Reward: 1.20 | Avg loss: 0.0410 | Epsilon: 0.010\n",
            "Episode 149/5000 | Reward: 2.50 | Avg loss: 0.0401 | Epsilon: 0.010\n",
            "Episode 150/5000 | Reward: -0.90 | Avg loss: 0.0388 | Epsilon: 0.010\n",
            "Episode 151/5000 | Reward: 2.00 | Avg loss: 0.0494 | Epsilon: 0.010\n",
            "Episode 152/5000 | Reward: 2.90 | Avg loss: 0.0438 | Epsilon: 0.010\n",
            "Episode 153/5000 | Reward: -0.30 | Avg loss: 0.0496 | Epsilon: 0.010\n",
            "Episode 154/5000 | Reward: 2.90 | Avg loss: 0.0406 | Epsilon: 0.010\n",
            "Episode 155/5000 | Reward: -3.60 | Avg loss: 0.0546 | Epsilon: 0.010\n",
            "Episode 156/5000 | Reward: 1.90 | Avg loss: 0.0505 | Epsilon: 0.010\n",
            "Episode 157/5000 | Reward: 3.00 | Avg loss: 0.0447 | Epsilon: 0.010\n",
            "Episode 158/5000 | Reward: -1.60 | Avg loss: 0.0474 | Epsilon: 0.010\n",
            "Episode 159/5000 | Reward: 2.90 | Avg loss: 0.0574 | Epsilon: 0.010\n",
            "Episode 160/5000 | Reward: 1.00 | Avg loss: 0.0470 | Epsilon: 0.010\n",
            "Episode 161/5000 | Reward: 2.40 | Avg loss: 0.0650 | Epsilon: 0.010\n",
            "Episode 162/5000 | Reward: 2.90 | Avg loss: 0.0729 | Epsilon: 0.010\n",
            "Episode 163/5000 | Reward: 2.10 | Avg loss: 0.0509 | Epsilon: 0.010\n",
            "Episode 164/5000 | Reward: 2.40 | Avg loss: 0.0682 | Epsilon: 0.010\n",
            "Episode 165/5000 | Reward: 1.00 | Avg loss: 0.0804 | Epsilon: 0.010\n",
            "Episode 166/5000 | Reward: 2.70 | Avg loss: 0.0720 | Epsilon: 0.010\n",
            "Episode 167/5000 | Reward: -0.90 | Avg loss: 0.0526 | Epsilon: 0.010\n",
            "Episode 168/5000 | Reward: 3.00 | Avg loss: 0.0738 | Epsilon: 0.010\n",
            "Episode 169/5000 | Reward: 0.60 | Avg loss: 0.0620 | Epsilon: 0.010\n",
            "Episode 170/5000 | Reward: -1.70 | Avg loss: 0.0628 | Epsilon: 0.010\n",
            "Episode 171/5000 | Reward: 3.00 | Avg loss: 0.0525 | Epsilon: 0.010\n",
            "Episode 172/5000 | Reward: 2.60 | Avg loss: 0.0478 | Epsilon: 0.010\n",
            "Episode 173/5000 | Reward: 0.80 | Avg loss: 0.0461 | Epsilon: 0.010\n",
            "Episode 174/5000 | Reward: -0.30 | Avg loss: 0.0537 | Epsilon: 0.010\n",
            "Episode 175/5000 | Reward: -0.60 | Avg loss: 0.0605 | Epsilon: 0.010\n",
            "Episode 176/5000 | Reward: 0.10 | Avg loss: 0.0525 | Epsilon: 0.010\n",
            "Episode 177/5000 | Reward: 0.20 | Avg loss: 0.0577 | Epsilon: 0.010\n",
            "Episode 178/5000 | Reward: 2.90 | Avg loss: 0.0511 | Epsilon: 0.010\n",
            "Episode 179/5000 | Reward: 2.30 | Avg loss: 0.0448 | Epsilon: 0.010\n",
            "Episode 180/5000 | Reward: 2.60 | Avg loss: 0.0513 | Epsilon: 0.010\n",
            "Episode 181/5000 | Reward: 4.20 | Avg loss: 0.0540 | Epsilon: 0.010\n",
            "Episode 182/5000 | Reward: 0.30 | Avg loss: 0.0481 | Epsilon: 0.010\n",
            "Episode 183/5000 | Reward: -2.40 | Avg loss: 0.0519 | Epsilon: 0.010\n",
            "Episode 184/5000 | Reward: -0.90 | Avg loss: 0.0559 | Epsilon: 0.010\n",
            "Episode 185/5000 | Reward: -1.00 | Avg loss: 0.0501 | Epsilon: 0.010\n",
            "Episode 186/5000 | Reward: -0.90 | Avg loss: 0.0659 | Epsilon: 0.010\n",
            "Episode 187/5000 | Reward: 1.90 | Avg loss: 0.0574 | Epsilon: 0.010\n",
            "Episode 188/5000 | Reward: 0.10 | Avg loss: 0.0757 | Epsilon: 0.010\n",
            "Episode 189/5000 | Reward: 0.90 | Avg loss: 0.0684 | Epsilon: 0.010\n",
            "Episode 190/5000 | Reward: 3.20 | Avg loss: 0.0554 | Epsilon: 0.010\n",
            "Episode 191/5000 | Reward: -0.10 | Avg loss: 0.0549 | Epsilon: 0.010\n",
            "Episode 192/5000 | Reward: -0.90 | Avg loss: 0.0588 | Epsilon: 0.010\n",
            "Episode 193/5000 | Reward: 2.80 | Avg loss: 0.0544 | Epsilon: 0.010\n",
            "Episode 194/5000 | Reward: -0.90 | Avg loss: 0.0647 | Epsilon: 0.010\n",
            "Episode 195/5000 | Reward: 1.10 | Avg loss: 0.0810 | Epsilon: 0.010\n",
            "Episode 196/5000 | Reward: -0.90 | Avg loss: 0.0842 | Epsilon: 0.010\n",
            "Episode 197/5000 | Reward: -0.90 | Avg loss: 0.0711 | Epsilon: 0.010\n",
            "Episode 198/5000 | Reward: -0.90 | Avg loss: 0.0628 | Epsilon: 0.010\n",
            "Episode 199/5000 | Reward: -0.90 | Avg loss: 0.0935 | Epsilon: 0.010\n",
            "Episode 200/5000 | Reward: 2.20 | Avg loss: 0.0700 | Epsilon: 0.010\n",
            "Episode 201/5000 | Reward: 4.30 | Avg loss: 0.0835 | Epsilon: 0.010\n",
            "Episode 202/5000 | Reward: 3.20 | Avg loss: 0.0650 | Epsilon: 0.010\n",
            "Episode 203/5000 | Reward: 2.40 | Avg loss: 0.0854 | Epsilon: 0.010\n",
            "Episode 204/5000 | Reward: 1.90 | Avg loss: 0.0861 | Epsilon: 0.010\n",
            "Episode 205/5000 | Reward: -0.30 | Avg loss: 0.0700 | Epsilon: 0.010\n",
            "Episode 206/5000 | Reward: 3.00 | Avg loss: 0.0731 | Epsilon: 0.010\n",
            "Episode 207/5000 | Reward: 2.00 | Avg loss: 0.0667 | Epsilon: 0.010\n",
            "Episode 208/5000 | Reward: 1.20 | Avg loss: 0.0787 | Epsilon: 0.010\n",
            "Episode 209/5000 | Reward: 2.30 | Avg loss: 0.0724 | Epsilon: 0.010\n",
            "Episode 210/5000 | Reward: 2.30 | Avg loss: 0.0662 | Epsilon: 0.010\n",
            "Episode 211/5000 | Reward: -1.00 | Avg loss: 0.0579 | Epsilon: 0.010\n",
            "Episode 212/5000 | Reward: 1.20 | Avg loss: 0.0654 | Epsilon: 0.010\n",
            "Episode 213/5000 | Reward: 1.10 | Avg loss: 0.0643 | Epsilon: 0.010\n",
            "Episode 214/5000 | Reward: 2.40 | Avg loss: 0.0527 | Epsilon: 0.010\n",
            "Episode 215/5000 | Reward: 2.80 | Avg loss: 0.0598 | Epsilon: 0.010\n",
            "Episode 216/5000 | Reward: 0.90 | Avg loss: 0.0704 | Epsilon: 0.010\n",
            "Episode 217/5000 | Reward: 2.10 | Avg loss: 0.0676 | Epsilon: 0.010\n",
            "Episode 218/5000 | Reward: 2.80 | Avg loss: 0.0625 | Epsilon: 0.010\n",
            "Episode 219/5000 | Reward: 2.90 | Avg loss: 0.0725 | Epsilon: 0.010\n",
            "Episode 220/5000 | Reward: 2.20 | Avg loss: 0.0661 | Epsilon: 0.010\n",
            "Episode 221/5000 | Reward: -0.30 | Avg loss: 0.0700 | Epsilon: 0.010\n",
            "Episode 222/5000 | Reward: 2.80 | Avg loss: 0.0674 | Epsilon: 0.010\n",
            "Episode 223/5000 | Reward: -0.80 | Avg loss: 0.0640 | Epsilon: 0.010\n",
            "Episode 224/5000 | Reward: 2.90 | Avg loss: 0.0995 | Epsilon: 0.010\n",
            "Episode 225/5000 | Reward: 2.80 | Avg loss: 0.1092 | Epsilon: 0.010\n",
            "Episode 226/5000 | Reward: -2.90 | Avg loss: 0.0867 | Epsilon: 0.010\n",
            "Episode 227/5000 | Reward: 0.90 | Avg loss: 0.0884 | Epsilon: 0.010\n",
            "Episode 228/5000 | Reward: 1.10 | Avg loss: 0.0904 | Epsilon: 0.010\n",
            "Episode 229/5000 | Reward: 2.60 | Avg loss: 0.0945 | Epsilon: 0.010\n",
            "Episode 230/5000 | Reward: -0.30 | Avg loss: 0.0779 | Epsilon: 0.010\n",
            "Episode 231/5000 | Reward: 0.30 | Avg loss: 0.0744 | Epsilon: 0.010\n",
            "Episode 232/5000 | Reward: 3.70 | Avg loss: 0.0698 | Epsilon: 0.010\n",
            "Episode 233/5000 | Reward: 2.80 | Avg loss: 0.0651 | Epsilon: 0.010\n",
            "Episode 234/5000 | Reward: -1.20 | Avg loss: 0.0783 | Epsilon: 0.010\n",
            "Episode 235/5000 | Reward: -4.20 | Avg loss: 0.0702 | Epsilon: 0.010\n",
            "Episode 236/5000 | Reward: -2.40 | Avg loss: 0.0672 | Epsilon: 0.010\n",
            "Episode 237/5000 | Reward: -0.30 | Avg loss: 0.0410 | Epsilon: 0.010\n",
            "Episode 238/5000 | Reward: -0.90 | Avg loss: 0.0485 | Epsilon: 0.010\n",
            "Episode 239/5000 | Reward: 2.80 | Avg loss: 0.0478 | Epsilon: 0.010\n",
            "Episode 240/5000 | Reward: -1.80 | Avg loss: 0.0484 | Epsilon: 0.010\n",
            "Episode 241/5000 | Reward: 0.70 | Avg loss: 0.0568 | Epsilon: 0.010\n",
            "Episode 242/5000 | Reward: 2.90 | Avg loss: 0.0519 | Epsilon: 0.010\n",
            "Episode 243/5000 | Reward: 2.80 | Avg loss: 0.0510 | Epsilon: 0.010\n",
            "Episode 244/5000 | Reward: 2.70 | Avg loss: 0.0488 | Epsilon: 0.010\n",
            "Episode 245/5000 | Reward: 3.00 | Avg loss: 0.0450 | Epsilon: 0.010\n",
            "Episode 246/5000 | Reward: -0.90 | Avg loss: 0.0468 | Epsilon: 0.010\n",
            "Episode 247/5000 | Reward: 0.00 | Avg loss: 0.0432 | Epsilon: 0.010\n",
            "Episode 248/5000 | Reward: 2.80 | Avg loss: 0.0464 | Epsilon: 0.010\n",
            "Episode 249/5000 | Reward: 2.90 | Avg loss: 0.0395 | Epsilon: 0.010\n",
            "Episode 250/5000 | Reward: 0.60 | Avg loss: 0.0420 | Epsilon: 0.010\n",
            "Episode 251/5000 | Reward: 2.40 | Avg loss: 0.0498 | Epsilon: 0.010\n",
            "Episode 252/5000 | Reward: 2.90 | Avg loss: 0.0547 | Epsilon: 0.010\n",
            "Episode 253/5000 | Reward: 3.00 | Avg loss: 0.0606 | Epsilon: 0.010\n",
            "Episode 254/5000 | Reward: 3.00 | Avg loss: 0.0443 | Epsilon: 0.010\n",
            "Episode 255/5000 | Reward: 2.30 | Avg loss: 0.0525 | Epsilon: 0.010\n",
            "Episode 256/5000 | Reward: 2.80 | Avg loss: 0.0476 | Epsilon: 0.010\n",
            "Episode 257/5000 | Reward: 2.40 | Avg loss: 0.0521 | Epsilon: 0.010\n",
            "Episode 258/5000 | Reward: 2.70 | Avg loss: 0.0528 | Epsilon: 0.010\n",
            "Episode 259/5000 | Reward: -1.30 | Avg loss: 0.0499 | Epsilon: 0.010\n",
            "Episode 260/5000 | Reward: -1.00 | Avg loss: 0.0506 | Epsilon: 0.010\n",
            "Episode 261/5000 | Reward: 2.80 | Avg loss: 0.0518 | Epsilon: 0.010\n",
            "Episode 262/5000 | Reward: -0.30 | Avg loss: 0.0613 | Epsilon: 0.010\n",
            "Episode 263/5000 | Reward: -0.90 | Avg loss: 0.0690 | Epsilon: 0.010\n",
            "Episode 264/5000 | Reward: 3.00 | Avg loss: 0.0508 | Epsilon: 0.010\n",
            "Episode 265/5000 | Reward: 2.70 | Avg loss: 0.0471 | Epsilon: 0.010\n",
            "Episode 266/5000 | Reward: 2.10 | Avg loss: 0.0476 | Epsilon: 0.010\n",
            "Episode 267/5000 | Reward: 2.80 | Avg loss: 0.0533 | Epsilon: 0.010\n",
            "Episode 268/5000 | Reward: 4.80 | Avg loss: 0.0541 | Epsilon: 0.010\n",
            "Episode 269/5000 | Reward: 2.40 | Avg loss: 0.0624 | Epsilon: 0.010\n",
            "Episode 270/5000 | Reward: -0.90 | Avg loss: 0.0477 | Epsilon: 0.010\n",
            "Episode 271/5000 | Reward: 1.00 | Avg loss: 0.0601 | Epsilon: 0.010\n",
            "Episode 272/5000 | Reward: 2.40 | Avg loss: 0.0562 | Epsilon: 0.010\n",
            "Episode 273/5000 | Reward: 3.00 | Avg loss: 0.0600 | Epsilon: 0.010\n",
            "Episode 274/5000 | Reward: 2.20 | Avg loss: 0.0578 | Epsilon: 0.010\n",
            "Episode 275/5000 | Reward: -0.90 | Avg loss: 0.0607 | Epsilon: 0.010\n",
            "Episode 276/5000 | Reward: 2.90 | Avg loss: 0.0736 | Epsilon: 0.010\n",
            "Episode 277/5000 | Reward: 2.50 | Avg loss: 0.0738 | Epsilon: 0.010\n",
            "Episode 278/5000 | Reward: -0.90 | Avg loss: 0.0700 | Epsilon: 0.010\n",
            "Episode 279/5000 | Reward: 3.60 | Avg loss: 0.0552 | Epsilon: 0.010\n",
            "Episode 280/5000 | Reward: 2.80 | Avg loss: 0.0559 | Epsilon: 0.010\n",
            "Episode 281/5000 | Reward: 3.00 | Avg loss: 0.0664 | Epsilon: 0.010\n",
            "Episode 282/5000 | Reward: 0.60 | Avg loss: 0.0591 | Epsilon: 0.010\n",
            "Episode 283/5000 | Reward: 4.00 | Avg loss: 0.0500 | Epsilon: 0.010\n",
            "Episode 284/5000 | Reward: 1.20 | Avg loss: 0.0520 | Epsilon: 0.010\n",
            "Episode 285/5000 | Reward: -0.90 | Avg loss: 0.0648 | Epsilon: 0.010\n",
            "Episode 286/5000 | Reward: 0.50 | Avg loss: 0.0558 | Epsilon: 0.010\n",
            "Episode 287/5000 | Reward: 1.80 | Avg loss: 0.0503 | Epsilon: 0.010\n",
            "Episode 288/5000 | Reward: -0.30 | Avg loss: 0.0538 | Epsilon: 0.010\n",
            "Episode 289/5000 | Reward: -1.50 | Avg loss: 0.0622 | Epsilon: 0.010\n",
            "Episode 290/5000 | Reward: 0.00 | Avg loss: 0.0624 | Epsilon: 0.010\n",
            "Episode 291/5000 | Reward: -0.60 | Avg loss: 0.0730 | Epsilon: 0.010\n",
            "Episode 292/5000 | Reward: -0.90 | Avg loss: 0.0607 | Epsilon: 0.010\n",
            "Episode 293/5000 | Reward: 0.10 | Avg loss: 0.0515 | Epsilon: 0.010\n",
            "Episode 294/5000 | Reward: 2.30 | Avg loss: 0.0468 | Epsilon: 0.010\n",
            "Episode 295/5000 | Reward: 2.40 | Avg loss: 0.0413 | Epsilon: 0.010\n",
            "Episode 296/5000 | Reward: 2.80 | Avg loss: 0.0421 | Epsilon: 0.010\n",
            "Episode 297/5000 | Reward: 2.70 | Avg loss: 0.0381 | Epsilon: 0.010\n",
            "Episode 298/5000 | Reward: 3.10 | Avg loss: 0.0390 | Epsilon: 0.010\n",
            "Episode 299/5000 | Reward: 0.40 | Avg loss: 0.0395 | Epsilon: 0.010\n",
            "Episode 300/5000 | Reward: 0.60 | Avg loss: 0.0422 | Epsilon: 0.010\n",
            "Episode 301/5000 | Reward: 0.00 | Avg loss: 0.0430 | Epsilon: 0.010\n",
            "Episode 302/5000 | Reward: -1.30 | Avg loss: 0.0485 | Epsilon: 0.010\n",
            "Episode 303/5000 | Reward: 1.30 | Avg loss: 0.0359 | Epsilon: 0.010\n",
            "Episode 304/5000 | Reward: 1.80 | Avg loss: 0.0412 | Epsilon: 0.010\n",
            "Episode 305/5000 | Reward: 3.00 | Avg loss: 0.0410 | Epsilon: 0.010\n",
            "Episode 306/5000 | Reward: 0.10 | Avg loss: 0.0479 | Epsilon: 0.010\n",
            "Episode 307/5000 | Reward: 2.40 | Avg loss: 0.0526 | Epsilon: 0.010\n",
            "Episode 308/5000 | Reward: 1.50 | Avg loss: 0.0463 | Epsilon: 0.010\n",
            "Episode 309/5000 | Reward: 1.30 | Avg loss: 0.0388 | Epsilon: 0.010\n",
            "Episode 310/5000 | Reward: 2.90 | Avg loss: 0.0381 | Epsilon: 0.010\n",
            "Episode 311/5000 | Reward: 3.20 | Avg loss: 0.0486 | Epsilon: 0.010\n",
            "Episode 312/5000 | Reward: 2.90 | Avg loss: 0.0614 | Epsilon: 0.010\n",
            "Episode 313/5000 | Reward: -0.60 | Avg loss: 0.0596 | Epsilon: 0.010\n",
            "Episode 314/5000 | Reward: 2.10 | Avg loss: 0.0561 | Epsilon: 0.010\n",
            "Episode 315/5000 | Reward: -0.70 | Avg loss: 0.0580 | Epsilon: 0.010\n",
            "Episode 316/5000 | Reward: -0.90 | Avg loss: 0.0559 | Epsilon: 0.010\n",
            "Episode 317/5000 | Reward: 2.70 | Avg loss: 0.0610 | Epsilon: 0.010\n",
            "Episode 318/5000 | Reward: 0.10 | Avg loss: 0.0869 | Epsilon: 0.010\n",
            "Episode 319/5000 | Reward: 0.00 | Avg loss: 0.0662 | Epsilon: 0.010\n",
            "Episode 320/5000 | Reward: 1.50 | Avg loss: 0.0709 | Epsilon: 0.010\n",
            "Episode 321/5000 | Reward: 2.70 | Avg loss: 0.0599 | Epsilon: 0.010\n",
            "Episode 322/5000 | Reward: 2.80 | Avg loss: 0.0593 | Epsilon: 0.010\n",
            "Episode 323/5000 | Reward: 4.20 | Avg loss: 0.0644 | Epsilon: 0.010\n",
            "Episode 324/5000 | Reward: 2.80 | Avg loss: 0.0492 | Epsilon: 0.010\n",
            "Episode 325/5000 | Reward: -0.90 | Avg loss: 0.0666 | Epsilon: 0.010\n",
            "Episode 326/5000 | Reward: 2.70 | Avg loss: 0.0488 | Epsilon: 0.010\n",
            "Episode 327/5000 | Reward: -0.90 | Avg loss: 0.0555 | Epsilon: 0.010\n",
            "Episode 328/5000 | Reward: 1.70 | Avg loss: 0.0452 | Epsilon: 0.010\n",
            "Episode 329/5000 | Reward: 2.70 | Avg loss: 0.0605 | Epsilon: 0.010\n",
            "Episode 330/5000 | Reward: -0.90 | Avg loss: 0.0505 | Epsilon: 0.010\n",
            "Episode 331/5000 | Reward: 2.80 | Avg loss: 0.0575 | Epsilon: 0.010\n",
            "Episode 332/5000 | Reward: 2.50 | Avg loss: 0.0436 | Epsilon: 0.010\n",
            "Episode 333/5000 | Reward: 2.10 | Avg loss: 0.0462 | Epsilon: 0.010\n",
            "Episode 334/5000 | Reward: 0.60 | Avg loss: 0.0473 | Epsilon: 0.010\n",
            "Episode 335/5000 | Reward: 3.00 | Avg loss: 0.0423 | Epsilon: 0.010\n",
            "Episode 336/5000 | Reward: 2.50 | Avg loss: 0.0400 | Epsilon: 0.010\n",
            "Episode 337/5000 | Reward: 0.60 | Avg loss: 0.0394 | Epsilon: 0.010\n",
            "Episode 338/5000 | Reward: 2.10 | Avg loss: 0.0352 | Epsilon: 0.010\n",
            "Episode 339/5000 | Reward: 3.00 | Avg loss: 0.0378 | Epsilon: 0.010\n",
            "Episode 340/5000 | Reward: 2.60 | Avg loss: 0.0469 | Epsilon: 0.010\n",
            "Episode 341/5000 | Reward: 2.80 | Avg loss: 0.0491 | Epsilon: 0.010\n",
            "Episode 342/5000 | Reward: 3.70 | Avg loss: 0.0548 | Epsilon: 0.010\n",
            "Episode 343/5000 | Reward: -0.60 | Avg loss: 0.0532 | Epsilon: 0.010\n",
            "Episode 344/5000 | Reward: 2.90 | Avg loss: 0.0495 | Epsilon: 0.010\n",
            "Episode 345/5000 | Reward: -0.90 | Avg loss: 0.0484 | Epsilon: 0.010\n",
            "Episode 346/5000 | Reward: 3.10 | Avg loss: 0.0523 | Epsilon: 0.010\n",
            "Episode 347/5000 | Reward: 2.90 | Avg loss: 0.0488 | Epsilon: 0.010\n",
            "Episode 348/5000 | Reward: 3.70 | Avg loss: 0.0495 | Epsilon: 0.010\n",
            "Episode 349/5000 | Reward: 0.60 | Avg loss: 0.0500 | Epsilon: 0.010\n",
            "Episode 350/5000 | Reward: 3.00 | Avg loss: 0.0623 | Epsilon: 0.010\n",
            "Episode 351/5000 | Reward: -0.90 | Avg loss: 0.0552 | Epsilon: 0.010\n",
            "Episode 352/5000 | Reward: 2.30 | Avg loss: 0.0400 | Epsilon: 0.010\n",
            "Episode 353/5000 | Reward: 3.00 | Avg loss: 0.0478 | Epsilon: 0.010\n",
            "Episode 354/5000 | Reward: 2.80 | Avg loss: 0.0477 | Epsilon: 0.010\n",
            "Episode 355/5000 | Reward: -0.90 | Avg loss: 0.0528 | Epsilon: 0.010\n",
            "Episode 356/5000 | Reward: 2.80 | Avg loss: 0.0426 | Epsilon: 0.010\n",
            "Episode 357/5000 | Reward: 3.00 | Avg loss: 0.0463 | Epsilon: 0.010\n",
            "Episode 358/5000 | Reward: -0.90 | Avg loss: 0.0442 | Epsilon: 0.010\n",
            "Episode 359/5000 | Reward: 2.10 | Avg loss: 0.0471 | Epsilon: 0.010\n",
            "Episode 360/5000 | Reward: 1.90 | Avg loss: 0.0518 | Epsilon: 0.010\n",
            "Episode 361/5000 | Reward: 2.80 | Avg loss: 0.0441 | Epsilon: 0.010\n",
            "Episode 362/5000 | Reward: 2.90 | Avg loss: 0.0465 | Epsilon: 0.010\n",
            "Episode 363/5000 | Reward: 3.10 | Avg loss: 0.0514 | Epsilon: 0.010\n",
            "Episode 364/5000 | Reward: 2.70 | Avg loss: 0.0513 | Epsilon: 0.010\n",
            "Episode 365/5000 | Reward: 1.50 | Avg loss: 0.0646 | Epsilon: 0.010\n",
            "Episode 366/5000 | Reward: -0.90 | Avg loss: 0.0530 | Epsilon: 0.010\n",
            "Episode 367/5000 | Reward: 2.20 | Avg loss: 0.0532 | Epsilon: 0.010\n",
            "Episode 368/5000 | Reward: 3.00 | Avg loss: 0.0600 | Epsilon: 0.010\n",
            "Episode 369/5000 | Reward: -0.40 | Avg loss: 0.0775 | Epsilon: 0.010\n",
            "Episode 370/5000 | Reward: 0.00 | Avg loss: 0.0789 | Epsilon: 0.010\n",
            "Episode 371/5000 | Reward: 2.90 | Avg loss: 0.0641 | Epsilon: 0.010\n",
            "Episode 372/5000 | Reward: 2.80 | Avg loss: 0.0589 | Epsilon: 0.010\n",
            "Episode 373/5000 | Reward: 2.40 | Avg loss: 0.0638 | Epsilon: 0.010\n",
            "Episode 374/5000 | Reward: 2.70 | Avg loss: 0.0844 | Epsilon: 0.010\n",
            "Episode 375/5000 | Reward: 2.90 | Avg loss: 0.0630 | Epsilon: 0.010\n",
            "Episode 376/5000 | Reward: 1.50 | Avg loss: 0.0462 | Epsilon: 0.010\n",
            "Episode 377/5000 | Reward: 2.60 | Avg loss: 0.0560 | Epsilon: 0.010\n",
            "Episode 378/5000 | Reward: -0.90 | Avg loss: 0.0522 | Epsilon: 0.010\n",
            "Episode 379/5000 | Reward: 3.00 | Avg loss: 0.0613 | Epsilon: 0.010\n",
            "Episode 380/5000 | Reward: -0.90 | Avg loss: 0.0440 | Epsilon: 0.010\n",
            "Episode 381/5000 | Reward: -0.90 | Avg loss: 0.0468 | Epsilon: 0.010\n",
            "Episode 382/5000 | Reward: -0.90 | Avg loss: 0.0532 | Epsilon: 0.010\n",
            "Episode 383/5000 | Reward: -0.30 | Avg loss: 0.0363 | Epsilon: 0.010\n",
            "Episode 384/5000 | Reward: 3.20 | Avg loss: 0.0335 | Epsilon: 0.010\n",
            "Episode 385/5000 | Reward: -0.60 | Avg loss: 0.0462 | Epsilon: 0.010\n",
            "Episode 386/5000 | Reward: -0.90 | Avg loss: 0.0451 | Epsilon: 0.010\n",
            "Episode 387/5000 | Reward: 3.20 | Avg loss: 0.0375 | Epsilon: 0.010\n",
            "Episode 388/5000 | Reward: 1.30 | Avg loss: 0.0346 | Epsilon: 0.010\n",
            "Episode 389/5000 | Reward: -0.90 | Avg loss: 0.0336 | Epsilon: 0.010\n",
            "Episode 390/5000 | Reward: 6.10 | Avg loss: 0.0378 | Epsilon: 0.010\n",
            "Episode 391/5000 | Reward: 2.20 | Avg loss: 0.0330 | Epsilon: 0.010\n",
            "Episode 392/5000 | Reward: 2.70 | Avg loss: 0.0397 | Epsilon: 0.010\n",
            "Episode 393/5000 | Reward: 2.80 | Avg loss: 0.0413 | Epsilon: 0.010\n",
            "Episode 394/5000 | Reward: 2.30 | Avg loss: 0.0521 | Epsilon: 0.010\n",
            "Episode 395/5000 | Reward: 2.20 | Avg loss: 0.0480 | Epsilon: 0.010\n",
            "Episode 396/5000 | Reward: 1.90 | Avg loss: 0.0496 | Epsilon: 0.010\n",
            "Episode 397/5000 | Reward: -0.90 | Avg loss: 0.0427 | Epsilon: 0.010\n",
            "Episode 398/5000 | Reward: 5.00 | Avg loss: 0.0499 | Epsilon: 0.010\n",
            "Episode 399/5000 | Reward: 2.90 | Avg loss: 0.0473 | Epsilon: 0.010\n",
            "Episode 400/5000 | Reward: 3.10 | Avg loss: 0.0396 | Epsilon: 0.010\n",
            "Episode 401/5000 | Reward: 0.60 | Avg loss: 0.0517 | Epsilon: 0.010\n",
            "Episode 402/5000 | Reward: 0.60 | Avg loss: 0.0520 | Epsilon: 0.010\n",
            "Episode 403/5000 | Reward: -0.90 | Avg loss: 0.0560 | Epsilon: 0.010\n",
            "Episode 404/5000 | Reward: 2.10 | Avg loss: 0.0549 | Epsilon: 0.010\n",
            "Episode 405/5000 | Reward: 3.00 | Avg loss: 0.0479 | Epsilon: 0.010\n",
            "Episode 406/5000 | Reward: -0.90 | Avg loss: 0.0707 | Epsilon: 0.010\n",
            "Episode 407/5000 | Reward: 2.10 | Avg loss: 0.0464 | Epsilon: 0.010\n",
            "Episode 408/5000 | Reward: 2.70 | Avg loss: 0.0412 | Epsilon: 0.010\n",
            "Episode 409/5000 | Reward: -0.40 | Avg loss: 0.0427 | Epsilon: 0.010\n",
            "Episode 410/5000 | Reward: 1.50 | Avg loss: 0.0416 | Epsilon: 0.010\n",
            "Episode 411/5000 | Reward: -0.60 | Avg loss: 0.0470 | Epsilon: 0.010\n",
            "Episode 412/5000 | Reward: 2.10 | Avg loss: 0.0411 | Epsilon: 0.010\n",
            "Episode 413/5000 | Reward: 2.10 | Avg loss: 0.0511 | Epsilon: 0.010\n",
            "Episode 414/5000 | Reward: 3.20 | Avg loss: 0.0533 | Epsilon: 0.010\n",
            "Episode 415/5000 | Reward: 1.40 | Avg loss: 0.0484 | Epsilon: 0.010\n",
            "Episode 416/5000 | Reward: 0.60 | Avg loss: 0.0445 | Epsilon: 0.010\n",
            "Episode 417/5000 | Reward: 3.20 | Avg loss: 0.0467 | Epsilon: 0.010\n",
            "Episode 418/5000 | Reward: 2.20 | Avg loss: 0.0470 | Epsilon: 0.010\n",
            "Episode 419/5000 | Reward: 2.70 | Avg loss: 0.0470 | Epsilon: 0.010\n",
            "Episode 420/5000 | Reward: 2.20 | Avg loss: 0.0517 | Epsilon: 0.010\n",
            "Episode 421/5000 | Reward: -0.30 | Avg loss: 0.0462 | Epsilon: 0.010\n",
            "Episode 422/5000 | Reward: 3.00 | Avg loss: 0.0483 | Epsilon: 0.010\n",
            "Episode 423/5000 | Reward: 2.80 | Avg loss: 0.0502 | Epsilon: 0.010\n",
            "Episode 424/5000 | Reward: 2.90 | Avg loss: 0.0507 | Epsilon: 0.010\n",
            "Episode 425/5000 | Reward: -0.90 | Avg loss: 0.0609 | Epsilon: 0.010\n",
            "Episode 426/5000 | Reward: 2.60 | Avg loss: 0.0534 | Epsilon: 0.010\n",
            "Episode 427/5000 | Reward: 2.60 | Avg loss: 0.0473 | Epsilon: 0.010\n",
            "Episode 428/5000 | Reward: 1.50 | Avg loss: 0.0490 | Epsilon: 0.010\n",
            "Episode 429/5000 | Reward: -2.40 | Avg loss: 0.0492 | Epsilon: 0.010\n",
            "Episode 430/5000 | Reward: 1.30 | Avg loss: 0.0507 | Epsilon: 0.010\n",
            "Episode 431/5000 | Reward: 0.50 | Avg loss: 0.0448 | Epsilon: 0.010\n",
            "Episode 432/5000 | Reward: 3.00 | Avg loss: 0.0378 | Epsilon: 0.010\n",
            "Episode 433/5000 | Reward: 2.40 | Avg loss: 0.0403 | Epsilon: 0.010\n",
            "Episode 434/5000 | Reward: -0.90 | Avg loss: 0.0355 | Epsilon: 0.010\n",
            "Episode 435/5000 | Reward: -1.30 | Avg loss: 0.0435 | Epsilon: 0.010\n",
            "Episode 436/5000 | Reward: -0.90 | Avg loss: 0.0404 | Epsilon: 0.010\n",
            "Episode 437/5000 | Reward: 3.80 | Avg loss: 0.0412 | Epsilon: 0.010\n",
            "Episode 438/5000 | Reward: -0.90 | Avg loss: 0.0493 | Epsilon: 0.010\n",
            "Episode 439/5000 | Reward: -0.90 | Avg loss: 0.0428 | Epsilon: 0.010\n",
            "Episode 440/5000 | Reward: -2.50 | Avg loss: 0.0433 | Epsilon: 0.010\n",
            "Episode 441/5000 | Reward: 2.30 | Avg loss: 0.0530 | Epsilon: 0.010\n",
            "Episode 442/5000 | Reward: 1.90 | Avg loss: 0.0645 | Epsilon: 0.010\n",
            "Episode 443/5000 | Reward: -0.90 | Avg loss: 0.0543 | Epsilon: 0.010\n",
            "Episode 444/5000 | Reward: 4.80 | Avg loss: 0.0483 | Epsilon: 0.010\n",
            "Episode 445/5000 | Reward: -0.90 | Avg loss: 0.0462 | Epsilon: 0.010\n",
            "Episode 446/5000 | Reward: -0.90 | Avg loss: 0.0439 | Epsilon: 0.010\n",
            "Episode 447/5000 | Reward: 2.40 | Avg loss: 0.0448 | Epsilon: 0.010\n",
            "Episode 448/5000 | Reward: 2.10 | Avg loss: 0.0472 | Epsilon: 0.010\n",
            "Episode 449/5000 | Reward: 3.80 | Avg loss: 0.0449 | Epsilon: 0.010\n",
            "Episode 450/5000 | Reward: 2.90 | Avg loss: 0.0490 | Epsilon: 0.010\n",
            "Episode 451/5000 | Reward: 2.80 | Avg loss: 0.0384 | Epsilon: 0.010\n",
            "Episode 452/5000 | Reward: 0.80 | Avg loss: 0.0559 | Epsilon: 0.010\n",
            "Episode 453/5000 | Reward: 3.10 | Avg loss: 0.0514 | Epsilon: 0.010\n",
            "Episode 454/5000 | Reward: 0.90 | Avg loss: 0.0661 | Epsilon: 0.010\n",
            "Episode 455/5000 | Reward: 3.60 | Avg loss: 0.0601 | Epsilon: 0.010\n",
            "Episode 456/5000 | Reward: 0.00 | Avg loss: 0.0538 | Epsilon: 0.010\n",
            "Episode 457/5000 | Reward: 2.10 | Avg loss: 0.0518 | Epsilon: 0.010\n",
            "Episode 458/5000 | Reward: 0.30 | Avg loss: 0.0526 | Epsilon: 0.010\n",
            "Episode 459/5000 | Reward: 0.30 | Avg loss: 0.0686 | Epsilon: 0.010\n",
            "Episode 460/5000 | Reward: 2.80 | Avg loss: 0.0653 | Epsilon: 0.010\n",
            "Episode 461/5000 | Reward: 0.00 | Avg loss: 0.0527 | Epsilon: 0.010\n",
            "Episode 462/5000 | Reward: 0.90 | Avg loss: 0.0820 | Epsilon: 0.010\n",
            "Episode 463/5000 | Reward: 3.00 | Avg loss: 0.0789 | Epsilon: 0.010\n",
            "Episode 464/5000 | Reward: 2.70 | Avg loss: 0.0670 | Epsilon: 0.010\n",
            "Episode 465/5000 | Reward: 0.60 | Avg loss: 0.0678 | Epsilon: 0.010\n",
            "Episode 466/5000 | Reward: -0.90 | Avg loss: 0.0734 | Epsilon: 0.010\n",
            "Episode 467/5000 | Reward: -0.90 | Avg loss: 0.0781 | Epsilon: 0.010\n",
            "Episode 468/5000 | Reward: 1.60 | Avg loss: 0.0661 | Epsilon: 0.010\n",
            "Episode 469/5000 | Reward: 3.10 | Avg loss: 0.0680 | Epsilon: 0.010\n",
            "Episode 470/5000 | Reward: 2.80 | Avg loss: 0.0591 | Epsilon: 0.010\n",
            "Episode 471/5000 | Reward: 3.80 | Avg loss: 0.0547 | Epsilon: 0.010\n",
            "Episode 472/5000 | Reward: 2.90 | Avg loss: 0.0563 | Epsilon: 0.010\n",
            "Episode 473/5000 | Reward: 3.00 | Avg loss: 0.0555 | Epsilon: 0.010\n",
            "Episode 474/5000 | Reward: 0.90 | Avg loss: 0.0440 | Epsilon: 0.010\n",
            "Episode 475/5000 | Reward: 2.70 | Avg loss: 0.0391 | Epsilon: 0.010\n",
            "Episode 476/5000 | Reward: 2.90 | Avg loss: 0.0520 | Epsilon: 0.010\n",
            "Episode 477/5000 | Reward: 1.80 | Avg loss: 0.0454 | Epsilon: 0.010\n",
            "Episode 478/5000 | Reward: 2.90 | Avg loss: 0.0392 | Epsilon: 0.010\n",
            "Episode 479/5000 | Reward: 1.90 | Avg loss: 0.0516 | Epsilon: 0.010\n",
            "Episode 480/5000 | Reward: 2.90 | Avg loss: 0.0431 | Epsilon: 0.010\n",
            "Episode 481/5000 | Reward: 2.70 | Avg loss: 0.0528 | Epsilon: 0.010\n",
            "Episode 482/5000 | Reward: 1.50 | Avg loss: 0.0473 | Epsilon: 0.010\n",
            "Episode 483/5000 | Reward: 3.10 | Avg loss: 0.0468 | Epsilon: 0.010\n",
            "Episode 484/5000 | Reward: 1.30 | Avg loss: 0.0458 | Epsilon: 0.010\n",
            "Episode 485/5000 | Reward: 2.90 | Avg loss: 0.0558 | Epsilon: 0.010\n",
            "Episode 486/5000 | Reward: 0.70 | Avg loss: 0.0528 | Epsilon: 0.010\n",
            "Episode 487/5000 | Reward: -0.90 | Avg loss: 0.0492 | Epsilon: 0.010\n",
            "Episode 488/5000 | Reward: 0.50 | Avg loss: 0.0562 | Epsilon: 0.010\n",
            "Episode 489/5000 | Reward: 0.00 | Avg loss: 0.0619 | Epsilon: 0.010\n",
            "Episode 490/5000 | Reward: -0.90 | Avg loss: 0.0708 | Epsilon: 0.010\n",
            "Episode 491/5000 | Reward: 2.70 | Avg loss: 0.0640 | Epsilon: 0.010\n",
            "Episode 492/5000 | Reward: 1.80 | Avg loss: 0.0713 | Epsilon: 0.010\n",
            "Episode 493/5000 | Reward: 1.70 | Avg loss: 0.0723 | Epsilon: 0.010\n",
            "Episode 494/5000 | Reward: 1.40 | Avg loss: 0.0550 | Epsilon: 0.010\n",
            "Episode 495/5000 | Reward: 3.20 | Avg loss: 0.0572 | Epsilon: 0.010\n",
            "Episode 496/5000 | Reward: 1.50 | Avg loss: 0.0642 | Epsilon: 0.010\n",
            "Episode 497/5000 | Reward: 2.70 | Avg loss: 0.0502 | Epsilon: 0.010\n",
            "Episode 498/5000 | Reward: 3.60 | Avg loss: 0.0608 | Epsilon: 0.010\n",
            "Episode 499/5000 | Reward: 1.00 | Avg loss: 0.0606 | Epsilon: 0.010\n",
            "Episode 500/5000 | Reward: -0.30 | Avg loss: 0.0514 | Epsilon: 0.010\n",
            "Episode 501/5000 | Reward: 0.60 | Avg loss: 0.0656 | Epsilon: 0.010\n",
            "Episode 502/5000 | Reward: 2.90 | Avg loss: 0.0579 | Epsilon: 0.010\n",
            "Episode 503/5000 | Reward: -0.90 | Avg loss: 0.0580 | Epsilon: 0.010\n",
            "Episode 504/5000 | Reward: 2.10 | Avg loss: 0.0497 | Epsilon: 0.010\n",
            "Episode 505/5000 | Reward: 3.00 | Avg loss: 0.0562 | Epsilon: 0.010\n",
            "Episode 506/5000 | Reward: -0.50 | Avg loss: 0.0537 | Epsilon: 0.010\n",
            "Episode 507/5000 | Reward: 1.00 | Avg loss: 0.0596 | Epsilon: 0.010\n",
            "Episode 508/5000 | Reward: -3.00 | Avg loss: 0.0537 | Epsilon: 0.010\n",
            "Episode 509/5000 | Reward: 2.10 | Avg loss: 0.0520 | Epsilon: 0.010\n",
            "Episode 510/5000 | Reward: 1.00 | Avg loss: 0.0576 | Epsilon: 0.010\n",
            "Episode 511/5000 | Reward: 4.80 | Avg loss: 0.0660 | Epsilon: 0.010\n",
            "Episode 512/5000 | Reward: 2.40 | Avg loss: 0.0647 | Epsilon: 0.010\n",
            "Episode 513/5000 | Reward: 4.90 | Avg loss: 0.0503 | Epsilon: 0.010\n",
            "Episode 514/5000 | Reward: 1.10 | Avg loss: 0.0562 | Epsilon: 0.010\n",
            "Episode 515/5000 | Reward: 1.80 | Avg loss: 0.0653 | Epsilon: 0.010\n",
            "Episode 516/5000 | Reward: 4.60 | Avg loss: 0.0667 | Epsilon: 0.010\n",
            "Episode 517/5000 | Reward: -0.90 | Avg loss: 0.0676 | Epsilon: 0.010\n",
            "Episode 518/5000 | Reward: 3.00 | Avg loss: 0.0705 | Epsilon: 0.010\n",
            "Episode 519/5000 | Reward: 3.00 | Avg loss: 0.0552 | Epsilon: 0.010\n",
            "Episode 520/5000 | Reward: 2.70 | Avg loss: 0.0578 | Epsilon: 0.010\n",
            "Episode 521/5000 | Reward: 5.20 | Avg loss: 0.0556 | Epsilon: 0.010\n",
            "Episode 522/5000 | Reward: 2.90 | Avg loss: 0.0550 | Epsilon: 0.010\n",
            "Episode 523/5000 | Reward: 0.30 | Avg loss: 0.0596 | Epsilon: 0.010\n",
            "Episode 524/5000 | Reward: 1.20 | Avg loss: 0.0615 | Epsilon: 0.010\n",
            "Episode 525/5000 | Reward: 3.20 | Avg loss: 0.0524 | Epsilon: 0.010\n",
            "Episode 526/5000 | Reward: 2.90 | Avg loss: 0.0587 | Epsilon: 0.010\n",
            "Episode 527/5000 | Reward: 1.60 | Avg loss: 0.0633 | Epsilon: 0.010\n",
            "Episode 528/5000 | Reward: 0.00 | Avg loss: 0.0692 | Epsilon: 0.010\n",
            "Episode 529/5000 | Reward: 2.90 | Avg loss: 0.0591 | Epsilon: 0.010\n",
            "Episode 530/5000 | Reward: 2.70 | Avg loss: 0.0551 | Epsilon: 0.010\n",
            "Episode 531/5000 | Reward: 3.00 | Avg loss: 0.0514 | Epsilon: 0.010\n",
            "Episode 532/5000 | Reward: -0.30 | Avg loss: 0.0625 | Epsilon: 0.010\n",
            "Episode 533/5000 | Reward: 2.80 | Avg loss: 0.0665 | Epsilon: 0.010\n",
            "Episode 534/5000 | Reward: 2.70 | Avg loss: 0.0569 | Epsilon: 0.010\n",
            "Episode 535/5000 | Reward: 2.00 | Avg loss: 0.0537 | Epsilon: 0.010\n",
            "Episode 536/5000 | Reward: -1.10 | Avg loss: 0.0494 | Epsilon: 0.010\n",
            "Episode 537/5000 | Reward: 2.70 | Avg loss: 0.0512 | Epsilon: 0.010\n",
            "Episode 538/5000 | Reward: 0.00 | Avg loss: 0.0777 | Epsilon: 0.010\n",
            "Episode 539/5000 | Reward: -0.60 | Avg loss: 0.0683 | Epsilon: 0.010\n",
            "Episode 540/5000 | Reward: 2.70 | Avg loss: 0.0906 | Epsilon: 0.010\n",
            "Episode 541/5000 | Reward: 2.20 | Avg loss: 0.0647 | Epsilon: 0.010\n",
            "Episode 542/5000 | Reward: 2.80 | Avg loss: 0.0622 | Epsilon: 0.010\n",
            "Episode 543/5000 | Reward: 2.40 | Avg loss: 0.0631 | Epsilon: 0.010\n",
            "Episode 544/5000 | Reward: 1.10 | Avg loss: 0.0714 | Epsilon: 0.010\n",
            "Episode 545/5000 | Reward: 3.10 | Avg loss: 0.0748 | Epsilon: 0.010\n",
            "Episode 546/5000 | Reward: -0.40 | Avg loss: 0.0660 | Epsilon: 0.010\n",
            "Episode 547/5000 | Reward: 2.70 | Avg loss: 0.0607 | Epsilon: 0.010\n",
            "Episode 548/5000 | Reward: 3.00 | Avg loss: 0.0596 | Epsilon: 0.010\n",
            "Episode 549/5000 | Reward: 2.40 | Avg loss: 0.0782 | Epsilon: 0.010\n",
            "Episode 550/5000 | Reward: 0.00 | Avg loss: 0.0728 | Epsilon: 0.010\n",
            "Episode 551/5000 | Reward: 0.70 | Avg loss: 0.0580 | Epsilon: 0.010\n",
            "Episode 552/5000 | Reward: 3.10 | Avg loss: 0.0592 | Epsilon: 0.010\n",
            "Episode 553/5000 | Reward: 0.90 | Avg loss: 0.0646 | Epsilon: 0.010\n",
            "Episode 554/5000 | Reward: 2.00 | Avg loss: 0.0514 | Epsilon: 0.010\n",
            "Episode 555/5000 | Reward: 3.00 | Avg loss: 0.0572 | Epsilon: 0.010\n",
            "Episode 556/5000 | Reward: 2.30 | Avg loss: 0.0493 | Epsilon: 0.010\n",
            "Episode 557/5000 | Reward: -1.70 | Avg loss: 0.0510 | Epsilon: 0.010\n",
            "Episode 558/5000 | Reward: 2.90 | Avg loss: 0.0463 | Epsilon: 0.010\n",
            "Episode 559/5000 | Reward: 2.10 | Avg loss: 0.0507 | Epsilon: 0.010\n",
            "Episode 560/5000 | Reward: 3.10 | Avg loss: 0.0432 | Epsilon: 0.010\n",
            "Episode 561/5000 | Reward: 0.00 | Avg loss: 0.0441 | Epsilon: 0.010\n",
            "Episode 562/5000 | Reward: -0.90 | Avg loss: 0.0531 | Epsilon: 0.010\n",
            "Episode 563/5000 | Reward: 0.50 | Avg loss: 0.0431 | Epsilon: 0.010\n",
            "Episode 564/5000 | Reward: 2.10 | Avg loss: 0.0590 | Epsilon: 0.010\n",
            "Episode 565/5000 | Reward: 0.30 | Avg loss: 0.0617 | Epsilon: 0.010\n",
            "Episode 566/5000 | Reward: 0.90 | Avg loss: 0.0605 | Epsilon: 0.010\n",
            "Episode 567/5000 | Reward: 2.10 | Avg loss: 0.0625 | Epsilon: 0.010\n",
            "Episode 568/5000 | Reward: -0.90 | Avg loss: 0.0628 | Epsilon: 0.010\n",
            "Episode 569/5000 | Reward: -0.60 | Avg loss: 0.0648 | Epsilon: 0.010\n",
            "Episode 570/5000 | Reward: -0.90 | Avg loss: 0.0727 | Epsilon: 0.010\n",
            "Episode 571/5000 | Reward: -1.00 | Avg loss: 0.0470 | Epsilon: 0.010\n",
            "Episode 572/5000 | Reward: -0.50 | Avg loss: 0.0501 | Epsilon: 0.010\n",
            "Episode 573/5000 | Reward: -0.90 | Avg loss: 0.0476 | Epsilon: 0.010\n",
            "Episode 574/5000 | Reward: 1.20 | Avg loss: 0.0453 | Epsilon: 0.010\n",
            "Episode 575/5000 | Reward: 4.50 | Avg loss: 0.0434 | Epsilon: 0.010\n",
            "Episode 576/5000 | Reward: 3.10 | Avg loss: 0.0504 | Epsilon: 0.010\n",
            "Episode 577/5000 | Reward: -0.30 | Avg loss: 0.0426 | Epsilon: 0.010\n",
            "Episode 578/5000 | Reward: 3.00 | Avg loss: 0.0385 | Epsilon: 0.010\n",
            "Episode 579/5000 | Reward: 0.00 | Avg loss: 0.0425 | Epsilon: 0.010\n",
            "Episode 580/5000 | Reward: 2.70 | Avg loss: 0.0436 | Epsilon: 0.010\n",
            "Episode 581/5000 | Reward: 3.20 | Avg loss: 0.0484 | Epsilon: 0.010\n",
            "Episode 582/5000 | Reward: 2.80 | Avg loss: 0.0545 | Epsilon: 0.010\n",
            "Episode 583/5000 | Reward: 2.90 | Avg loss: 0.0441 | Epsilon: 0.010\n",
            "Episode 584/5000 | Reward: 2.00 | Avg loss: 0.0452 | Epsilon: 0.010\n",
            "Episode 585/5000 | Reward: 2.10 | Avg loss: 0.0463 | Epsilon: 0.010\n",
            "Episode 586/5000 | Reward: 2.70 | Avg loss: 0.0375 | Epsilon: 0.010\n",
            "Episode 587/5000 | Reward: 2.70 | Avg loss: 0.0323 | Epsilon: 0.010\n",
            "Episode 588/5000 | Reward: -0.90 | Avg loss: 0.0386 | Epsilon: 0.010\n",
            "Episode 589/5000 | Reward: 1.60 | Avg loss: 0.0391 | Epsilon: 0.010\n",
            "Episode 590/5000 | Reward: 3.90 | Avg loss: 0.0382 | Epsilon: 0.010\n",
            "Episode 591/5000 | Reward: 0.60 | Avg loss: 0.0382 | Epsilon: 0.010\n",
            "Episode 592/5000 | Reward: 1.50 | Avg loss: 0.0295 | Epsilon: 0.010\n",
            "Episode 593/5000 | Reward: 1.10 | Avg loss: 0.0465 | Epsilon: 0.010\n",
            "Episode 594/5000 | Reward: 2.10 | Avg loss: 0.0496 | Epsilon: 0.010\n",
            "Episode 595/5000 | Reward: -2.80 | Avg loss: 0.0430 | Epsilon: 0.010\n",
            "Episode 596/5000 | Reward: 2.90 | Avg loss: 0.0463 | Epsilon: 0.010\n",
            "Episode 597/5000 | Reward: -2.40 | Avg loss: 0.0516 | Epsilon: 0.010\n",
            "Episode 598/5000 | Reward: -0.50 | Avg loss: 0.0472 | Epsilon: 0.010\n",
            "Episode 599/5000 | Reward: 3.00 | Avg loss: 0.0559 | Epsilon: 0.010\n",
            "Episode 600/5000 | Reward: 3.00 | Avg loss: 0.0544 | Epsilon: 0.010\n",
            "Episode 601/5000 | Reward: -0.90 | Avg loss: 0.0557 | Epsilon: 0.010\n",
            "Episode 602/5000 | Reward: 2.90 | Avg loss: 0.0486 | Epsilon: 0.010\n",
            "Episode 603/5000 | Reward: -0.90 | Avg loss: 0.0508 | Epsilon: 0.010\n",
            "Episode 604/5000 | Reward: 1.50 | Avg loss: 0.0483 | Epsilon: 0.010\n",
            "Episode 605/5000 | Reward: -1.20 | Avg loss: 0.0519 | Epsilon: 0.010\n",
            "Episode 606/5000 | Reward: 0.10 | Avg loss: 0.0568 | Epsilon: 0.010\n",
            "Episode 607/5000 | Reward: 2.20 | Avg loss: 0.0599 | Epsilon: 0.010\n",
            "Episode 608/5000 | Reward: 6.00 | Avg loss: 0.0459 | Epsilon: 0.010\n",
            "Episode 609/5000 | Reward: 2.30 | Avg loss: 0.0502 | Epsilon: 0.010\n",
            "Episode 610/5000 | Reward: 1.50 | Avg loss: 0.0565 | Epsilon: 0.010\n",
            "Episode 611/5000 | Reward: 0.80 | Avg loss: 0.0570 | Epsilon: 0.010\n",
            "Episode 612/5000 | Reward: 1.50 | Avg loss: 0.0543 | Epsilon: 0.010\n",
            "Episode 613/5000 | Reward: -0.90 | Avg loss: 0.0510 | Epsilon: 0.010\n",
            "Episode 614/5000 | Reward: -0.30 | Avg loss: 0.0446 | Epsilon: 0.010\n",
            "Episode 615/5000 | Reward: 2.80 | Avg loss: 0.0429 | Epsilon: 0.010\n",
            "Episode 616/5000 | Reward: -1.20 | Avg loss: 0.0516 | Epsilon: 0.010\n",
            "Episode 617/5000 | Reward: 2.80 | Avg loss: 0.0499 | Epsilon: 0.010\n",
            "Episode 618/5000 | Reward: 3.00 | Avg loss: 0.0556 | Epsilon: 0.010\n",
            "Episode 619/5000 | Reward: 0.30 | Avg loss: 0.0525 | Epsilon: 0.010\n",
            "Episode 620/5000 | Reward: -0.30 | Avg loss: 0.0579 | Epsilon: 0.010\n",
            "Episode 621/5000 | Reward: 1.50 | Avg loss: 0.0742 | Epsilon: 0.010\n",
            "Episode 622/5000 | Reward: 2.80 | Avg loss: 0.0455 | Epsilon: 0.010\n",
            "Episode 623/5000 | Reward: 5.90 | Avg loss: 0.0558 | Epsilon: 0.010\n",
            "Episode 624/5000 | Reward: 3.60 | Avg loss: 0.0456 | Epsilon: 0.010\n",
            "Episode 625/5000 | Reward: 4.00 | Avg loss: 0.0556 | Epsilon: 0.010\n",
            "Episode 626/5000 | Reward: -0.90 | Avg loss: 0.0614 | Epsilon: 0.010\n",
            "Episode 627/5000 | Reward: 2.60 | Avg loss: 0.0605 | Epsilon: 0.010\n",
            "Episode 628/5000 | Reward: 0.50 | Avg loss: 0.0637 | Epsilon: 0.010\n",
            "Episode 629/5000 | Reward: 3.80 | Avg loss: 0.0630 | Epsilon: 0.010\n",
            "Episode 630/5000 | Reward: 1.10 | Avg loss: 0.0595 | Epsilon: 0.010\n",
            "Episode 631/5000 | Reward: 4.40 | Avg loss: 0.0503 | Epsilon: 0.010\n",
            "Episode 632/5000 | Reward: 3.00 | Avg loss: 0.0477 | Epsilon: 0.010\n",
            "Episode 633/5000 | Reward: -1.20 | Avg loss: 0.0518 | Epsilon: 0.010\n",
            "Episode 634/5000 | Reward: 1.80 | Avg loss: 0.0498 | Epsilon: 0.010\n",
            "Episode 635/5000 | Reward: -0.30 | Avg loss: 0.0555 | Epsilon: 0.010\n",
            "Episode 636/5000 | Reward: -0.90 | Avg loss: 0.0618 | Epsilon: 0.010\n",
            "Episode 637/5000 | Reward: 2.80 | Avg loss: 0.0440 | Epsilon: 0.010\n",
            "Episode 638/5000 | Reward: 2.90 | Avg loss: 0.0505 | Epsilon: 0.010\n",
            "Episode 639/5000 | Reward: -1.10 | Avg loss: 0.0484 | Epsilon: 0.010\n",
            "Episode 640/5000 | Reward: 0.50 | Avg loss: 0.0437 | Epsilon: 0.010\n",
            "Episode 641/5000 | Reward: 3.00 | Avg loss: 0.0476 | Epsilon: 0.010\n",
            "Episode 642/5000 | Reward: 4.30 | Avg loss: 0.0542 | Epsilon: 0.010\n",
            "Episode 643/5000 | Reward: 0.00 | Avg loss: 0.0461 | Epsilon: 0.010\n",
            "Episode 644/5000 | Reward: 1.50 | Avg loss: 0.0464 | Epsilon: 0.010\n",
            "Episode 645/5000 | Reward: 3.10 | Avg loss: 0.0461 | Epsilon: 0.010\n",
            "Episode 646/5000 | Reward: 1.00 | Avg loss: 0.0642 | Epsilon: 0.010\n",
            "Episode 647/5000 | Reward: -1.80 | Avg loss: 0.0506 | Epsilon: 0.010\n",
            "Episode 648/5000 | Reward: -0.90 | Avg loss: 0.0455 | Epsilon: 0.010\n",
            "Episode 649/5000 | Reward: 2.80 | Avg loss: 0.0446 | Epsilon: 0.010\n",
            "Episode 650/5000 | Reward: -0.90 | Avg loss: 0.0433 | Epsilon: 0.010\n",
            "Episode 651/5000 | Reward: 2.20 | Avg loss: 0.0544 | Epsilon: 0.010\n",
            "Episode 652/5000 | Reward: -0.90 | Avg loss: 0.0424 | Epsilon: 0.010\n",
            "Episode 653/5000 | Reward: 2.60 | Avg loss: 0.0389 | Epsilon: 0.010\n",
            "Episode 654/5000 | Reward: 1.30 | Avg loss: 0.0446 | Epsilon: 0.010\n",
            "Episode 655/5000 | Reward: -0.90 | Avg loss: 0.0410 | Epsilon: 0.010\n",
            "Episode 656/5000 | Reward: 1.00 | Avg loss: 0.0456 | Epsilon: 0.010\n",
            "Episode 657/5000 | Reward: -0.90 | Avg loss: 0.0510 | Epsilon: 0.010\n",
            "Episode 658/5000 | Reward: 0.80 | Avg loss: 0.0382 | Epsilon: 0.010\n",
            "Episode 659/5000 | Reward: -0.90 | Avg loss: 0.0352 | Epsilon: 0.010\n",
            "Episode 660/5000 | Reward: 2.10 | Avg loss: 0.0484 | Epsilon: 0.010\n",
            "Episode 661/5000 | Reward: 3.00 | Avg loss: 0.0437 | Epsilon: 0.010\n",
            "Episode 662/5000 | Reward: 4.60 | Avg loss: 0.0490 | Epsilon: 0.010\n",
            "Episode 663/5000 | Reward: -0.00 | Avg loss: 0.0490 | Epsilon: 0.010\n",
            "Episode 664/5000 | Reward: 3.00 | Avg loss: 0.0456 | Epsilon: 0.010\n",
            "Episode 665/5000 | Reward: 3.00 | Avg loss: 0.0524 | Epsilon: 0.010\n",
            "Episode 666/5000 | Reward: 2.20 | Avg loss: 0.0431 | Epsilon: 0.010\n",
            "Episode 667/5000 | Reward: 0.10 | Avg loss: 0.0516 | Epsilon: 0.010\n",
            "Episode 668/5000 | Reward: 0.30 | Avg loss: 0.0614 | Epsilon: 0.010\n",
            "Episode 669/5000 | Reward: 2.10 | Avg loss: 0.0550 | Epsilon: 0.010\n",
            "Episode 670/5000 | Reward: -1.20 | Avg loss: 0.0757 | Epsilon: 0.010\n",
            "Episode 671/5000 | Reward: -0.20 | Avg loss: 0.0689 | Epsilon: 0.010\n",
            "Episode 672/5000 | Reward: 3.20 | Avg loss: 0.0624 | Epsilon: 0.010\n",
            "Episode 673/5000 | Reward: -0.10 | Avg loss: 0.0656 | Epsilon: 0.010\n",
            "Episode 674/5000 | Reward: 2.90 | Avg loss: 0.0645 | Epsilon: 0.010\n",
            "Episode 675/5000 | Reward: 1.30 | Avg loss: 0.0520 | Epsilon: 0.010\n",
            "Episode 676/5000 | Reward: 0.90 | Avg loss: 0.0486 | Epsilon: 0.010\n",
            "Episode 677/5000 | Reward: 3.10 | Avg loss: 0.0526 | Epsilon: 0.010\n",
            "Episode 678/5000 | Reward: 1.50 | Avg loss: 0.0524 | Epsilon: 0.010\n",
            "Episode 679/5000 | Reward: 3.00 | Avg loss: 0.0463 | Epsilon: 0.010\n",
            "Episode 680/5000 | Reward: 8.70 | Avg loss: 0.0545 | Epsilon: 0.010\n",
            "Episode 681/5000 | Reward: -0.20 | Avg loss: 0.0421 | Epsilon: 0.010\n",
            "Episode 682/5000 | Reward: 3.50 | Avg loss: 0.0552 | Epsilon: 0.010\n",
            "Episode 683/5000 | Reward: 0.70 | Avg loss: 0.0563 | Epsilon: 0.010\n",
            "Episode 684/5000 | Reward: 3.00 | Avg loss: 0.0615 | Epsilon: 0.010\n",
            "Episode 685/5000 | Reward: 2.70 | Avg loss: 0.0537 | Epsilon: 0.010\n",
            "Episode 686/5000 | Reward: -0.90 | Avg loss: 0.0583 | Epsilon: 0.010\n",
            "Episode 687/5000 | Reward: 7.10 | Avg loss: 0.0598 | Epsilon: 0.010\n",
            "Episode 688/5000 | Reward: 2.40 | Avg loss: 0.0766 | Epsilon: 0.010\n",
            "Episode 689/5000 | Reward: 1.30 | Avg loss: 0.0565 | Epsilon: 0.010\n",
            "Episode 690/5000 | Reward: -0.60 | Avg loss: 0.0681 | Epsilon: 0.010\n",
            "Episode 691/5000 | Reward: 3.80 | Avg loss: 0.0697 | Epsilon: 0.010\n",
            "Episode 692/5000 | Reward: 4.40 | Avg loss: 0.0517 | Epsilon: 0.010\n",
            "Episode 693/5000 | Reward: 1.30 | Avg loss: 0.0452 | Epsilon: 0.010\n",
            "Episode 694/5000 | Reward: -0.90 | Avg loss: 0.0562 | Epsilon: 0.010\n",
            "Episode 695/5000 | Reward: 1.00 | Avg loss: 0.0578 | Epsilon: 0.010\n",
            "Episode 696/5000 | Reward: -0.90 | Avg loss: 0.0563 | Epsilon: 0.010\n",
            "Episode 697/5000 | Reward: 2.70 | Avg loss: 0.0477 | Epsilon: 0.010\n",
            "Episode 698/5000 | Reward: 0.70 | Avg loss: 0.0620 | Epsilon: 0.010\n",
            "Episode 699/5000 | Reward: 2.10 | Avg loss: 0.0591 | Epsilon: 0.010\n",
            "Episode 700/5000 | Reward: 1.50 | Avg loss: 0.0630 | Epsilon: 0.010\n",
            "Episode 701/5000 | Reward: 2.70 | Avg loss: 0.0562 | Epsilon: 0.010\n",
            "Episode 702/5000 | Reward: 2.90 | Avg loss: 0.0599 | Epsilon: 0.010\n",
            "Episode 703/5000 | Reward: 2.80 | Avg loss: 0.0724 | Epsilon: 0.010\n",
            "Episode 704/5000 | Reward: 2.50 | Avg loss: 0.0559 | Epsilon: 0.010\n",
            "Episode 705/5000 | Reward: -0.90 | Avg loss: 0.0646 | Epsilon: 0.010\n",
            "Episode 706/5000 | Reward: 2.90 | Avg loss: 0.0591 | Epsilon: 0.010\n",
            "Episode 707/5000 | Reward: -0.90 | Avg loss: 0.0532 | Epsilon: 0.010\n",
            "Episode 708/5000 | Reward: 3.10 | Avg loss: 0.0502 | Epsilon: 0.010\n",
            "Episode 709/5000 | Reward: 2.10 | Avg loss: 0.0642 | Epsilon: 0.010\n",
            "Episode 710/5000 | Reward: 2.20 | Avg loss: 0.0553 | Epsilon: 0.010\n",
            "Episode 711/5000 | Reward: -0.90 | Avg loss: 0.0598 | Epsilon: 0.010\n",
            "Episode 712/5000 | Reward: 0.90 | Avg loss: 0.0565 | Epsilon: 0.010\n",
            "Episode 713/5000 | Reward: 0.60 | Avg loss: 0.0566 | Epsilon: 0.010\n",
            "Episode 714/5000 | Reward: 2.80 | Avg loss: 0.0614 | Epsilon: 0.010\n",
            "Episode 715/5000 | Reward: 2.80 | Avg loss: 0.0514 | Epsilon: 0.010\n",
            "Episode 716/5000 | Reward: 0.50 | Avg loss: 0.0492 | Epsilon: 0.010\n",
            "Episode 717/5000 | Reward: 5.50 | Avg loss: 0.0521 | Epsilon: 0.010\n",
            "Episode 718/5000 | Reward: 3.20 | Avg loss: 0.0534 | Epsilon: 0.010\n",
            "Episode 719/5000 | Reward: 2.70 | Avg loss: 0.0575 | Epsilon: 0.010\n",
            "Episode 720/5000 | Reward: 2.80 | Avg loss: 0.0595 | Epsilon: 0.010\n",
            "Episode 721/5000 | Reward: 2.70 | Avg loss: 0.0631 | Epsilon: 0.010\n",
            "Episode 722/5000 | Reward: 2.80 | Avg loss: 0.0538 | Epsilon: 0.010\n",
            "Episode 723/5000 | Reward: 2.90 | Avg loss: 0.0564 | Epsilon: 0.010\n",
            "Episode 724/5000 | Reward: 2.70 | Avg loss: 0.0604 | Epsilon: 0.010\n",
            "Episode 725/5000 | Reward: -0.30 | Avg loss: 0.0584 | Epsilon: 0.010\n",
            "Episode 726/5000 | Reward: 2.80 | Avg loss: 0.0619 | Epsilon: 0.010\n",
            "Episode 727/5000 | Reward: 2.30 | Avg loss: 0.0632 | Epsilon: 0.010\n",
            "Episode 728/5000 | Reward: 2.40 | Avg loss: 0.0489 | Epsilon: 0.010\n",
            "Episode 729/5000 | Reward: 3.20 | Avg loss: 0.0519 | Epsilon: 0.010\n",
            "Episode 730/5000 | Reward: 2.10 | Avg loss: 0.0561 | Epsilon: 0.010\n",
            "Episode 731/5000 | Reward: 3.00 | Avg loss: 0.0469 | Epsilon: 0.010\n",
            "Episode 732/5000 | Reward: 2.90 | Avg loss: 0.0477 | Epsilon: 0.010\n",
            "Episode 733/5000 | Reward: 2.70 | Avg loss: 0.0513 | Epsilon: 0.010\n",
            "Episode 734/5000 | Reward: 7.70 | Avg loss: 0.0645 | Epsilon: 0.010\n",
            "Episode 735/5000 | Reward: -0.90 | Avg loss: 0.0554 | Epsilon: 0.010\n",
            "Episode 736/5000 | Reward: 2.80 | Avg loss: 0.0613 | Epsilon: 0.010\n",
            "Episode 737/5000 | Reward: 2.50 | Avg loss: 0.0509 | Epsilon: 0.010\n",
            "Episode 738/5000 | Reward: 2.50 | Avg loss: 0.0465 | Epsilon: 0.010\n",
            "Episode 739/5000 | Reward: -0.90 | Avg loss: 0.0423 | Epsilon: 0.010\n",
            "Episode 740/5000 | Reward: 0.50 | Avg loss: 0.0512 | Epsilon: 0.010\n",
            "Episode 741/5000 | Reward: 3.10 | Avg loss: 0.0664 | Epsilon: 0.010\n",
            "Episode 742/5000 | Reward: 2.70 | Avg loss: 0.0512 | Epsilon: 0.010\n",
            "Episode 743/5000 | Reward: 1.50 | Avg loss: 0.0436 | Epsilon: 0.010\n",
            "Episode 744/5000 | Reward: 0.30 | Avg loss: 0.0452 | Epsilon: 0.010\n",
            "Episode 745/5000 | Reward: 2.50 | Avg loss: 0.0518 | Epsilon: 0.010\n",
            "Episode 746/5000 | Reward: 2.70 | Avg loss: 0.0545 | Epsilon: 0.010\n",
            "Episode 747/5000 | Reward: 2.20 | Avg loss: 0.0539 | Epsilon: 0.010\n",
            "Episode 748/5000 | Reward: -0.90 | Avg loss: 0.0478 | Epsilon: 0.010\n",
            "Episode 749/5000 | Reward: 2.40 | Avg loss: 0.0449 | Epsilon: 0.010\n",
            "Episode 750/5000 | Reward: 2.30 | Avg loss: 0.0501 | Epsilon: 0.010\n",
            "Episode 751/5000 | Reward: -0.20 | Avg loss: 0.0505 | Epsilon: 0.010\n",
            "Episode 752/5000 | Reward: 2.40 | Avg loss: 0.0395 | Epsilon: 0.010\n",
            "Episode 753/5000 | Reward: -0.90 | Avg loss: 0.0368 | Epsilon: 0.010\n",
            "Episode 754/5000 | Reward: 6.20 | Avg loss: 0.0452 | Epsilon: 0.010\n",
            "Episode 755/5000 | Reward: 2.30 | Avg loss: 0.0529 | Epsilon: 0.010\n",
            "Episode 756/5000 | Reward: 2.10 | Avg loss: 0.0538 | Epsilon: 0.010\n",
            "Episode 757/5000 | Reward: -0.90 | Avg loss: 0.0437 | Epsilon: 0.010\n",
            "Episode 758/5000 | Reward: 4.70 | Avg loss: 0.0478 | Epsilon: 0.010\n",
            "Episode 759/5000 | Reward: 6.50 | Avg loss: 0.0502 | Epsilon: 0.010\n",
            "Episode 760/5000 | Reward: 2.90 | Avg loss: 0.0526 | Epsilon: 0.010\n",
            "Episode 761/5000 | Reward: 1.30 | Avg loss: 0.0477 | Epsilon: 0.010\n",
            "Episode 762/5000 | Reward: -0.60 | Avg loss: 0.0477 | Epsilon: 0.010\n",
            "Episode 763/5000 | Reward: -0.90 | Avg loss: 0.0531 | Epsilon: 0.010\n",
            "Episode 764/5000 | Reward: 2.40 | Avg loss: 0.0577 | Epsilon: 0.010\n",
            "Episode 765/5000 | Reward: -0.90 | Avg loss: 0.0637 | Epsilon: 0.010\n",
            "Episode 766/5000 | Reward: 3.80 | Avg loss: 0.0645 | Epsilon: 0.010\n",
            "Episode 767/5000 | Reward: 1.20 | Avg loss: 0.0553 | Epsilon: 0.010\n",
            "Episode 768/5000 | Reward: 0.80 | Avg loss: 0.0487 | Epsilon: 0.010\n",
            "Episode 769/5000 | Reward: 1.00 | Avg loss: 0.0497 | Epsilon: 0.010\n",
            "Episode 770/5000 | Reward: 2.70 | Avg loss: 0.0519 | Epsilon: 0.010\n",
            "Episode 771/5000 | Reward: 0.30 | Avg loss: 0.0438 | Epsilon: 0.010\n",
            "Episode 772/5000 | Reward: -0.90 | Avg loss: 0.0469 | Epsilon: 0.010\n",
            "Episode 773/5000 | Reward: -1.00 | Avg loss: 0.0506 | Epsilon: 0.010\n",
            "Episode 774/5000 | Reward: 2.10 | Avg loss: 0.0493 | Epsilon: 0.010\n",
            "Episode 775/5000 | Reward: -0.90 | Avg loss: 0.0485 | Epsilon: 0.010\n",
            "Episode 776/5000 | Reward: -1.70 | Avg loss: 0.0481 | Epsilon: 0.010\n",
            "Episode 777/5000 | Reward: 2.30 | Avg loss: 0.0621 | Epsilon: 0.010\n",
            "Episode 778/5000 | Reward: 3.00 | Avg loss: 0.0664 | Epsilon: 0.010\n",
            "Episode 779/5000 | Reward: 0.80 | Avg loss: 0.0652 | Epsilon: 0.010\n",
            "Episode 780/5000 | Reward: 2.70 | Avg loss: 0.0706 | Epsilon: 0.010\n",
            "Episode 781/5000 | Reward: 2.50 | Avg loss: 0.0680 | Epsilon: 0.010\n",
            "Episode 782/5000 | Reward: 0.00 | Avg loss: 0.0558 | Epsilon: 0.010\n",
            "Episode 783/5000 | Reward: 0.60 | Avg loss: 0.0554 | Epsilon: 0.010\n",
            "Episode 784/5000 | Reward: 1.50 | Avg loss: 0.0619 | Epsilon: 0.010\n",
            "Episode 785/5000 | Reward: 2.80 | Avg loss: 0.0483 | Epsilon: 0.010\n",
            "Episode 786/5000 | Reward: 3.20 | Avg loss: 0.0502 | Epsilon: 0.010\n",
            "Episode 787/5000 | Reward: 1.50 | Avg loss: 0.0477 | Epsilon: 0.010\n",
            "Episode 788/5000 | Reward: 2.80 | Avg loss: 0.0574 | Epsilon: 0.010\n",
            "Episode 789/5000 | Reward: 2.90 | Avg loss: 0.0488 | Epsilon: 0.010\n",
            "Episode 790/5000 | Reward: 2.70 | Avg loss: 0.0497 | Epsilon: 0.010\n",
            "Episode 791/5000 | Reward: 3.10 | Avg loss: 0.0538 | Epsilon: 0.010\n",
            "Episode 792/5000 | Reward: -0.90 | Avg loss: 0.0427 | Epsilon: 0.010\n",
            "Episode 793/5000 | Reward: 2.20 | Avg loss: 0.0527 | Epsilon: 0.010\n",
            "Episode 794/5000 | Reward: -1.40 | Avg loss: 0.0480 | Epsilon: 0.010\n",
            "Episode 795/5000 | Reward: -0.50 | Avg loss: 0.0629 | Epsilon: 0.010\n",
            "Episode 796/5000 | Reward: -0.50 | Avg loss: 0.0474 | Epsilon: 0.010\n",
            "Episode 797/5000 | Reward: 2.90 | Avg loss: 0.0526 | Epsilon: 0.010\n",
            "Episode 798/5000 | Reward: 1.60 | Avg loss: 0.0544 | Epsilon: 0.010\n",
            "Episode 799/5000 | Reward: 2.70 | Avg loss: 0.0559 | Epsilon: 0.010\n",
            "Episode 800/5000 | Reward: -1.00 | Avg loss: 0.0532 | Epsilon: 0.010\n",
            "Episode 801/5000 | Reward: 1.90 | Avg loss: 0.0492 | Epsilon: 0.010\n",
            "Episode 802/5000 | Reward: 2.10 | Avg loss: 0.0645 | Epsilon: 0.010\n",
            "Episode 803/5000 | Reward: 2.20 | Avg loss: 0.0549 | Epsilon: 0.010\n",
            "Episode 804/5000 | Reward: 3.00 | Avg loss: 0.0516 | Epsilon: 0.010\n",
            "Episode 805/5000 | Reward: 3.00 | Avg loss: 0.0504 | Epsilon: 0.010\n",
            "Episode 806/5000 | Reward: 2.70 | Avg loss: 0.0394 | Epsilon: 0.010\n",
            "Episode 807/5000 | Reward: 3.00 | Avg loss: 0.0408 | Epsilon: 0.010\n",
            "Episode 808/5000 | Reward: 2.70 | Avg loss: 0.0391 | Epsilon: 0.010\n",
            "Episode 809/5000 | Reward: 5.30 | Avg loss: 0.0387 | Epsilon: 0.010\n",
            "Episode 810/5000 | Reward: -1.10 | Avg loss: 0.0412 | Epsilon: 0.010\n",
            "Episode 811/5000 | Reward: 4.10 | Avg loss: 0.0494 | Epsilon: 0.010\n",
            "Episode 812/5000 | Reward: 4.30 | Avg loss: 0.0417 | Epsilon: 0.010\n",
            "Episode 813/5000 | Reward: 0.60 | Avg loss: 0.0443 | Epsilon: 0.010\n",
            "Episode 814/5000 | Reward: 4.30 | Avg loss: 0.0401 | Epsilon: 0.010\n",
            "Episode 815/5000 | Reward: -7.30 | Avg loss: 0.0497 | Epsilon: 0.010\n",
            "Episode 816/5000 | Reward: 3.00 | Avg loss: 0.0533 | Epsilon: 0.010\n",
            "Episode 817/5000 | Reward: -0.90 | Avg loss: 0.0550 | Epsilon: 0.010\n",
            "Episode 818/5000 | Reward: -0.90 | Avg loss: 0.0536 | Epsilon: 0.010\n",
            "Episode 819/5000 | Reward: 2.90 | Avg loss: 0.0714 | Epsilon: 0.010\n",
            "Episode 820/5000 | Reward: 5.90 | Avg loss: 0.0686 | Epsilon: 0.010\n",
            "Episode 821/5000 | Reward: 3.80 | Avg loss: 0.0777 | Epsilon: 0.010\n",
            "Episode 822/5000 | Reward: -0.90 | Avg loss: 0.0663 | Epsilon: 0.010\n",
            "Episode 823/5000 | Reward: 2.40 | Avg loss: 0.0667 | Epsilon: 0.010\n",
            "Episode 824/5000 | Reward: 3.00 | Avg loss: 0.0726 | Epsilon: 0.010\n",
            "Episode 825/5000 | Reward: 2.40 | Avg loss: 0.0820 | Epsilon: 0.010\n",
            "Episode 826/5000 | Reward: 0.30 | Avg loss: 0.0746 | Epsilon: 0.010\n",
            "Episode 827/5000 | Reward: 4.70 | Avg loss: 0.0563 | Epsilon: 0.010\n",
            "Episode 828/5000 | Reward: 2.90 | Avg loss: 0.0584 | Epsilon: 0.010\n",
            "Episode 829/5000 | Reward: -0.90 | Avg loss: 0.0709 | Epsilon: 0.010\n",
            "Episode 830/5000 | Reward: -0.90 | Avg loss: 0.0657 | Epsilon: 0.010\n",
            "Episode 831/5000 | Reward: -0.30 | Avg loss: 0.0747 | Epsilon: 0.010\n",
            "Episode 832/5000 | Reward: 6.10 | Avg loss: 0.0682 | Epsilon: 0.010\n",
            "Episode 833/5000 | Reward: 0.50 | Avg loss: 0.0553 | Epsilon: 0.010\n",
            "Episode 834/5000 | Reward: -0.90 | Avg loss: 0.0569 | Epsilon: 0.010\n",
            "Episode 835/5000 | Reward: 1.30 | Avg loss: 0.0580 | Epsilon: 0.010\n",
            "Episode 836/5000 | Reward: 2.90 | Avg loss: 0.0599 | Epsilon: 0.010\n",
            "Episode 837/5000 | Reward: 2.80 | Avg loss: 0.0527 | Epsilon: 0.010\n",
            "Episode 838/5000 | Reward: 2.80 | Avg loss: 0.0455 | Epsilon: 0.010\n",
            "Episode 839/5000 | Reward: 2.90 | Avg loss: 0.0515 | Epsilon: 0.010\n",
            "Episode 840/5000 | Reward: 2.40 | Avg loss: 0.0632 | Epsilon: 0.010\n",
            "Episode 841/5000 | Reward: 2.40 | Avg loss: 0.0663 | Epsilon: 0.010\n",
            "Episode 842/5000 | Reward: 2.50 | Avg loss: 0.0672 | Epsilon: 0.010\n",
            "Episode 843/5000 | Reward: 1.70 | Avg loss: 0.0703 | Epsilon: 0.010\n",
            "Episode 844/5000 | Reward: 2.70 | Avg loss: 0.0622 | Epsilon: 0.010\n",
            "Episode 845/5000 | Reward: 0.70 | Avg loss: 0.0741 | Epsilon: 0.010\n",
            "Episode 846/5000 | Reward: 2.70 | Avg loss: 0.0646 | Epsilon: 0.010\n",
            "Episode 847/5000 | Reward: 3.00 | Avg loss: 0.0626 | Epsilon: 0.010\n",
            "Episode 848/5000 | Reward: 2.90 | Avg loss: 0.0632 | Epsilon: 0.010\n",
            "Episode 849/5000 | Reward: 1.00 | Avg loss: 0.0713 | Epsilon: 0.010\n",
            "Episode 850/5000 | Reward: 2.40 | Avg loss: 0.0671 | Epsilon: 0.010\n",
            "Episode 851/5000 | Reward: 0.60 | Avg loss: 0.0587 | Epsilon: 0.010\n",
            "Episode 852/5000 | Reward: 2.70 | Avg loss: 0.0643 | Epsilon: 0.010\n",
            "Episode 853/5000 | Reward: 2.70 | Avg loss: 0.0609 | Epsilon: 0.010\n",
            "Episode 854/5000 | Reward: 2.50 | Avg loss: 0.0865 | Epsilon: 0.010\n",
            "Episode 855/5000 | Reward: 1.00 | Avg loss: 0.0734 | Epsilon: 0.010\n",
            "Episode 856/5000 | Reward: 2.70 | Avg loss: 0.0632 | Epsilon: 0.010\n",
            "Episode 857/5000 | Reward: 2.30 | Avg loss: 0.0593 | Epsilon: 0.010\n",
            "Episode 858/5000 | Reward: 2.20 | Avg loss: 0.0682 | Epsilon: 0.010\n",
            "Episode 859/5000 | Reward: 3.20 | Avg loss: 0.0695 | Epsilon: 0.010\n",
            "Episode 860/5000 | Reward: 3.00 | Avg loss: 0.0543 | Epsilon: 0.010\n",
            "Episode 861/5000 | Reward: 4.90 | Avg loss: 0.0632 | Epsilon: 0.010\n",
            "Episode 862/5000 | Reward: 2.80 | Avg loss: 0.0630 | Epsilon: 0.010\n",
            "Episode 863/5000 | Reward: 2.80 | Avg loss: 0.0614 | Epsilon: 0.010\n",
            "Episode 864/5000 | Reward: 0.90 | Avg loss: 0.0631 | Epsilon: 0.010\n",
            "Episode 865/5000 | Reward: -0.30 | Avg loss: 0.0750 | Epsilon: 0.010\n",
            "Episode 866/5000 | Reward: 4.30 | Avg loss: 0.0641 | Epsilon: 0.010\n",
            "Episode 867/5000 | Reward: 3.60 | Avg loss: 0.0598 | Epsilon: 0.010\n",
            "Episode 868/5000 | Reward: -1.70 | Avg loss: 0.0635 | Epsilon: 0.010\n",
            "Episode 869/5000 | Reward: 0.30 | Avg loss: 0.0634 | Epsilon: 0.010\n",
            "Episode 870/5000 | Reward: 2.20 | Avg loss: 0.0537 | Epsilon: 0.010\n",
            "Episode 871/5000 | Reward: 3.50 | Avg loss: 0.0544 | Epsilon: 0.010\n",
            "Episode 872/5000 | Reward: 2.70 | Avg loss: 0.0448 | Epsilon: 0.010\n",
            "Episode 873/5000 | Reward: 2.90 | Avg loss: 0.0421 | Epsilon: 0.010\n",
            "Episode 874/5000 | Reward: 2.80 | Avg loss: 0.0491 | Epsilon: 0.010\n",
            "Episode 875/5000 | Reward: 8.80 | Avg loss: 0.0519 | Epsilon: 0.010\n",
            "Episode 876/5000 | Reward: 2.90 | Avg loss: 0.0671 | Epsilon: 0.010\n",
            "Episode 877/5000 | Reward: 7.10 | Avg loss: 0.0578 | Epsilon: 0.010\n",
            "Episode 878/5000 | Reward: 4.40 | Avg loss: 0.0642 | Epsilon: 0.010\n",
            "Episode 879/5000 | Reward: -0.90 | Avg loss: 0.0793 | Epsilon: 0.010\n",
            "Episode 880/5000 | Reward: 3.00 | Avg loss: 0.0728 | Epsilon: 0.010\n",
            "Episode 881/5000 | Reward: 3.10 | Avg loss: 0.0685 | Epsilon: 0.010\n",
            "Episode 882/5000 | Reward: 2.60 | Avg loss: 0.0758 | Epsilon: 0.010\n",
            "Episode 883/5000 | Reward: 0.30 | Avg loss: 0.0638 | Epsilon: 0.010\n",
            "Episode 884/5000 | Reward: 2.20 | Avg loss: 0.0741 | Epsilon: 0.010\n",
            "Episode 885/5000 | Reward: 2.50 | Avg loss: 0.0660 | Epsilon: 0.010\n",
            "Episode 886/5000 | Reward: 1.80 | Avg loss: 0.0643 | Epsilon: 0.010\n",
            "Episode 887/5000 | Reward: 2.60 | Avg loss: 0.0740 | Epsilon: 0.010\n",
            "Episode 888/5000 | Reward: 2.70 | Avg loss: 0.0746 | Epsilon: 0.010\n",
            "Episode 889/5000 | Reward: 0.90 | Avg loss: 0.0716 | Epsilon: 0.010\n",
            "Episode 890/5000 | Reward: -0.90 | Avg loss: 0.0647 | Epsilon: 0.010\n",
            "Episode 891/5000 | Reward: 4.80 | Avg loss: 0.0851 | Epsilon: 0.010\n",
            "Episode 892/5000 | Reward: 2.90 | Avg loss: 0.0750 | Epsilon: 0.010\n",
            "Episode 893/5000 | Reward: 2.40 | Avg loss: 0.0836 | Epsilon: 0.010\n",
            "Episode 894/5000 | Reward: -0.90 | Avg loss: 0.0778 | Epsilon: 0.010\n",
            "Episode 895/5000 | Reward: 2.80 | Avg loss: 0.0644 | Epsilon: 0.010\n",
            "Episode 896/5000 | Reward: -6.30 | Avg loss: 0.0860 | Epsilon: 0.010\n",
            "Episode 897/5000 | Reward: -0.10 | Avg loss: 0.0818 | Epsilon: 0.010\n",
            "Episode 898/5000 | Reward: 3.00 | Avg loss: 0.0642 | Epsilon: 0.010\n",
            "Episode 899/5000 | Reward: 2.70 | Avg loss: 0.0687 | Epsilon: 0.010\n",
            "Episode 900/5000 | Reward: 2.40 | Avg loss: 0.0669 | Epsilon: 0.010\n",
            "Episode 901/5000 | Reward: 2.80 | Avg loss: 0.0578 | Epsilon: 0.010\n",
            "Episode 902/5000 | Reward: -0.10 | Avg loss: 0.0687 | Epsilon: 0.010\n",
            "Episode 903/5000 | Reward: -0.40 | Avg loss: 0.0739 | Epsilon: 0.010\n",
            "Episode 904/5000 | Reward: 2.70 | Avg loss: 0.0847 | Epsilon: 0.010\n",
            "Episode 905/5000 | Reward: 2.90 | Avg loss: 0.0760 | Epsilon: 0.010\n",
            "Episode 906/5000 | Reward: 1.90 | Avg loss: 0.0755 | Epsilon: 0.010\n",
            "Episode 907/5000 | Reward: 2.90 | Avg loss: 0.0683 | Epsilon: 0.010\n",
            "Episode 908/5000 | Reward: 4.30 | Avg loss: 0.0747 | Epsilon: 0.010\n",
            "Episode 909/5000 | Reward: -0.10 | Avg loss: 0.0724 | Epsilon: 0.010\n",
            "Episode 910/5000 | Reward: 2.50 | Avg loss: 0.0678 | Epsilon: 0.010\n",
            "Episode 911/5000 | Reward: 0.60 | Avg loss: 0.0617 | Epsilon: 0.010\n",
            "Episode 912/5000 | Reward: 3.20 | Avg loss: 0.0765 | Epsilon: 0.010\n",
            "Episode 913/5000 | Reward: 3.90 | Avg loss: 0.0598 | Epsilon: 0.010\n",
            "Episode 914/5000 | Reward: 3.40 | Avg loss: 0.0741 | Epsilon: 0.010\n",
            "Episode 915/5000 | Reward: 2.50 | Avg loss: 0.0675 | Epsilon: 0.010\n",
            "Episode 916/5000 | Reward: 2.90 | Avg loss: 0.0623 | Epsilon: 0.010\n",
            "Episode 917/5000 | Reward: 2.30 | Avg loss: 0.0726 | Epsilon: 0.010\n",
            "Episode 918/5000 | Reward: 4.30 | Avg loss: 0.0742 | Epsilon: 0.010\n",
            "Episode 919/5000 | Reward: 5.50 | Avg loss: 0.0718 | Epsilon: 0.010\n",
            "Episode 920/5000 | Reward: 0.30 | Avg loss: 0.0696 | Epsilon: 0.010\n",
            "Episode 921/5000 | Reward: 3.20 | Avg loss: 0.0664 | Epsilon: 0.010\n",
            "Episode 922/5000 | Reward: 2.70 | Avg loss: 0.0711 | Epsilon: 0.010\n",
            "Episode 923/5000 | Reward: 11.00 | Avg loss: 0.0641 | Epsilon: 0.010\n",
            "Episode 924/5000 | Reward: 3.00 | Avg loss: 0.0703 | Epsilon: 0.010\n",
            "Episode 925/5000 | Reward: 3.20 | Avg loss: 0.0653 | Epsilon: 0.010\n",
            "Episode 926/5000 | Reward: 2.90 | Avg loss: 0.0648 | Epsilon: 0.010\n",
            "Episode 927/5000 | Reward: 2.80 | Avg loss: 0.0615 | Epsilon: 0.010\n",
            "Episode 928/5000 | Reward: -0.90 | Avg loss: 0.0628 | Epsilon: 0.010\n",
            "Episode 929/5000 | Reward: 1.90 | Avg loss: 0.0593 | Epsilon: 0.010\n",
            "Episode 930/5000 | Reward: -0.90 | Avg loss: 0.0604 | Epsilon: 0.010\n",
            "Episode 931/5000 | Reward: 1.10 | Avg loss: 0.0617 | Epsilon: 0.010\n",
            "Episode 932/5000 | Reward: 2.90 | Avg loss: 0.0646 | Epsilon: 0.010\n",
            "Episode 933/5000 | Reward: 1.00 | Avg loss: 0.0677 | Epsilon: 0.010\n",
            "Episode 934/5000 | Reward: 5.80 | Avg loss: 0.0626 | Epsilon: 0.010\n",
            "Episode 935/5000 | Reward: 2.10 | Avg loss: 0.0548 | Epsilon: 0.010\n",
            "Episode 936/5000 | Reward: -0.40 | Avg loss: 0.0720 | Epsilon: 0.010\n",
            "Episode 937/5000 | Reward: 3.00 | Avg loss: 0.0692 | Epsilon: 0.010\n",
            "Episode 938/5000 | Reward: 0.40 | Avg loss: 0.0610 | Epsilon: 0.010\n",
            "Episode 939/5000 | Reward: 1.50 | Avg loss: 0.0575 | Epsilon: 0.010\n",
            "Episode 940/5000 | Reward: 3.00 | Avg loss: 0.0552 | Epsilon: 0.010\n",
            "Episode 941/5000 | Reward: 2.30 | Avg loss: 0.0722 | Epsilon: 0.010\n",
            "Episode 942/5000 | Reward: 2.90 | Avg loss: 0.0605 | Epsilon: 0.010\n",
            "Episode 943/5000 | Reward: 6.30 | Avg loss: 0.0688 | Epsilon: 0.010\n",
            "Episode 944/5000 | Reward: 2.60 | Avg loss: 0.0477 | Epsilon: 0.010\n",
            "Episode 945/5000 | Reward: 3.10 | Avg loss: 0.0616 | Epsilon: 0.010\n",
            "Episode 946/5000 | Reward: 3.90 | Avg loss: 0.0688 | Epsilon: 0.010\n",
            "Episode 947/5000 | Reward: -5.70 | Avg loss: 0.0680 | Epsilon: 0.010\n",
            "Episode 948/5000 | Reward: 2.30 | Avg loss: 0.0600 | Epsilon: 0.010\n",
            "Episode 949/5000 | Reward: 2.40 | Avg loss: 0.0644 | Epsilon: 0.010\n",
            "Episode 950/5000 | Reward: 2.90 | Avg loss: 0.0701 | Epsilon: 0.010\n",
            "Episode 951/5000 | Reward: -6.90 | Avg loss: 0.0569 | Epsilon: 0.010\n",
            "Episode 952/5000 | Reward: 2.90 | Avg loss: 0.0640 | Epsilon: 0.010\n",
            "Episode 953/5000 | Reward: 14.20 | Avg loss: 0.0644 | Epsilon: 0.010\n",
            "Episode 954/5000 | Reward: 3.00 | Avg loss: 0.0657 | Epsilon: 0.010\n",
            "Episode 955/5000 | Reward: 2.70 | Avg loss: 0.0718 | Epsilon: 0.010\n",
            "Episode 956/5000 | Reward: 0.30 | Avg loss: 0.0572 | Epsilon: 0.010\n",
            "Episode 957/5000 | Reward: 2.70 | Avg loss: 0.0570 | Epsilon: 0.010\n",
            "Episode 958/5000 | Reward: -0.90 | Avg loss: 0.0677 | Epsilon: 0.010\n",
            "Episode 959/5000 | Reward: 0.00 | Avg loss: 0.0763 | Epsilon: 0.010\n",
            "Episode 960/5000 | Reward: 2.90 | Avg loss: 0.0607 | Epsilon: 0.010\n",
            "Episode 961/5000 | Reward: 6.90 | Avg loss: 0.0697 | Epsilon: 0.010\n",
            "Episode 962/5000 | Reward: 2.80 | Avg loss: 0.0754 | Epsilon: 0.010\n",
            "Episode 963/5000 | Reward: -6.90 | Avg loss: 0.0778 | Epsilon: 0.010\n",
            "Episode 964/5000 | Reward: 3.20 | Avg loss: 0.0717 | Epsilon: 0.010\n",
            "Episode 965/5000 | Reward: 3.00 | Avg loss: 0.0822 | Epsilon: 0.010\n",
            "Episode 966/5000 | Reward: 1.40 | Avg loss: 0.0671 | Epsilon: 0.010\n",
            "Episode 967/5000 | Reward: 5.20 | Avg loss: 0.0574 | Epsilon: 0.010\n",
            "Episode 968/5000 | Reward: 5.10 | Avg loss: 0.0686 | Epsilon: 0.010\n",
            "Episode 969/5000 | Reward: 2.20 | Avg loss: 0.0541 | Epsilon: 0.010\n",
            "Episode 970/5000 | Reward: 2.70 | Avg loss: 0.0631 | Epsilon: 0.010\n",
            "Episode 971/5000 | Reward: 3.00 | Avg loss: 0.0619 | Epsilon: 0.010\n",
            "Episode 972/5000 | Reward: 8.70 | Avg loss: 0.0551 | Epsilon: 0.010\n",
            "Episode 973/5000 | Reward: 0.20 | Avg loss: 0.0546 | Epsilon: 0.010\n",
            "Episode 974/5000 | Reward: 3.00 | Avg loss: 0.0474 | Epsilon: 0.010\n",
            "Episode 975/5000 | Reward: -0.90 | Avg loss: 0.0570 | Epsilon: 0.010\n",
            "Episode 976/5000 | Reward: 12.20 | Avg loss: 0.0625 | Epsilon: 0.010\n",
            "Episode 977/5000 | Reward: 2.30 | Avg loss: 0.0702 | Epsilon: 0.010\n",
            "Episode 978/5000 | Reward: -0.90 | Avg loss: 0.0612 | Epsilon: 0.010\n",
            "Episode 979/5000 | Reward: 2.90 | Avg loss: 0.0530 | Epsilon: 0.010\n",
            "Episode 980/5000 | Reward: 3.10 | Avg loss: 0.0663 | Epsilon: 0.010\n",
            "Episode 981/5000 | Reward: -6.30 | Avg loss: 0.0510 | Epsilon: 0.010\n",
            "Episode 982/5000 | Reward: 5.30 | Avg loss: 0.0577 | Epsilon: 0.010\n",
            "Episode 983/5000 | Reward: 1.00 | Avg loss: 0.0540 | Epsilon: 0.010\n",
            "Episode 984/5000 | Reward: 4.90 | Avg loss: 0.0577 | Epsilon: 0.010\n",
            "Episode 985/5000 | Reward: 2.90 | Avg loss: 0.0690 | Epsilon: 0.010\n",
            "Episode 986/5000 | Reward: 2.80 | Avg loss: 0.0502 | Epsilon: 0.010\n",
            "Episode 987/5000 | Reward: 3.60 | Avg loss: 0.0608 | Epsilon: 0.010\n",
            "Episode 988/5000 | Reward: 2.40 | Avg loss: 0.0578 | Epsilon: 0.010\n",
            "Episode 989/5000 | Reward: 3.00 | Avg loss: 0.0560 | Epsilon: 0.010\n",
            "Episode 990/5000 | Reward: 4.20 | Avg loss: 0.0675 | Epsilon: 0.010\n",
            "Episode 991/5000 | Reward: 1.70 | Avg loss: 0.0587 | Epsilon: 0.010\n",
            "Episode 992/5000 | Reward: 2.50 | Avg loss: 0.0662 | Epsilon: 0.010\n",
            "Episode 993/5000 | Reward: -0.60 | Avg loss: 0.0587 | Epsilon: 0.010\n",
            "Episode 994/5000 | Reward: 0.80 | Avg loss: 0.0665 | Epsilon: 0.010\n",
            "Episode 995/5000 | Reward: 2.80 | Avg loss: 0.0742 | Epsilon: 0.010\n",
            "Episode 996/5000 | Reward: 2.80 | Avg loss: 0.0767 | Epsilon: 0.010\n",
            "Episode 997/5000 | Reward: 2.30 | Avg loss: 0.0714 | Epsilon: 0.010\n",
            "Episode 998/5000 | Reward: 3.10 | Avg loss: 0.0670 | Epsilon: 0.010\n",
            "Episode 999/5000 | Reward: 2.90 | Avg loss: 0.0723 | Epsilon: 0.010\n",
            "Episode 1000/5000 | Reward: 5.10 | Avg loss: 0.0732 | Epsilon: 0.010\n",
            "Episode 1001/5000 | Reward: 2.70 | Avg loss: 0.0826 | Epsilon: 0.010\n",
            "Episode 1002/5000 | Reward: 1.50 | Avg loss: 0.0925 | Epsilon: 0.010\n",
            "Episode 1003/5000 | Reward: 2.90 | Avg loss: 0.0770 | Epsilon: 0.010\n",
            "Episode 1004/5000 | Reward: -0.90 | Avg loss: 0.0828 | Epsilon: 0.010\n",
            "Episode 1005/5000 | Reward: 2.20 | Avg loss: 0.0813 | Epsilon: 0.010\n",
            "Episode 1006/5000 | Reward: 2.20 | Avg loss: 0.0640 | Epsilon: 0.010\n",
            "Episode 1007/5000 | Reward: 3.40 | Avg loss: 0.0779 | Epsilon: 0.010\n",
            "Episode 1008/5000 | Reward: 2.90 | Avg loss: 0.0535 | Epsilon: 0.010\n",
            "Episode 1009/5000 | Reward: 4.50 | Avg loss: 0.0622 | Epsilon: 0.010\n",
            "Episode 1010/5000 | Reward: 2.00 | Avg loss: 0.0713 | Epsilon: 0.010\n",
            "Episode 1011/5000 | Reward: -0.30 | Avg loss: 0.0589 | Epsilon: 0.010\n",
            "Episode 1012/5000 | Reward: 2.90 | Avg loss: 0.0626 | Epsilon: 0.010\n",
            "Episode 1013/5000 | Reward: 0.30 | Avg loss: 0.0618 | Epsilon: 0.010\n",
            "Episode 1014/5000 | Reward: 9.70 | Avg loss: 0.0622 | Epsilon: 0.010\n",
            "Episode 1015/5000 | Reward: 2.30 | Avg loss: 0.0578 | Epsilon: 0.010\n",
            "Episode 1016/5000 | Reward: -0.90 | Avg loss: 0.0526 | Epsilon: 0.010\n",
            "Episode 1017/5000 | Reward: 0.80 | Avg loss: 0.0547 | Epsilon: 0.010\n",
            "Episode 1018/5000 | Reward: 0.50 | Avg loss: 0.0549 | Epsilon: 0.010\n",
            "Episode 1019/5000 | Reward: 2.90 | Avg loss: 0.0543 | Epsilon: 0.010\n",
            "Episode 1020/5000 | Reward: 2.80 | Avg loss: 0.0530 | Epsilon: 0.010\n",
            "Episode 1021/5000 | Reward: -0.90 | Avg loss: 0.0545 | Epsilon: 0.010\n",
            "Episode 1022/5000 | Reward: 2.90 | Avg loss: 0.0640 | Epsilon: 0.010\n",
            "Episode 1023/5000 | Reward: -0.90 | Avg loss: 0.0548 | Epsilon: 0.010\n",
            "Episode 1024/5000 | Reward: 2.90 | Avg loss: 0.0682 | Epsilon: 0.010\n",
            "Episode 1025/5000 | Reward: 2.70 | Avg loss: 0.0598 | Epsilon: 0.010\n",
            "Episode 1026/5000 | Reward: 2.90 | Avg loss: 0.0736 | Epsilon: 0.010\n",
            "Episode 1027/5000 | Reward: 2.90 | Avg loss: 0.0690 | Epsilon: 0.010\n",
            "Episode 1028/5000 | Reward: 2.90 | Avg loss: 0.0753 | Epsilon: 0.010\n",
            "Episode 1029/5000 | Reward: 1.30 | Avg loss: 0.0761 | Epsilon: 0.010\n",
            "Episode 1030/5000 | Reward: 1.10 | Avg loss: 0.0588 | Epsilon: 0.010\n",
            "Episode 1031/5000 | Reward: 2.90 | Avg loss: 0.0620 | Epsilon: 0.010\n",
            "Episode 1032/5000 | Reward: 2.10 | Avg loss: 0.0706 | Epsilon: 0.010\n",
            "Episode 1033/5000 | Reward: 2.90 | Avg loss: 0.0728 | Epsilon: 0.010\n",
            "Episode 1034/5000 | Reward: -1.90 | Avg loss: 0.0876 | Epsilon: 0.010\n",
            "Episode 1035/5000 | Reward: 2.90 | Avg loss: 0.0519 | Epsilon: 0.010\n",
            "Episode 1036/5000 | Reward: 2.90 | Avg loss: 0.0617 | Epsilon: 0.010\n",
            "Episode 1037/5000 | Reward: -0.90 | Avg loss: 0.0819 | Epsilon: 0.010\n",
            "Episode 1038/5000 | Reward: 0.90 | Avg loss: 0.0791 | Epsilon: 0.010\n",
            "Episode 1039/5000 | Reward: 0.10 | Avg loss: 0.0590 | Epsilon: 0.010\n",
            "Episode 1040/5000 | Reward: 1.10 | Avg loss: 0.0628 | Epsilon: 0.010\n",
            "Episode 1041/5000 | Reward: 3.10 | Avg loss: 0.0636 | Epsilon: 0.010\n",
            "Episode 1042/5000 | Reward: -0.90 | Avg loss: 0.0624 | Epsilon: 0.010\n",
            "Episode 1043/5000 | Reward: 0.20 | Avg loss: 0.0599 | Epsilon: 0.010\n",
            "Episode 1044/5000 | Reward: 3.90 | Avg loss: 0.0543 | Epsilon: 0.010\n",
            "Episode 1045/5000 | Reward: -0.60 | Avg loss: 0.0634 | Epsilon: 0.010\n",
            "Episode 1046/5000 | Reward: 2.40 | Avg loss: 0.0629 | Epsilon: 0.010\n",
            "Episode 1047/5000 | Reward: -0.90 | Avg loss: 0.0548 | Epsilon: 0.010\n",
            "Episode 1048/5000 | Reward: 3.60 | Avg loss: 0.0672 | Epsilon: 0.010\n",
            "Episode 1049/5000 | Reward: -0.90 | Avg loss: 0.0620 | Epsilon: 0.010\n",
            "Episode 1050/5000 | Reward: -0.90 | Avg loss: 0.0755 | Epsilon: 0.010\n",
            "Episode 1051/5000 | Reward: 2.20 | Avg loss: 0.0705 | Epsilon: 0.010\n",
            "Episode 1052/5000 | Reward: 2.70 | Avg loss: 0.0767 | Epsilon: 0.010\n",
            "Episode 1053/5000 | Reward: 2.90 | Avg loss: 0.0723 | Epsilon: 0.010\n",
            "Episode 1054/5000 | Reward: 7.60 | Avg loss: 0.0820 | Epsilon: 0.010\n",
            "Episode 1055/5000 | Reward: 2.90 | Avg loss: 0.0889 | Epsilon: 0.010\n",
            "Episode 1056/5000 | Reward: 0.30 | Avg loss: 0.0808 | Epsilon: 0.010\n",
            "Episode 1057/5000 | Reward: 14.00 | Avg loss: 0.0827 | Epsilon: 0.010\n",
            "Episode 1058/5000 | Reward: -0.10 | Avg loss: 0.0776 | Epsilon: 0.010\n",
            "Episode 1059/5000 | Reward: 2.90 | Avg loss: 0.0866 | Epsilon: 0.010\n",
            "Episode 1060/5000 | Reward: 4.90 | Avg loss: 0.0791 | Epsilon: 0.010\n",
            "Episode 1061/5000 | Reward: 2.90 | Avg loss: 0.0805 | Epsilon: 0.010\n",
            "Episode 1062/5000 | Reward: 3.10 | Avg loss: 0.0753 | Epsilon: 0.010\n",
            "Episode 1063/5000 | Reward: 2.30 | Avg loss: 0.0662 | Epsilon: 0.010\n",
            "Episode 1064/5000 | Reward: 2.80 | Avg loss: 0.0600 | Epsilon: 0.010\n",
            "Episode 1065/5000 | Reward: 2.80 | Avg loss: 0.0721 | Epsilon: 0.010\n",
            "Episode 1066/5000 | Reward: 2.70 | Avg loss: 0.0642 | Epsilon: 0.010\n",
            "Episode 1067/5000 | Reward: -0.60 | Avg loss: 0.0593 | Epsilon: 0.010\n",
            "Episode 1068/5000 | Reward: 3.00 | Avg loss: 0.0626 | Epsilon: 0.010\n",
            "Episode 1069/5000 | Reward: -0.90 | Avg loss: 0.0552 | Epsilon: 0.010\n",
            "Episode 1070/5000 | Reward: -0.90 | Avg loss: 0.0673 | Epsilon: 0.010\n",
            "Episode 1071/5000 | Reward: 3.10 | Avg loss: 0.0630 | Epsilon: 0.010\n",
            "Episode 1072/5000 | Reward: 6.60 | Avg loss: 0.0712 | Epsilon: 0.010\n",
            "Episode 1073/5000 | Reward: 3.00 | Avg loss: 0.0594 | Epsilon: 0.010\n",
            "Episode 1074/5000 | Reward: 2.20 | Avg loss: 0.0885 | Epsilon: 0.010\n",
            "Episode 1075/5000 | Reward: 0.10 | Avg loss: 0.0719 | Epsilon: 0.010\n",
            "Episode 1076/5000 | Reward: -0.30 | Avg loss: 0.0634 | Epsilon: 0.010\n",
            "Episode 1077/5000 | Reward: 0.80 | Avg loss: 0.0710 | Epsilon: 0.010\n",
            "Episode 1078/5000 | Reward: 2.00 | Avg loss: 0.0716 | Epsilon: 0.010\n",
            "Episode 1079/5000 | Reward: 3.00 | Avg loss: 0.0556 | Epsilon: 0.010\n",
            "Episode 1080/5000 | Reward: -0.90 | Avg loss: 0.0540 | Epsilon: 0.010\n",
            "Episode 1081/5000 | Reward: 2.30 | Avg loss: 0.0488 | Epsilon: 0.010\n",
            "Episode 1082/5000 | Reward: 3.00 | Avg loss: 0.0544 | Epsilon: 0.010\n",
            "Episode 1083/5000 | Reward: 2.90 | Avg loss: 0.0525 | Epsilon: 0.010\n",
            "Episode 1084/5000 | Reward: -0.90 | Avg loss: 0.0650 | Epsilon: 0.010\n",
            "Episode 1085/5000 | Reward: 2.20 | Avg loss: 0.0619 | Epsilon: 0.010\n",
            "Episode 1086/5000 | Reward: 2.10 | Avg loss: 0.0602 | Epsilon: 0.010\n",
            "Episode 1087/5000 | Reward: 2.40 | Avg loss: 0.0718 | Epsilon: 0.010\n",
            "Episode 1088/5000 | Reward: 2.40 | Avg loss: 0.0625 | Epsilon: 0.010\n",
            "Episode 1089/5000 | Reward: 2.60 | Avg loss: 0.0526 | Epsilon: 0.010\n",
            "Episode 1090/5000 | Reward: 2.80 | Avg loss: 0.0654 | Epsilon: 0.010\n",
            "Episode 1091/5000 | Reward: 3.00 | Avg loss: 0.0658 | Epsilon: 0.010\n",
            "Episode 1092/5000 | Reward: 4.20 | Avg loss: 0.0633 | Epsilon: 0.010\n",
            "Episode 1093/5000 | Reward: 2.90 | Avg loss: 0.0610 | Epsilon: 0.010\n",
            "Episode 1094/5000 | Reward: 2.80 | Avg loss: 0.0683 | Epsilon: 0.010\n",
            "Episode 1095/5000 | Reward: 2.80 | Avg loss: 0.0666 | Epsilon: 0.010\n",
            "Episode 1096/5000 | Reward: 2.70 | Avg loss: 0.0629 | Epsilon: 0.010\n",
            "Episode 1097/5000 | Reward: 2.90 | Avg loss: 0.0663 | Epsilon: 0.010\n",
            "Episode 1098/5000 | Reward: 2.90 | Avg loss: 0.0724 | Epsilon: 0.010\n",
            "Episode 1099/5000 | Reward: 3.10 | Avg loss: 0.0639 | Epsilon: 0.010\n",
            "Episode 1100/5000 | Reward: 2.40 | Avg loss: 0.0689 | Epsilon: 0.010\n",
            "Episode 1101/5000 | Reward: 4.30 | Avg loss: 0.0787 | Epsilon: 0.010\n",
            "Episode 1102/5000 | Reward: -0.90 | Avg loss: 0.0854 | Epsilon: 0.010\n",
            "Episode 1103/5000 | Reward: 6.20 | Avg loss: 0.0740 | Epsilon: 0.010\n",
            "Episode 1104/5000 | Reward: 2.90 | Avg loss: 0.0646 | Epsilon: 0.010\n",
            "Episode 1105/5000 | Reward: -5.70 | Avg loss: 0.0798 | Epsilon: 0.010\n",
            "Episode 1106/5000 | Reward: 6.50 | Avg loss: 0.0605 | Epsilon: 0.010\n",
            "Episode 1107/5000 | Reward: 0.30 | Avg loss: 0.0649 | Epsilon: 0.010\n",
            "Episode 1108/5000 | Reward: -0.90 | Avg loss: 0.0564 | Epsilon: 0.010\n",
            "Episode 1109/5000 | Reward: 3.00 | Avg loss: 0.0525 | Epsilon: 0.010\n",
            "Episode 1110/5000 | Reward: 2.90 | Avg loss: 0.0589 | Epsilon: 0.010\n",
            "Episode 1111/5000 | Reward: 3.10 | Avg loss: 0.0561 | Epsilon: 0.010\n",
            "Episode 1112/5000 | Reward: -0.90 | Avg loss: 0.0521 | Epsilon: 0.010\n",
            "Episode 1113/5000 | Reward: 2.60 | Avg loss: 0.0524 | Epsilon: 0.010\n",
            "Episode 1114/5000 | Reward: 4.90 | Avg loss: 0.0463 | Epsilon: 0.010\n",
            "Episode 1115/5000 | Reward: 3.00 | Avg loss: 0.0623 | Epsilon: 0.010\n",
            "Episode 1116/5000 | Reward: 2.50 | Avg loss: 0.0603 | Epsilon: 0.010\n",
            "Episode 1117/5000 | Reward: 2.30 | Avg loss: 0.0686 | Epsilon: 0.010\n",
            "Episode 1118/5000 | Reward: 2.30 | Avg loss: 0.0678 | Epsilon: 0.010\n",
            "Episode 1119/5000 | Reward: 3.00 | Avg loss: 0.0816 | Epsilon: 0.010\n",
            "Episode 1120/5000 | Reward: 3.20 | Avg loss: 0.0715 | Epsilon: 0.010\n",
            "Episode 1121/5000 | Reward: -0.90 | Avg loss: 0.0741 | Epsilon: 0.010\n",
            "Episode 1122/5000 | Reward: 2.80 | Avg loss: 0.0718 | Epsilon: 0.010\n",
            "Episode 1123/5000 | Reward: -3.60 | Avg loss: 0.0765 | Epsilon: 0.010\n",
            "Episode 1124/5000 | Reward: -1.10 | Avg loss: 0.0862 | Epsilon: 0.010\n",
            "Episode 1125/5000 | Reward: 0.00 | Avg loss: 0.0825 | Epsilon: 0.010\n",
            "Episode 1126/5000 | Reward: 8.00 | Avg loss: 0.0929 | Epsilon: 0.010\n",
            "Episode 1127/5000 | Reward: 2.90 | Avg loss: 0.0937 | Epsilon: 0.010\n",
            "Episode 1128/5000 | Reward: 4.90 | Avg loss: 0.1074 | Epsilon: 0.010\n",
            "Episode 1129/5000 | Reward: 5.10 | Avg loss: 0.0828 | Epsilon: 0.010\n",
            "Episode 1130/5000 | Reward: 3.50 | Avg loss: 0.0807 | Epsilon: 0.010\n",
            "Episode 1131/5000 | Reward: 2.80 | Avg loss: 0.0931 | Epsilon: 0.010\n",
            "Episode 1132/5000 | Reward: 2.90 | Avg loss: 0.0770 | Epsilon: 0.010\n",
            "Episode 1133/5000 | Reward: 1.50 | Avg loss: 0.0681 | Epsilon: 0.010\n",
            "Episode 1134/5000 | Reward: 1.10 | Avg loss: 0.0888 | Epsilon: 0.010\n",
            "Episode 1135/5000 | Reward: 3.20 | Avg loss: 0.0783 | Epsilon: 0.010\n",
            "Episode 1136/5000 | Reward: 3.70 | Avg loss: 0.0637 | Epsilon: 0.010\n",
            "Episode 1137/5000 | Reward: 1.50 | Avg loss: 0.0741 | Epsilon: 0.010\n",
            "Episode 1138/5000 | Reward: 2.40 | Avg loss: 0.0551 | Epsilon: 0.010\n",
            "Episode 1139/5000 | Reward: 2.20 | Avg loss: 0.0666 | Epsilon: 0.010\n",
            "Episode 1140/5000 | Reward: -0.90 | Avg loss: 0.0655 | Epsilon: 0.010\n",
            "Episode 1141/5000 | Reward: 1.90 | Avg loss: 0.0664 | Epsilon: 0.010\n",
            "Episode 1142/5000 | Reward: -0.90 | Avg loss: 0.0681 | Epsilon: 0.010\n",
            "Episode 1143/5000 | Reward: 3.10 | Avg loss: 0.0681 | Epsilon: 0.010\n",
            "Episode 1144/5000 | Reward: 3.00 | Avg loss: 0.0617 | Epsilon: 0.010\n",
            "Episode 1145/5000 | Reward: 6.10 | Avg loss: 0.0647 | Epsilon: 0.010\n",
            "Episode 1146/5000 | Reward: -0.70 | Avg loss: 0.0718 | Epsilon: 0.010\n",
            "Episode 1147/5000 | Reward: 2.30 | Avg loss: 0.0763 | Epsilon: 0.010\n",
            "Episode 1148/5000 | Reward: 6.70 | Avg loss: 0.0818 | Epsilon: 0.010\n",
            "Episode 1149/5000 | Reward: 1.50 | Avg loss: 0.0939 | Epsilon: 0.010\n",
            "Episode 1150/5000 | Reward: 2.80 | Avg loss: 0.0652 | Epsilon: 0.010\n",
            "Episode 1151/5000 | Reward: 5.50 | Avg loss: 0.0665 | Epsilon: 0.010\n",
            "Episode 1152/5000 | Reward: 2.30 | Avg loss: 0.0732 | Epsilon: 0.010\n",
            "Episode 1153/5000 | Reward: -0.90 | Avg loss: 0.0715 | Epsilon: 0.010\n",
            "Episode 1154/5000 | Reward: 4.30 | Avg loss: 0.0677 | Epsilon: 0.010\n",
            "Episode 1155/5000 | Reward: 2.40 | Avg loss: 0.0657 | Epsilon: 0.010\n",
            "Episode 1156/5000 | Reward: 2.70 | Avg loss: 0.0653 | Epsilon: 0.010\n",
            "Episode 1157/5000 | Reward: 1.60 | Avg loss: 0.0658 | Epsilon: 0.010\n",
            "Episode 1158/5000 | Reward: 3.00 | Avg loss: 0.0675 | Epsilon: 0.010\n",
            "Episode 1159/5000 | Reward: 3.10 | Avg loss: 0.0657 | Epsilon: 0.010\n",
            "Episode 1160/5000 | Reward: 3.20 | Avg loss: 0.0712 | Epsilon: 0.010\n",
            "Episode 1161/5000 | Reward: 2.10 | Avg loss: 0.0579 | Epsilon: 0.010\n",
            "Episode 1162/5000 | Reward: 2.80 | Avg loss: 0.0699 | Epsilon: 0.010\n",
            "Episode 1163/5000 | Reward: 2.40 | Avg loss: 0.0791 | Epsilon: 0.010\n",
            "Episode 1164/5000 | Reward: 0.00 | Avg loss: 0.0600 | Epsilon: 0.010\n",
            "Episode 1165/5000 | Reward: 2.30 | Avg loss: 0.0579 | Epsilon: 0.010\n",
            "Episode 1166/5000 | Reward: -0.90 | Avg loss: 0.0542 | Epsilon: 0.010\n",
            "Episode 1167/5000 | Reward: 6.10 | Avg loss: 0.0678 | Epsilon: 0.010\n",
            "Episode 1168/5000 | Reward: 3.10 | Avg loss: 0.0674 | Epsilon: 0.010\n",
            "Episode 1169/5000 | Reward: 2.80 | Avg loss: 0.0591 | Epsilon: 0.010\n",
            "Episode 1170/5000 | Reward: -0.90 | Avg loss: 0.0553 | Epsilon: 0.010\n",
            "Episode 1171/5000 | Reward: 2.80 | Avg loss: 0.0599 | Epsilon: 0.010\n",
            "Episode 1172/5000 | Reward: 3.00 | Avg loss: 0.0747 | Epsilon: 0.010\n",
            "Episode 1173/5000 | Reward: -0.90 | Avg loss: 0.0670 | Epsilon: 0.010\n",
            "Episode 1174/5000 | Reward: 7.00 | Avg loss: 0.0658 | Epsilon: 0.010\n",
            "Episode 1175/5000 | Reward: 2.70 | Avg loss: 0.0609 | Epsilon: 0.010\n",
            "Episode 1176/5000 | Reward: 2.30 | Avg loss: 0.0738 | Epsilon: 0.010\n",
            "Episode 1177/5000 | Reward: 2.80 | Avg loss: 0.0645 | Epsilon: 0.010\n",
            "Episode 1178/5000 | Reward: 0.80 | Avg loss: 0.0604 | Epsilon: 0.010\n",
            "Episode 1179/5000 | Reward: 0.00 | Avg loss: 0.0546 | Epsilon: 0.010\n",
            "Episode 1180/5000 | Reward: 2.90 | Avg loss: 0.0542 | Epsilon: 0.010\n",
            "Episode 1181/5000 | Reward: 5.60 | Avg loss: 0.0667 | Epsilon: 0.010\n",
            "Episode 1182/5000 | Reward: 1.80 | Avg loss: 0.0705 | Epsilon: 0.010\n",
            "Episode 1183/5000 | Reward: 2.80 | Avg loss: 0.0555 | Epsilon: 0.010\n",
            "Episode 1184/5000 | Reward: 1.50 | Avg loss: 0.0553 | Epsilon: 0.010\n",
            "Episode 1185/5000 | Reward: 2.90 | Avg loss: 0.0601 | Epsilon: 0.010\n",
            "Episode 1186/5000 | Reward: -1.40 | Avg loss: 0.0679 | Epsilon: 0.010\n",
            "Episode 1187/5000 | Reward: 2.70 | Avg loss: 0.0690 | Epsilon: 0.010\n",
            "Episode 1188/5000 | Reward: 2.80 | Avg loss: 0.0800 | Epsilon: 0.010\n",
            "Episode 1189/5000 | Reward: 4.70 | Avg loss: 0.0638 | Epsilon: 0.010\n",
            "Episode 1190/5000 | Reward: -8.10 | Avg loss: 0.0653 | Epsilon: 0.010\n",
            "Episode 1191/5000 | Reward: 2.80 | Avg loss: 0.0725 | Epsilon: 0.010\n",
            "Episode 1192/5000 | Reward: 2.90 | Avg loss: 0.0755 | Epsilon: 0.010\n",
            "Episode 1193/5000 | Reward: 2.80 | Avg loss: 0.0686 | Epsilon: 0.010\n",
            "Episode 1194/5000 | Reward: 8.90 | Avg loss: 0.0776 | Epsilon: 0.010\n",
            "Episode 1195/5000 | Reward: 2.70 | Avg loss: 0.0805 | Epsilon: 0.010\n",
            "Episode 1196/5000 | Reward: 2.20 | Avg loss: 0.0817 | Epsilon: 0.010\n",
            "Episode 1197/5000 | Reward: 2.10 | Avg loss: 0.1021 | Epsilon: 0.010\n",
            "Episode 1198/5000 | Reward: 3.90 | Avg loss: 0.0947 | Epsilon: 0.010\n",
            "Episode 1199/5000 | Reward: 2.60 | Avg loss: 0.0927 | Epsilon: 0.010\n",
            "Episode 1200/5000 | Reward: 1.80 | Avg loss: 0.0802 | Epsilon: 0.010\n",
            "Episode 1201/5000 | Reward: 6.10 | Avg loss: 0.0961 | Epsilon: 0.010\n",
            "Episode 1202/5000 | Reward: -0.90 | Avg loss: 0.1119 | Epsilon: 0.010\n",
            "Episode 1203/5000 | Reward: 3.90 | Avg loss: 0.1019 | Epsilon: 0.010\n",
            "Episode 1204/5000 | Reward: 4.90 | Avg loss: 0.0958 | Epsilon: 0.010\n",
            "Episode 1205/5000 | Reward: 11.40 | Avg loss: 0.0857 | Epsilon: 0.010\n",
            "Episode 1206/5000 | Reward: 3.10 | Avg loss: 0.0998 | Epsilon: 0.010\n",
            "Episode 1207/5000 | Reward: 2.70 | Avg loss: 0.0913 | Epsilon: 0.010\n",
            "Episode 1208/5000 | Reward: -0.90 | Avg loss: 0.0823 | Epsilon: 0.010\n",
            "Episode 1209/5000 | Reward: 2.70 | Avg loss: 0.0811 | Epsilon: 0.010\n",
            "Episode 1210/5000 | Reward: 0.80 | Avg loss: 0.0829 | Epsilon: 0.010\n",
            "Episode 1211/5000 | Reward: -0.90 | Avg loss: 0.0725 | Epsilon: 0.010\n",
            "Episode 1212/5000 | Reward: 3.10 | Avg loss: 0.0804 | Epsilon: 0.010\n",
            "Episode 1213/5000 | Reward: 0.50 | Avg loss: 0.0693 | Epsilon: 0.010\n",
            "Episode 1214/5000 | Reward: 3.20 | Avg loss: 0.0758 | Epsilon: 0.010\n",
            "Episode 1215/5000 | Reward: 2.30 | Avg loss: 0.0734 | Epsilon: 0.010\n",
            "Episode 1216/5000 | Reward: 2.60 | Avg loss: 0.0650 | Epsilon: 0.010\n",
            "Episode 1217/5000 | Reward: 9.10 | Avg loss: 0.0684 | Epsilon: 0.010\n",
            "Episode 1218/5000 | Reward: 4.90 | Avg loss: 0.0733 | Epsilon: 0.010\n",
            "Episode 1219/5000 | Reward: 2.80 | Avg loss: 0.0689 | Epsilon: 0.010\n",
            "Episode 1220/5000 | Reward: -1.50 | Avg loss: 0.0859 | Epsilon: 0.010\n",
            "Episode 1221/5000 | Reward: 2.20 | Avg loss: 0.0690 | Epsilon: 0.010\n",
            "Episode 1222/5000 | Reward: 2.30 | Avg loss: 0.0877 | Epsilon: 0.010\n",
            "Episode 1223/5000 | Reward: 2.00 | Avg loss: 0.0844 | Epsilon: 0.010\n",
            "Episode 1224/5000 | Reward: 14.70 | Avg loss: 0.0775 | Epsilon: 0.010\n",
            "Episode 1225/5000 | Reward: 0.90 | Avg loss: 0.0869 | Epsilon: 0.010\n",
            "Episode 1226/5000 | Reward: 3.00 | Avg loss: 0.0726 | Epsilon: 0.010\n",
            "Episode 1227/5000 | Reward: -0.90 | Avg loss: 0.0783 | Epsilon: 0.010\n",
            "Episode 1228/5000 | Reward: -1.70 | Avg loss: 0.0726 | Epsilon: 0.010\n",
            "Episode 1229/5000 | Reward: -0.90 | Avg loss: 0.0774 | Epsilon: 0.010\n",
            "Episode 1230/5000 | Reward: 2.60 | Avg loss: 0.0711 | Epsilon: 0.010\n",
            "Episode 1231/5000 | Reward: -0.50 | Avg loss: 0.0806 | Epsilon: 0.010\n",
            "Episode 1232/5000 | Reward: 3.00 | Avg loss: 0.0696 | Epsilon: 0.010\n",
            "Episode 1233/5000 | Reward: 2.10 | Avg loss: 0.0904 | Epsilon: 0.010\n",
            "Episode 1234/5000 | Reward: 0.80 | Avg loss: 0.0679 | Epsilon: 0.010\n",
            "Episode 1235/5000 | Reward: 2.00 | Avg loss: 0.0665 | Epsilon: 0.010\n",
            "Episode 1236/5000 | Reward: -0.90 | Avg loss: 0.0641 | Epsilon: 0.010\n",
            "Episode 1237/5000 | Reward: 1.00 | Avg loss: 0.0636 | Epsilon: 0.010\n",
            "Episode 1238/5000 | Reward: 0.90 | Avg loss: 0.0540 | Epsilon: 0.010\n",
            "Episode 1239/5000 | Reward: 3.30 | Avg loss: 0.0589 | Epsilon: 0.010\n",
            "Episode 1240/5000 | Reward: 3.60 | Avg loss: 0.0736 | Epsilon: 0.010\n",
            "Episode 1241/5000 | Reward: 2.30 | Avg loss: 0.0760 | Epsilon: 0.010\n",
            "Episode 1242/5000 | Reward: -0.90 | Avg loss: 0.0625 | Epsilon: 0.010\n",
            "Episode 1243/5000 | Reward: -0.40 | Avg loss: 0.0644 | Epsilon: 0.010\n",
            "Episode 1244/5000 | Reward: -0.90 | Avg loss: 0.0764 | Epsilon: 0.010\n",
            "Episode 1245/5000 | Reward: 2.60 | Avg loss: 0.0721 | Epsilon: 0.010\n",
            "Episode 1246/5000 | Reward: 5.40 | Avg loss: 0.0786 | Epsilon: 0.010\n",
            "Episode 1247/5000 | Reward: 2.80 | Avg loss: 0.0937 | Epsilon: 0.010\n",
            "Episode 1248/5000 | Reward: 1.40 | Avg loss: 0.0781 | Epsilon: 0.010\n",
            "Episode 1249/5000 | Reward: 0.90 | Avg loss: 0.0881 | Epsilon: 0.010\n",
            "Episode 1250/5000 | Reward: 1.50 | Avg loss: 0.0922 | Epsilon: 0.010\n",
            "Episode 1251/5000 | Reward: 2.60 | Avg loss: 0.0869 | Epsilon: 0.010\n",
            "Episode 1252/5000 | Reward: 2.70 | Avg loss: 0.0890 | Epsilon: 0.010\n",
            "Episode 1253/5000 | Reward: 3.00 | Avg loss: 0.0949 | Epsilon: 0.010\n",
            "Episode 1254/5000 | Reward: 2.10 | Avg loss: 0.0920 | Epsilon: 0.010\n",
            "Episode 1255/5000 | Reward: 2.10 | Avg loss: 0.0931 | Epsilon: 0.010\n",
            "Episode 1256/5000 | Reward: 2.60 | Avg loss: 0.0733 | Epsilon: 0.010\n",
            "Episode 1257/5000 | Reward: 7.00 | Avg loss: 0.0993 | Epsilon: 0.010\n",
            "Episode 1258/5000 | Reward: 0.70 | Avg loss: 0.0842 | Epsilon: 0.010\n",
            "Episode 1259/5000 | Reward: 5.30 | Avg loss: 0.0996 | Epsilon: 0.010\n",
            "Episode 1260/5000 | Reward: 2.20 | Avg loss: 0.0886 | Epsilon: 0.010\n",
            "Episode 1261/5000 | Reward: -0.90 | Avg loss: 0.0827 | Epsilon: 0.010\n",
            "Episode 1262/5000 | Reward: 2.90 | Avg loss: 0.0898 | Epsilon: 0.010\n",
            "Episode 1263/5000 | Reward: -0.90 | Avg loss: 0.0885 | Epsilon: 0.010\n",
            "Episode 1264/5000 | Reward: 10.60 | Avg loss: 0.0836 | Epsilon: 0.010\n",
            "Episode 1265/5000 | Reward: 4.80 | Avg loss: 0.0724 | Epsilon: 0.010\n",
            "Episode 1266/5000 | Reward: 0.90 | Avg loss: 0.0823 | Epsilon: 0.010\n",
            "Episode 1267/5000 | Reward: 0.20 | Avg loss: 0.0761 | Epsilon: 0.010\n",
            "Episode 1268/5000 | Reward: 3.00 | Avg loss: 0.0753 | Epsilon: 0.010\n",
            "Episode 1269/5000 | Reward: 2.10 | Avg loss: 0.0888 | Epsilon: 0.010\n",
            "Episode 1270/5000 | Reward: 3.20 | Avg loss: 0.0939 | Epsilon: 0.010\n",
            "Episode 1271/5000 | Reward: 3.90 | Avg loss: 0.0936 | Epsilon: 0.010\n",
            "Episode 1272/5000 | Reward: 1.90 | Avg loss: 0.0846 | Epsilon: 0.010\n",
            "Episode 1273/5000 | Reward: 2.90 | Avg loss: 0.0872 | Epsilon: 0.010\n",
            "Episode 1274/5000 | Reward: -0.30 | Avg loss: 0.0783 | Epsilon: 0.010\n",
            "Episode 1275/5000 | Reward: 0.90 | Avg loss: 0.0802 | Epsilon: 0.010\n",
            "Episode 1276/5000 | Reward: 2.50 | Avg loss: 0.0803 | Epsilon: 0.010\n",
            "Episode 1277/5000 | Reward: -0.90 | Avg loss: 0.1015 | Epsilon: 0.010\n",
            "Episode 1278/5000 | Reward: -1.10 | Avg loss: 0.0895 | Epsilon: 0.010\n",
            "Episode 1279/5000 | Reward: 3.10 | Avg loss: 0.0846 | Epsilon: 0.010\n",
            "Episode 1280/5000 | Reward: 0.70 | Avg loss: 0.0850 | Epsilon: 0.010\n",
            "Episode 1281/5000 | Reward: 6.50 | Avg loss: 0.0970 | Epsilon: 0.010\n",
            "Episode 1282/5000 | Reward: 2.90 | Avg loss: 0.0856 | Epsilon: 0.010\n",
            "Episode 1283/5000 | Reward: 8.80 | Avg loss: 0.0914 | Epsilon: 0.010\n",
            "Episode 1284/5000 | Reward: 10.20 | Avg loss: 0.1032 | Epsilon: 0.010\n",
            "Episode 1285/5000 | Reward: 10.40 | Avg loss: 0.1144 | Epsilon: 0.010\n",
            "Episode 1286/5000 | Reward: 1.20 | Avg loss: 0.0804 | Epsilon: 0.010\n",
            "Episode 1287/5000 | Reward: 3.10 | Avg loss: 0.0972 | Epsilon: 0.010\n",
            "Episode 1288/5000 | Reward: 2.80 | Avg loss: 0.1123 | Epsilon: 0.010\n",
            "Episode 1289/5000 | Reward: -1.70 | Avg loss: 0.0963 | Epsilon: 0.010\n",
            "Episode 1290/5000 | Reward: 2.70 | Avg loss: 0.1040 | Epsilon: 0.010\n",
            "Episode 1291/5000 | Reward: 11.50 | Avg loss: 0.0925 | Epsilon: 0.010\n",
            "Episode 1292/5000 | Reward: 2.90 | Avg loss: 0.0979 | Epsilon: 0.010\n",
            "Episode 1293/5000 | Reward: 2.80 | Avg loss: 0.0861 | Epsilon: 0.010\n",
            "Episode 1294/5000 | Reward: 0.80 | Avg loss: 0.0868 | Epsilon: 0.010\n",
            "Episode 1295/5000 | Reward: 3.00 | Avg loss: 0.0907 | Epsilon: 0.010\n",
            "Episode 1296/5000 | Reward: 2.20 | Avg loss: 0.0808 | Epsilon: 0.010\n",
            "Episode 1297/5000 | Reward: 2.40 | Avg loss: 0.0687 | Epsilon: 0.010\n",
            "Episode 1298/5000 | Reward: 7.50 | Avg loss: 0.0806 | Epsilon: 0.010\n",
            "Episode 1299/5000 | Reward: -1.80 | Avg loss: 0.0639 | Epsilon: 0.010\n",
            "Episode 1300/5000 | Reward: 2.90 | Avg loss: 0.0723 | Epsilon: 0.010\n",
            "Episode 1301/5000 | Reward: 2.90 | Avg loss: 0.0644 | Epsilon: 0.010\n",
            "Episode 1302/5000 | Reward: 3.10 | Avg loss: 0.0618 | Epsilon: 0.010\n",
            "Episode 1303/5000 | Reward: 10.50 | Avg loss: 0.0753 | Epsilon: 0.010\n",
            "Episode 1304/5000 | Reward: 5.20 | Avg loss: 0.0775 | Epsilon: 0.010\n",
            "Episode 1305/5000 | Reward: 14.30 | Avg loss: 0.0777 | Epsilon: 0.010\n",
            "Episode 1306/5000 | Reward: 5.20 | Avg loss: 0.0780 | Epsilon: 0.010\n",
            "Episode 1307/5000 | Reward: 2.90 | Avg loss: 0.1040 | Epsilon: 0.010\n",
            "Episode 1308/5000 | Reward: 4.40 | Avg loss: 0.0843 | Epsilon: 0.010\n",
            "Episode 1309/5000 | Reward: 3.10 | Avg loss: 0.0911 | Epsilon: 0.010\n",
            "Episode 1310/5000 | Reward: 3.40 | Avg loss: 0.0824 | Epsilon: 0.010\n",
            "Episode 1311/5000 | Reward: 4.20 | Avg loss: 0.0708 | Epsilon: 0.010\n",
            "Episode 1312/5000 | Reward: 2.80 | Avg loss: 0.0923 | Epsilon: 0.010\n",
            "Episode 1313/5000 | Reward: -0.30 | Avg loss: 0.0694 | Epsilon: 0.010\n",
            "Episode 1314/5000 | Reward: 11.10 | Avg loss: 0.1045 | Epsilon: 0.010\n",
            "Episode 1315/5000 | Reward: 4.90 | Avg loss: 0.0821 | Epsilon: 0.010\n",
            "Episode 1316/5000 | Reward: 3.10 | Avg loss: 0.0839 | Epsilon: 0.010\n",
            "Episode 1317/5000 | Reward: 1.40 | Avg loss: 0.0794 | Epsilon: 0.010\n",
            "Episode 1318/5000 | Reward: 7.40 | Avg loss: 0.0882 | Epsilon: 0.010\n",
            "Episode 1319/5000 | Reward: -1.00 | Avg loss: 0.1073 | Epsilon: 0.010\n",
            "Episode 1320/5000 | Reward: 2.10 | Avg loss: 0.1044 | Epsilon: 0.010\n",
            "Episode 1321/5000 | Reward: 1.50 | Avg loss: 0.1020 | Epsilon: 0.010\n",
            "Episode 1322/5000 | Reward: -0.10 | Avg loss: 0.1066 | Epsilon: 0.010\n",
            "Episode 1323/5000 | Reward: 3.00 | Avg loss: 0.1047 | Epsilon: 0.010\n",
            "Episode 1324/5000 | Reward: 2.90 | Avg loss: 0.0769 | Epsilon: 0.010\n",
            "Episode 1325/5000 | Reward: 2.90 | Avg loss: 0.0769 | Epsilon: 0.010\n",
            "Episode 1326/5000 | Reward: -1.20 | Avg loss: 0.1040 | Epsilon: 0.010\n",
            "Episode 1327/5000 | Reward: -1.00 | Avg loss: 0.0917 | Epsilon: 0.010\n",
            "Episode 1328/5000 | Reward: 3.00 | Avg loss: 0.0995 | Epsilon: 0.010\n",
            "Episode 1329/5000 | Reward: 2.90 | Avg loss: 0.0796 | Epsilon: 0.010\n",
            "Episode 1330/5000 | Reward: 2.60 | Avg loss: 0.0796 | Epsilon: 0.010\n",
            "Episode 1331/5000 | Reward: 0.30 | Avg loss: 0.0782 | Epsilon: 0.010\n",
            "Episode 1332/5000 | Reward: 3.00 | Avg loss: 0.0759 | Epsilon: 0.010\n",
            "Episode 1333/5000 | Reward: 3.10 | Avg loss: 0.0915 | Epsilon: 0.010\n",
            "Episode 1334/5000 | Reward: -0.70 | Avg loss: 0.0855 | Epsilon: 0.010\n",
            "Episode 1335/5000 | Reward: 4.10 | Avg loss: 0.0803 | Epsilon: 0.010\n",
            "Episode 1336/5000 | Reward: 2.60 | Avg loss: 0.0778 | Epsilon: 0.010\n",
            "Episode 1337/5000 | Reward: 0.00 | Avg loss: 0.0778 | Epsilon: 0.010\n",
            "Episode 1338/5000 | Reward: 2.30 | Avg loss: 0.0664 | Epsilon: 0.010\n",
            "Episode 1339/5000 | Reward: 5.80 | Avg loss: 0.0738 | Epsilon: 0.010\n",
            "Episode 1340/5000 | Reward: -0.10 | Avg loss: 0.1052 | Epsilon: 0.010\n",
            "Episode 1341/5000 | Reward: 6.20 | Avg loss: 0.1117 | Epsilon: 0.010\n",
            "Episode 1342/5000 | Reward: 3.20 | Avg loss: 0.1012 | Epsilon: 0.010\n",
            "Episode 1343/5000 | Reward: 5.00 | Avg loss: 0.1037 | Epsilon: 0.010\n",
            "Episode 1344/5000 | Reward: 1.30 | Avg loss: 0.1030 | Epsilon: 0.010\n",
            "Episode 1345/5000 | Reward: 2.70 | Avg loss: 0.1109 | Epsilon: 0.010\n",
            "Episode 1346/5000 | Reward: 0.40 | Avg loss: 0.1038 | Epsilon: 0.010\n",
            "Episode 1347/5000 | Reward: 2.70 | Avg loss: 0.1173 | Epsilon: 0.010\n",
            "Episode 1348/5000 | Reward: 2.90 | Avg loss: 0.1233 | Epsilon: 0.010\n",
            "Episode 1349/5000 | Reward: -1.20 | Avg loss: 0.1046 | Epsilon: 0.010\n",
            "Episode 1350/5000 | Reward: 14.00 | Avg loss: 0.0944 | Epsilon: 0.010\n",
            "Episode 1351/5000 | Reward: 2.80 | Avg loss: 0.0826 | Epsilon: 0.010\n",
            "Episode 1352/5000 | Reward: 2.90 | Avg loss: 0.0951 | Epsilon: 0.010\n",
            "Episode 1353/5000 | Reward: 3.10 | Avg loss: 0.0901 | Epsilon: 0.010\n",
            "Episode 1354/5000 | Reward: 3.10 | Avg loss: 0.0777 | Epsilon: 0.010\n",
            "Episode 1355/5000 | Reward: 8.20 | Avg loss: 0.0845 | Epsilon: 0.010\n",
            "Episode 1356/5000 | Reward: 2.30 | Avg loss: 0.0774 | Epsilon: 0.010\n",
            "Episode 1357/5000 | Reward: 3.00 | Avg loss: 0.0842 | Epsilon: 0.010\n",
            "Episode 1358/5000 | Reward: -1.20 | Avg loss: 0.0867 | Epsilon: 0.010\n",
            "Episode 1359/5000 | Reward: 4.60 | Avg loss: 0.0967 | Epsilon: 0.010\n",
            "Episode 1360/5000 | Reward: 2.90 | Avg loss: 0.0853 | Epsilon: 0.010\n",
            "Episode 1361/5000 | Reward: 4.30 | Avg loss: 0.0908 | Epsilon: 0.010\n",
            "Episode 1362/5000 | Reward: 2.40 | Avg loss: 0.0909 | Epsilon: 0.010\n",
            "Episode 1363/5000 | Reward: -1.10 | Avg loss: 0.0924 | Epsilon: 0.010\n",
            "Episode 1364/5000 | Reward: 6.10 | Avg loss: 0.0927 | Epsilon: 0.010\n",
            "Episode 1365/5000 | Reward: 4.90 | Avg loss: 0.0824 | Epsilon: 0.010\n",
            "Episode 1366/5000 | Reward: -0.50 | Avg loss: 0.0927 | Epsilon: 0.010\n",
            "Episode 1367/5000 | Reward: 2.90 | Avg loss: 0.0924 | Epsilon: 0.010\n",
            "Episode 1368/5000 | Reward: 16.30 | Avg loss: 0.0897 | Epsilon: 0.010\n",
            "Episode 1369/5000 | Reward: 3.10 | Avg loss: 0.0802 | Epsilon: 0.010\n",
            "Episode 1370/5000 | Reward: 2.80 | Avg loss: 0.0934 | Epsilon: 0.010\n",
            "Episode 1371/5000 | Reward: 5.70 | Avg loss: 0.0948 | Epsilon: 0.010\n",
            "Episode 1372/5000 | Reward: 7.00 | Avg loss: 0.0830 | Epsilon: 0.010\n",
            "Episode 1373/5000 | Reward: 1.30 | Avg loss: 0.1095 | Epsilon: 0.010\n",
            "Episode 1374/5000 | Reward: 7.70 | Avg loss: 0.0854 | Epsilon: 0.010\n",
            "Episode 1375/5000 | Reward: 3.00 | Avg loss: 0.0828 | Epsilon: 0.010\n",
            "Episode 1376/5000 | Reward: -0.90 | Avg loss: 0.0835 | Epsilon: 0.010\n",
            "Episode 1377/5000 | Reward: 0.40 | Avg loss: 0.0936 | Epsilon: 0.010\n",
            "Episode 1378/5000 | Reward: -0.90 | Avg loss: 0.0649 | Epsilon: 0.010\n",
            "Episode 1379/5000 | Reward: -0.90 | Avg loss: 0.0629 | Epsilon: 0.010\n",
            "Episode 1380/5000 | Reward: 8.90 | Avg loss: 0.0867 | Epsilon: 0.010\n",
            "Episode 1381/5000 | Reward: 2.50 | Avg loss: 0.0758 | Epsilon: 0.010\n",
            "Episode 1382/5000 | Reward: 2.90 | Avg loss: 0.0757 | Epsilon: 0.010\n",
            "Episode 1383/5000 | Reward: 2.90 | Avg loss: 0.0843 | Epsilon: 0.010\n",
            "Episode 1384/5000 | Reward: 4.30 | Avg loss: 0.0711 | Epsilon: 0.010\n",
            "Episode 1385/5000 | Reward: 3.40 | Avg loss: 0.0812 | Epsilon: 0.010\n",
            "Episode 1386/5000 | Reward: 2.40 | Avg loss: 0.0860 | Epsilon: 0.010\n",
            "Episode 1387/5000 | Reward: -0.30 | Avg loss: 0.0889 | Epsilon: 0.010\n",
            "Episode 1388/5000 | Reward: 3.10 | Avg loss: 0.0888 | Epsilon: 0.010\n",
            "Episode 1389/5000 | Reward: 2.90 | Avg loss: 0.0821 | Epsilon: 0.010\n",
            "Episode 1390/5000 | Reward: 2.80 | Avg loss: 0.0725 | Epsilon: 0.010\n",
            "Episode 1391/5000 | Reward: 4.40 | Avg loss: 0.0936 | Epsilon: 0.010\n",
            "Episode 1392/5000 | Reward: 7.50 | Avg loss: 0.0906 | Epsilon: 0.010\n",
            "Episode 1393/5000 | Reward: -0.90 | Avg loss: 0.0889 | Epsilon: 0.010\n",
            "Episode 1394/5000 | Reward: -0.30 | Avg loss: 0.0931 | Epsilon: 0.010\n",
            "Episode 1395/5000 | Reward: 3.20 | Avg loss: 0.0833 | Epsilon: 0.010\n",
            "Episode 1396/5000 | Reward: 3.20 | Avg loss: 0.0779 | Epsilon: 0.010\n",
            "Episode 1397/5000 | Reward: 1.20 | Avg loss: 0.0778 | Epsilon: 0.010\n",
            "Episode 1398/5000 | Reward: -0.90 | Avg loss: 0.0878 | Epsilon: 0.010\n",
            "Episode 1399/5000 | Reward: 6.50 | Avg loss: 0.0862 | Epsilon: 0.010\n",
            "Episode 1400/5000 | Reward: 2.80 | Avg loss: 0.0894 | Epsilon: 0.010\n",
            "Episode 1401/5000 | Reward: 2.90 | Avg loss: 0.1027 | Epsilon: 0.010\n",
            "Episode 1402/5000 | Reward: 3.30 | Avg loss: 0.0783 | Epsilon: 0.010\n",
            "Episode 1403/5000 | Reward: 5.00 | Avg loss: 0.0830 | Epsilon: 0.010\n",
            "Episode 1404/5000 | Reward: 2.60 | Avg loss: 0.0910 | Epsilon: 0.010\n",
            "Episode 1405/5000 | Reward: 2.80 | Avg loss: 0.0763 | Epsilon: 0.010\n",
            "Episode 1406/5000 | Reward: 3.00 | Avg loss: 0.0832 | Epsilon: 0.010\n",
            "Episode 1407/5000 | Reward: 2.70 | Avg loss: 0.0703 | Epsilon: 0.010\n",
            "Episode 1408/5000 | Reward: -0.10 | Avg loss: 0.0903 | Epsilon: 0.010\n",
            "Episode 1409/5000 | Reward: 2.90 | Avg loss: 0.0762 | Epsilon: 0.010\n",
            "Episode 1410/5000 | Reward: 0.90 | Avg loss: 0.0964 | Epsilon: 0.010\n",
            "Episode 1411/5000 | Reward: 3.00 | Avg loss: 0.0814 | Epsilon: 0.010\n",
            "Episode 1412/5000 | Reward: 2.90 | Avg loss: 0.0751 | Epsilon: 0.010\n",
            "Episode 1413/5000 | Reward: 0.10 | Avg loss: 0.0702 | Epsilon: 0.010\n",
            "Episode 1414/5000 | Reward: -2.10 | Avg loss: 0.0804 | Epsilon: 0.010\n",
            "Episode 1415/5000 | Reward: 3.00 | Avg loss: 0.0789 | Epsilon: 0.010\n",
            "Episode 1416/5000 | Reward: 2.90 | Avg loss: 0.0858 | Epsilon: 0.010\n",
            "Episode 1417/5000 | Reward: 4.60 | Avg loss: 0.0797 | Epsilon: 0.010\n",
            "Episode 1418/5000 | Reward: 5.80 | Avg loss: 0.0898 | Epsilon: 0.010\n",
            "Episode 1419/5000 | Reward: -0.90 | Avg loss: 0.0734 | Epsilon: 0.010\n",
            "Episode 1420/5000 | Reward: 4.40 | Avg loss: 0.0912 | Epsilon: 0.010\n",
            "Episode 1421/5000 | Reward: 5.80 | Avg loss: 0.0985 | Epsilon: 0.010\n",
            "Episode 1422/5000 | Reward: 0.30 | Avg loss: 0.0875 | Epsilon: 0.010\n",
            "Episode 1423/5000 | Reward: 1.80 | Avg loss: 0.1002 | Epsilon: 0.010\n",
            "Episode 1424/5000 | Reward: 2.90 | Avg loss: 0.0911 | Epsilon: 0.010\n",
            "Episode 1425/5000 | Reward: 2.80 | Avg loss: 0.1061 | Epsilon: 0.010\n",
            "Episode 1426/5000 | Reward: 5.50 | Avg loss: 0.1036 | Epsilon: 0.010\n",
            "Episode 1427/5000 | Reward: 1.90 | Avg loss: 0.1170 | Epsilon: 0.010\n",
            "Episode 1428/5000 | Reward: 3.10 | Avg loss: 0.0809 | Epsilon: 0.010\n",
            "Episode 1429/5000 | Reward: -0.50 | Avg loss: 0.0762 | Epsilon: 0.010\n",
            "Episode 1430/5000 | Reward: -0.90 | Avg loss: 0.0993 | Epsilon: 0.010\n",
            "Episode 1431/5000 | Reward: 0.30 | Avg loss: 0.1034 | Epsilon: 0.010\n",
            "Episode 1432/5000 | Reward: 2.50 | Avg loss: 0.0943 | Epsilon: 0.010\n",
            "Episode 1433/5000 | Reward: 3.30 | Avg loss: 0.0955 | Epsilon: 0.010\n",
            "Episode 1434/5000 | Reward: 2.40 | Avg loss: 0.0916 | Epsilon: 0.010\n",
            "Episode 1435/5000 | Reward: 1.40 | Avg loss: 0.0854 | Epsilon: 0.010\n",
            "Episode 1436/5000 | Reward: 8.70 | Avg loss: 0.0828 | Epsilon: 0.010\n",
            "Episode 1437/5000 | Reward: 2.70 | Avg loss: 0.1171 | Epsilon: 0.010\n",
            "Episode 1438/5000 | Reward: 8.10 | Avg loss: 0.1036 | Epsilon: 0.010\n",
            "Episode 1439/5000 | Reward: 3.00 | Avg loss: 0.1076 | Epsilon: 0.010\n",
            "Episode 1440/5000 | Reward: -2.10 | Avg loss: 0.0967 | Epsilon: 0.010\n",
            "Episode 1441/5000 | Reward: -0.90 | Avg loss: 0.0787 | Epsilon: 0.010\n",
            "Episode 1442/5000 | Reward: 4.70 | Avg loss: 0.1172 | Epsilon: 0.010\n",
            "Episode 1443/5000 | Reward: 2.80 | Avg loss: 0.1017 | Epsilon: 0.010\n",
            "Episode 1444/5000 | Reward: 2.90 | Avg loss: 0.1032 | Epsilon: 0.010\n",
            "Episode 1445/5000 | Reward: 5.60 | Avg loss: 0.1117 | Epsilon: 0.010\n",
            "Episode 1446/5000 | Reward: 2.80 | Avg loss: 0.1207 | Epsilon: 0.010\n",
            "Episode 1447/5000 | Reward: 16.50 | Avg loss: 0.1155 | Epsilon: 0.010\n",
            "Episode 1448/5000 | Reward: -5.70 | Avg loss: 0.1045 | Epsilon: 0.010\n",
            "Episode 1449/5000 | Reward: 2.20 | Avg loss: 0.0962 | Epsilon: 0.010\n",
            "Episode 1450/5000 | Reward: 1.90 | Avg loss: 0.0946 | Epsilon: 0.010\n",
            "Episode 1451/5000 | Reward: 2.90 | Avg loss: 0.1211 | Epsilon: 0.010\n",
            "Episode 1452/5000 | Reward: -1.70 | Avg loss: 0.1180 | Epsilon: 0.010\n",
            "Episode 1453/5000 | Reward: 3.50 | Avg loss: 0.1093 | Epsilon: 0.010\n",
            "Episode 1454/5000 | Reward: -1.00 | Avg loss: 0.1270 | Epsilon: 0.010\n",
            "Episode 1455/5000 | Reward: -0.20 | Avg loss: 0.1186 | Epsilon: 0.010\n",
            "Episode 1456/5000 | Reward: 4.40 | Avg loss: 0.0932 | Epsilon: 0.010\n",
            "Episode 1457/5000 | Reward: 3.10 | Avg loss: 0.0734 | Epsilon: 0.010\n",
            "Episode 1458/5000 | Reward: -0.00 | Avg loss: 0.0788 | Epsilon: 0.010\n",
            "Episode 1459/5000 | Reward: 3.20 | Avg loss: 0.0767 | Epsilon: 0.010\n",
            "Episode 1460/5000 | Reward: 1.30 | Avg loss: 0.0873 | Epsilon: 0.010\n",
            "Episode 1461/5000 | Reward: 0.10 | Avg loss: 0.0841 | Epsilon: 0.010\n",
            "Episode 1462/5000 | Reward: -0.90 | Avg loss: 0.0904 | Epsilon: 0.010\n",
            "Episode 1463/5000 | Reward: 2.60 | Avg loss: 0.0940 | Epsilon: 0.010\n",
            "Episode 1464/5000 | Reward: 2.50 | Avg loss: 0.0877 | Epsilon: 0.010\n",
            "Episode 1465/5000 | Reward: 4.00 | Avg loss: 0.1000 | Epsilon: 0.010\n",
            "Episode 1466/5000 | Reward: 2.20 | Avg loss: 0.1105 | Epsilon: 0.010\n",
            "Episode 1467/5000 | Reward: 7.00 | Avg loss: 0.1035 | Epsilon: 0.010\n",
            "Episode 1468/5000 | Reward: 2.70 | Avg loss: 0.0914 | Epsilon: 0.010\n",
            "Episode 1469/5000 | Reward: 2.80 | Avg loss: 0.1052 | Epsilon: 0.010\n",
            "Episode 1470/5000 | Reward: 2.60 | Avg loss: 0.0891 | Epsilon: 0.010\n",
            "Episode 1471/5000 | Reward: 4.50 | Avg loss: 0.0929 | Epsilon: 0.010\n",
            "Episode 1472/5000 | Reward: 4.50 | Avg loss: 0.0662 | Epsilon: 0.010\n",
            "Episode 1473/5000 | Reward: 8.90 | Avg loss: 0.0849 | Epsilon: 0.010\n",
            "Episode 1474/5000 | Reward: 4.70 | Avg loss: 0.0702 | Epsilon: 0.010\n",
            "Episode 1475/5000 | Reward: 2.70 | Avg loss: 0.0833 | Epsilon: 0.010\n",
            "Episode 1476/5000 | Reward: 2.50 | Avg loss: 0.0883 | Epsilon: 0.010\n",
            "Episode 1477/5000 | Reward: 2.40 | Avg loss: 0.0983 | Epsilon: 0.010\n",
            "Episode 1478/5000 | Reward: 2.40 | Avg loss: 0.0872 | Epsilon: 0.010\n",
            "Episode 1479/5000 | Reward: 2.20 | Avg loss: 0.0954 | Epsilon: 0.010\n",
            "Episode 1480/5000 | Reward: 2.90 | Avg loss: 0.1001 | Epsilon: 0.010\n",
            "Episode 1481/5000 | Reward: 7.50 | Avg loss: 0.0899 | Epsilon: 0.010\n",
            "Episode 1482/5000 | Reward: 2.20 | Avg loss: 0.1081 | Epsilon: 0.010\n",
            "Episode 1483/5000 | Reward: 2.50 | Avg loss: 0.0980 | Epsilon: 0.010\n",
            "Episode 1484/5000 | Reward: 2.40 | Avg loss: 0.0916 | Epsilon: 0.010\n",
            "Episode 1485/5000 | Reward: 2.80 | Avg loss: 0.0863 | Epsilon: 0.010\n",
            "Episode 1486/5000 | Reward: 3.10 | Avg loss: 0.0687 | Epsilon: 0.010\n",
            "Episode 1487/5000 | Reward: 2.80 | Avg loss: 0.0738 | Epsilon: 0.010\n",
            "Episode 1488/5000 | Reward: 2.00 | Avg loss: 0.0897 | Epsilon: 0.010\n",
            "Episode 1489/5000 | Reward: 2.30 | Avg loss: 0.0723 | Epsilon: 0.010\n",
            "Episode 1490/5000 | Reward: 2.40 | Avg loss: 0.0944 | Epsilon: 0.010\n",
            "Episode 1491/5000 | Reward: 1.40 | Avg loss: 0.0865 | Epsilon: 0.010\n",
            "Episode 1492/5000 | Reward: 2.50 | Avg loss: 0.0870 | Epsilon: 0.010\n",
            "Episode 1493/5000 | Reward: 0.50 | Avg loss: 0.0690 | Epsilon: 0.010\n",
            "Episode 1494/5000 | Reward: 0.10 | Avg loss: 0.0869 | Epsilon: 0.010\n",
            "Episode 1495/5000 | Reward: 3.00 | Avg loss: 0.0828 | Epsilon: 0.010\n",
            "Episode 1496/5000 | Reward: 2.00 | Avg loss: 0.0804 | Epsilon: 0.010\n",
            "Episode 1497/5000 | Reward: 0.70 | Avg loss: 0.0621 | Epsilon: 0.010\n",
            "Episode 1498/5000 | Reward: -0.90 | Avg loss: 0.0758 | Epsilon: 0.010\n",
            "Episode 1499/5000 | Reward: -0.90 | Avg loss: 0.0763 | Epsilon: 0.010\n",
            "Episode 1500/5000 | Reward: -0.90 | Avg loss: 0.0777 | Epsilon: 0.010\n",
            "Episode 1501/5000 | Reward: 2.90 | Avg loss: 0.0859 | Epsilon: 0.010\n",
            "Episode 1502/5000 | Reward: -0.90 | Avg loss: 0.0753 | Epsilon: 0.010\n",
            "Episode 1503/5000 | Reward: 2.90 | Avg loss: 0.0743 | Epsilon: 0.010\n",
            "Episode 1504/5000 | Reward: 16.00 | Avg loss: 0.0780 | Epsilon: 0.010\n",
            "Episode 1505/5000 | Reward: 2.90 | Avg loss: 0.0774 | Epsilon: 0.010\n",
            "Episode 1506/5000 | Reward: 0.70 | Avg loss: 0.0890 | Epsilon: 0.010\n",
            "Episode 1507/5000 | Reward: 8.40 | Avg loss: 0.0822 | Epsilon: 0.010\n",
            "Episode 1508/5000 | Reward: 2.90 | Avg loss: 0.1048 | Epsilon: 0.010\n",
            "Episode 1509/5000 | Reward: -0.90 | Avg loss: 0.0925 | Epsilon: 0.010\n",
            "Episode 1510/5000 | Reward: 1.40 | Avg loss: 0.0823 | Epsilon: 0.010\n",
            "Episode 1511/5000 | Reward: -0.60 | Avg loss: 0.0836 | Epsilon: 0.010\n",
            "Episode 1512/5000 | Reward: 2.00 | Avg loss: 0.0753 | Epsilon: 0.010\n",
            "Episode 1513/5000 | Reward: -1.30 | Avg loss: 0.0742 | Epsilon: 0.010\n",
            "Episode 1514/5000 | Reward: 1.10 | Avg loss: 0.0852 | Epsilon: 0.010\n",
            "Episode 1515/5000 | Reward: 3.00 | Avg loss: 0.0914 | Epsilon: 0.010\n",
            "Episode 1516/5000 | Reward: 10.10 | Avg loss: 0.0916 | Epsilon: 0.010\n",
            "Episode 1517/5000 | Reward: 2.40 | Avg loss: 0.1031 | Epsilon: 0.010\n",
            "Episode 1518/5000 | Reward: 0.90 | Avg loss: 0.1009 | Epsilon: 0.010\n",
            "Episode 1519/5000 | Reward: 3.10 | Avg loss: 0.0931 | Epsilon: 0.010\n",
            "Episode 1520/5000 | Reward: 1.40 | Avg loss: 0.1063 | Epsilon: 0.010\n",
            "Episode 1521/5000 | Reward: 5.80 | Avg loss: 0.0983 | Epsilon: 0.010\n",
            "Episode 1522/5000 | Reward: 2.80 | Avg loss: 0.0905 | Epsilon: 0.010\n",
            "Episode 1523/5000 | Reward: 2.40 | Avg loss: 0.1017 | Epsilon: 0.010\n",
            "Episode 1524/5000 | Reward: 2.90 | Avg loss: 0.0914 | Epsilon: 0.010\n",
            "Episode 1525/5000 | Reward: 3.20 | Avg loss: 0.0881 | Epsilon: 0.010\n",
            "Episode 1526/5000 | Reward: 8.40 | Avg loss: 0.1039 | Epsilon: 0.010\n",
            "Episode 1527/5000 | Reward: 1.10 | Avg loss: 0.1016 | Epsilon: 0.010\n",
            "Episode 1528/5000 | Reward: 3.00 | Avg loss: 0.0906 | Epsilon: 0.010\n",
            "Episode 1529/5000 | Reward: 2.90 | Avg loss: 0.0792 | Epsilon: 0.010\n",
            "Episode 1530/5000 | Reward: 3.00 | Avg loss: 0.0928 | Epsilon: 0.010\n",
            "Episode 1531/5000 | Reward: -0.90 | Avg loss: 0.0922 | Epsilon: 0.010\n",
            "Episode 1532/5000 | Reward: -7.50 | Avg loss: 0.0849 | Epsilon: 0.010\n",
            "Episode 1533/5000 | Reward: 2.40 | Avg loss: 0.1012 | Epsilon: 0.010\n",
            "Episode 1534/5000 | Reward: 1.30 | Avg loss: 0.0877 | Epsilon: 0.010\n",
            "Episode 1535/5000 | Reward: 3.00 | Avg loss: 0.0825 | Epsilon: 0.010\n",
            "Episode 1536/5000 | Reward: 2.00 | Avg loss: 0.0949 | Epsilon: 0.010\n",
            "Episode 1537/5000 | Reward: 3.20 | Avg loss: 0.0995 | Epsilon: 0.010\n",
            "Episode 1538/5000 | Reward: 2.90 | Avg loss: 0.0799 | Epsilon: 0.010\n",
            "Episode 1539/5000 | Reward: 2.70 | Avg loss: 0.0859 | Epsilon: 0.010\n",
            "Episode 1540/5000 | Reward: 3.10 | Avg loss: 0.0734 | Epsilon: 0.010\n",
            "Episode 1541/5000 | Reward: 2.40 | Avg loss: 0.0866 | Epsilon: 0.010\n",
            "Episode 1542/5000 | Reward: -0.90 | Avg loss: 0.0953 | Epsilon: 0.010\n",
            "Episode 1543/5000 | Reward: 6.20 | Avg loss: 0.0794 | Epsilon: 0.010\n",
            "Episode 1544/5000 | Reward: 3.10 | Avg loss: 0.1052 | Epsilon: 0.010\n",
            "Episode 1545/5000 | Reward: 1.20 | Avg loss: 0.0917 | Epsilon: 0.010\n",
            "Episode 1546/5000 | Reward: 3.10 | Avg loss: 0.1119 | Epsilon: 0.010\n",
            "Episode 1547/5000 | Reward: 0.30 | Avg loss: 0.1011 | Epsilon: 0.010\n",
            "Episode 1548/5000 | Reward: 2.90 | Avg loss: 0.0949 | Epsilon: 0.010\n",
            "Episode 1549/5000 | Reward: -0.20 | Avg loss: 0.0793 | Epsilon: 0.010\n",
            "Episode 1550/5000 | Reward: 2.50 | Avg loss: 0.0787 | Epsilon: 0.010\n",
            "Episode 1551/5000 | Reward: 2.80 | Avg loss: 0.0880 | Epsilon: 0.010\n",
            "Episode 1552/5000 | Reward: 4.20 | Avg loss: 0.1071 | Epsilon: 0.010\n",
            "Episode 1553/5000 | Reward: 2.10 | Avg loss: 0.0953 | Epsilon: 0.010\n",
            "Episode 1554/5000 | Reward: 2.30 | Avg loss: 0.0641 | Epsilon: 0.010\n",
            "Episode 1555/5000 | Reward: 2.50 | Avg loss: 0.0963 | Epsilon: 0.010\n",
            "Episode 1556/5000 | Reward: 0.30 | Avg loss: 0.0816 | Epsilon: 0.010\n",
            "Episode 1557/5000 | Reward: 5.80 | Avg loss: 0.0940 | Epsilon: 0.010\n",
            "Episode 1558/5000 | Reward: 7.00 | Avg loss: 0.0997 | Epsilon: 0.010\n",
            "Episode 1559/5000 | Reward: 2.40 | Avg loss: 0.1146 | Epsilon: 0.010\n",
            "Episode 1560/5000 | Reward: 2.40 | Avg loss: 0.0921 | Epsilon: 0.010\n",
            "Episode 1561/5000 | Reward: 3.10 | Avg loss: 0.0927 | Epsilon: 0.010\n",
            "Episode 1562/5000 | Reward: -0.40 | Avg loss: 0.0859 | Epsilon: 0.010\n",
            "Episode 1563/5000 | Reward: 0.10 | Avg loss: 0.0949 | Epsilon: 0.010\n",
            "Episode 1564/5000 | Reward: 4.40 | Avg loss: 0.1049 | Epsilon: 0.010\n",
            "Episode 1565/5000 | Reward: 10.20 | Avg loss: 0.0909 | Epsilon: 0.010\n",
            "Episode 1566/5000 | Reward: 2.60 | Avg loss: 0.1033 | Epsilon: 0.010\n",
            "Episode 1567/5000 | Reward: 2.50 | Avg loss: 0.0906 | Epsilon: 0.010\n",
            "Episode 1568/5000 | Reward: 3.00 | Avg loss: 0.0912 | Epsilon: 0.010\n",
            "Episode 1569/5000 | Reward: 7.80 | Avg loss: 0.0949 | Epsilon: 0.010\n",
            "Episode 1570/5000 | Reward: 2.10 | Avg loss: 0.0759 | Epsilon: 0.010\n",
            "Episode 1571/5000 | Reward: 2.90 | Avg loss: 0.0864 | Epsilon: 0.010\n",
            "Episode 1572/5000 | Reward: 0.50 | Avg loss: 0.0980 | Epsilon: 0.010\n",
            "Episode 1573/5000 | Reward: 2.50 | Avg loss: 0.1078 | Epsilon: 0.010\n",
            "Episode 1574/5000 | Reward: -0.30 | Avg loss: 0.0906 | Epsilon: 0.010\n",
            "Episode 1575/5000 | Reward: -0.90 | Avg loss: 0.0960 | Epsilon: 0.010\n",
            "Episode 1576/5000 | Reward: -0.90 | Avg loss: 0.0893 | Epsilon: 0.010\n",
            "Episode 1577/5000 | Reward: 2.50 | Avg loss: 0.0793 | Epsilon: 0.010\n",
            "Episode 1578/5000 | Reward: -0.50 | Avg loss: 0.1017 | Epsilon: 0.010\n",
            "Episode 1579/5000 | Reward: 9.10 | Avg loss: 0.1182 | Epsilon: 0.010\n",
            "Episode 1580/5000 | Reward: 3.40 | Avg loss: 0.1339 | Epsilon: 0.010\n",
            "Episode 1581/5000 | Reward: 10.10 | Avg loss: 0.1235 | Epsilon: 0.010\n",
            "Episode 1582/5000 | Reward: 2.80 | Avg loss: 0.1099 | Epsilon: 0.010\n",
            "Episode 1583/5000 | Reward: 2.60 | Avg loss: 0.1152 | Epsilon: 0.010\n",
            "Episode 1584/5000 | Reward: 2.60 | Avg loss: 0.1004 | Epsilon: 0.010\n",
            "Episode 1585/5000 | Reward: 4.80 | Avg loss: 0.1165 | Epsilon: 0.010\n",
            "Episode 1586/5000 | Reward: 2.20 | Avg loss: 0.1096 | Epsilon: 0.010\n",
            "Episode 1587/5000 | Reward: 2.00 | Avg loss: 0.0984 | Epsilon: 0.010\n",
            "Episode 1588/5000 | Reward: -2.80 | Avg loss: 0.1115 | Epsilon: 0.010\n",
            "Episode 1589/5000 | Reward: 9.00 | Avg loss: 0.1055 | Epsilon: 0.010\n",
            "Episode 1590/5000 | Reward: 6.40 | Avg loss: 0.0938 | Epsilon: 0.010\n",
            "Episode 1591/5000 | Reward: 3.00 | Avg loss: 0.1080 | Epsilon: 0.010\n",
            "Episode 1592/5000 | Reward: 1.80 | Avg loss: 0.0975 | Epsilon: 0.010\n",
            "Episode 1593/5000 | Reward: 2.70 | Avg loss: 0.0814 | Epsilon: 0.010\n",
            "Episode 1594/5000 | Reward: 3.10 | Avg loss: 0.1082 | Epsilon: 0.010\n",
            "Episode 1595/5000 | Reward: -0.90 | Avg loss: 0.1153 | Epsilon: 0.010\n",
            "Episode 1596/5000 | Reward: 8.70 | Avg loss: 0.1155 | Epsilon: 0.010\n",
            "Episode 1597/5000 | Reward: -0.40 | Avg loss: 0.1115 | Epsilon: 0.010\n",
            "Episode 1598/5000 | Reward: 5.90 | Avg loss: 0.1044 | Epsilon: 0.010\n",
            "Episode 1599/5000 | Reward: 5.80 | Avg loss: 0.1002 | Epsilon: 0.010\n",
            "Episode 1600/5000 | Reward: 3.40 | Avg loss: 0.1194 | Epsilon: 0.010\n",
            "Episode 1601/5000 | Reward: 5.10 | Avg loss: 0.1281 | Epsilon: 0.010\n",
            "Episode 1602/5000 | Reward: -0.90 | Avg loss: 0.1145 | Epsilon: 0.010\n",
            "Episode 1603/5000 | Reward: 4.90 | Avg loss: 0.1195 | Epsilon: 0.010\n",
            "Episode 1604/5000 | Reward: 4.60 | Avg loss: 0.1156 | Epsilon: 0.010\n",
            "Episode 1605/5000 | Reward: 2.90 | Avg loss: 0.1073 | Epsilon: 0.010\n",
            "Episode 1606/5000 | Reward: 3.00 | Avg loss: 0.0967 | Epsilon: 0.010\n",
            "Episode 1607/5000 | Reward: -0.90 | Avg loss: 0.1180 | Epsilon: 0.010\n",
            "Episode 1608/5000 | Reward: 2.50 | Avg loss: 0.1057 | Epsilon: 0.010\n",
            "Episode 1609/5000 | Reward: 6.70 | Avg loss: 0.1097 | Epsilon: 0.010\n",
            "Episode 1610/5000 | Reward: 1.30 | Avg loss: 0.1133 | Epsilon: 0.010\n",
            "Episode 1611/5000 | Reward: 0.60 | Avg loss: 0.1126 | Epsilon: 0.010\n",
            "Episode 1612/5000 | Reward: 6.30 | Avg loss: 0.0869 | Epsilon: 0.010\n",
            "Episode 1613/5000 | Reward: 0.60 | Avg loss: 0.1180 | Epsilon: 0.010\n",
            "Episode 1614/5000 | Reward: 2.40 | Avg loss: 0.0896 | Epsilon: 0.010\n",
            "Episode 1615/5000 | Reward: 5.10 | Avg loss: 0.1037 | Epsilon: 0.010\n",
            "Episode 1616/5000 | Reward: 2.50 | Avg loss: 0.1080 | Epsilon: 0.010\n",
            "Episode 1617/5000 | Reward: 2.90 | Avg loss: 0.1192 | Epsilon: 0.010\n",
            "Episode 1618/5000 | Reward: -0.90 | Avg loss: 0.1145 | Epsilon: 0.010\n",
            "Episode 1619/5000 | Reward: 1.40 | Avg loss: 0.0905 | Epsilon: 0.010\n",
            "Episode 1620/5000 | Reward: 3.40 | Avg loss: 0.1092 | Epsilon: 0.010\n",
            "Episode 1621/5000 | Reward: 3.10 | Avg loss: 0.1169 | Epsilon: 0.010\n",
            "Episode 1622/5000 | Reward: 3.00 | Avg loss: 0.1129 | Epsilon: 0.010\n",
            "Episode 1623/5000 | Reward: 2.50 | Avg loss: 0.1219 | Epsilon: 0.010\n",
            "Episode 1624/5000 | Reward: 6.70 | Avg loss: 0.0825 | Epsilon: 0.010\n",
            "Episode 1625/5000 | Reward: 5.50 | Avg loss: 0.0946 | Epsilon: 0.010\n",
            "Episode 1626/5000 | Reward: 2.90 | Avg loss: 0.0930 | Epsilon: 0.010\n",
            "Episode 1627/5000 | Reward: 2.90 | Avg loss: 0.0873 | Epsilon: 0.010\n",
            "Episode 1628/5000 | Reward: 6.70 | Avg loss: 0.1212 | Epsilon: 0.010\n",
            "Episode 1629/5000 | Reward: 7.90 | Avg loss: 0.1131 | Epsilon: 0.010\n",
            "Episode 1630/5000 | Reward: -0.90 | Avg loss: 0.1132 | Epsilon: 0.010\n",
            "Episode 1631/5000 | Reward: 8.10 | Avg loss: 0.1033 | Epsilon: 0.010\n",
            "Episode 1632/5000 | Reward: 3.10 | Avg loss: 0.0986 | Epsilon: 0.010\n",
            "Episode 1633/5000 | Reward: 3.10 | Avg loss: 0.1300 | Epsilon: 0.010\n",
            "Episode 1634/5000 | Reward: 3.00 | Avg loss: 0.0939 | Epsilon: 0.010\n",
            "Episode 1635/5000 | Reward: 2.10 | Avg loss: 0.1179 | Epsilon: 0.010\n",
            "Episode 1636/5000 | Reward: 3.00 | Avg loss: 0.1102 | Epsilon: 0.010\n",
            "Episode 1637/5000 | Reward: 2.50 | Avg loss: 0.1139 | Epsilon: 0.010\n",
            "Episode 1638/5000 | Reward: -1.40 | Avg loss: 0.0918 | Epsilon: 0.010\n",
            "Episode 1639/5000 | Reward: 2.80 | Avg loss: 0.0897 | Epsilon: 0.010\n",
            "Episode 1640/5000 | Reward: 2.30 | Avg loss: 0.1172 | Epsilon: 0.010\n",
            "Episode 1641/5000 | Reward: 4.70 | Avg loss: 0.1211 | Epsilon: 0.010\n",
            "Episode 1642/5000 | Reward: 1.80 | Avg loss: 0.1039 | Epsilon: 0.010\n",
            "Episode 1643/5000 | Reward: 0.60 | Avg loss: 0.1169 | Epsilon: 0.010\n",
            "Episode 1644/5000 | Reward: 0.20 | Avg loss: 0.1000 | Epsilon: 0.010\n",
            "Episode 1645/5000 | Reward: 2.10 | Avg loss: 0.1022 | Epsilon: 0.010\n",
            "Episode 1646/5000 | Reward: 3.20 | Avg loss: 0.0836 | Epsilon: 0.010\n",
            "Episode 1647/5000 | Reward: 6.20 | Avg loss: 0.0989 | Epsilon: 0.010\n",
            "Episode 1648/5000 | Reward: -0.90 | Avg loss: 0.1038 | Epsilon: 0.010\n",
            "Episode 1649/5000 | Reward: 2.40 | Avg loss: 0.1067 | Epsilon: 0.010\n",
            "Episode 1650/5000 | Reward: -0.90 | Avg loss: 0.1023 | Epsilon: 0.010\n",
            "Episode 1651/5000 | Reward: -1.00 | Avg loss: 0.1305 | Epsilon: 0.010\n",
            "Episode 1652/5000 | Reward: 0.70 | Avg loss: 0.0923 | Epsilon: 0.010\n",
            "Episode 1653/5000 | Reward: -0.40 | Avg loss: 0.1003 | Epsilon: 0.010\n",
            "Episode 1654/5000 | Reward: 3.10 | Avg loss: 0.1016 | Epsilon: 0.010\n",
            "Episode 1655/5000 | Reward: 6.40 | Avg loss: 0.0881 | Epsilon: 0.010\n",
            "Episode 1656/5000 | Reward: 2.60 | Avg loss: 0.0959 | Epsilon: 0.010\n",
            "Episode 1657/5000 | Reward: -0.10 | Avg loss: 0.1047 | Epsilon: 0.010\n",
            "Episode 1658/5000 | Reward: -2.70 | Avg loss: 0.0986 | Epsilon: 0.010\n",
            "Episode 1659/5000 | Reward: 5.30 | Avg loss: 0.1013 | Epsilon: 0.010\n",
            "Episode 1660/5000 | Reward: 4.80 | Avg loss: 0.1205 | Epsilon: 0.010\n",
            "Episode 1661/5000 | Reward: 8.70 | Avg loss: 0.1192 | Epsilon: 0.010\n",
            "Episode 1662/5000 | Reward: 3.00 | Avg loss: 0.1229 | Epsilon: 0.010\n",
            "Episode 1663/5000 | Reward: 0.90 | Avg loss: 0.0961 | Epsilon: 0.010\n",
            "Episode 1664/5000 | Reward: 2.20 | Avg loss: 0.1143 | Epsilon: 0.010\n",
            "Episode 1665/5000 | Reward: -0.30 | Avg loss: 0.1261 | Epsilon: 0.010\n",
            "Episode 1666/5000 | Reward: 12.70 | Avg loss: 0.1188 | Epsilon: 0.010\n",
            "Episode 1667/5000 | Reward: 4.40 | Avg loss: 0.1142 | Epsilon: 0.010\n",
            "Episode 1668/5000 | Reward: 8.40 | Avg loss: 0.1142 | Epsilon: 0.010\n",
            "Episode 1669/5000 | Reward: 7.70 | Avg loss: 0.1283 | Epsilon: 0.010\n",
            "Episode 1670/5000 | Reward: 3.00 | Avg loss: 0.1203 | Epsilon: 0.010\n",
            "Episode 1671/5000 | Reward: 2.50 | Avg loss: 0.1255 | Epsilon: 0.010\n",
            "Episode 1672/5000 | Reward: 2.10 | Avg loss: 0.1235 | Epsilon: 0.010\n",
            "Episode 1673/5000 | Reward: 0.70 | Avg loss: 0.1044 | Epsilon: 0.010\n",
            "Episode 1674/5000 | Reward: 1.80 | Avg loss: 0.0982 | Epsilon: 0.010\n",
            "Episode 1675/5000 | Reward: 5.80 | Avg loss: 0.1002 | Epsilon: 0.010\n",
            "Episode 1676/5000 | Reward: 3.00 | Avg loss: 0.1008 | Epsilon: 0.010\n",
            "Episode 1677/5000 | Reward: 2.40 | Avg loss: 0.0987 | Epsilon: 0.010\n",
            "Episode 1678/5000 | Reward: 2.90 | Avg loss: 0.0904 | Epsilon: 0.010\n",
            "Episode 1679/5000 | Reward: 0.70 | Avg loss: 0.1034 | Epsilon: 0.010\n",
            "Episode 1680/5000 | Reward: 3.00 | Avg loss: 0.0897 | Epsilon: 0.010\n",
            "Episode 1681/5000 | Reward: 0.60 | Avg loss: 0.0947 | Epsilon: 0.010\n",
            "Episode 1682/5000 | Reward: 2.90 | Avg loss: 0.1005 | Epsilon: 0.010\n",
            "Episode 1683/5000 | Reward: 6.60 | Avg loss: 0.0927 | Epsilon: 0.010\n",
            "Episode 1684/5000 | Reward: 3.00 | Avg loss: 0.0918 | Epsilon: 0.010\n",
            "Episode 1685/5000 | Reward: 5.10 | Avg loss: 0.1075 | Epsilon: 0.010\n",
            "Episode 1686/5000 | Reward: -0.90 | Avg loss: 0.1152 | Epsilon: 0.010\n",
            "Episode 1687/5000 | Reward: 4.20 | Avg loss: 0.1093 | Epsilon: 0.010\n",
            "Episode 1688/5000 | Reward: 1.00 | Avg loss: 0.1000 | Epsilon: 0.010\n",
            "Episode 1689/5000 | Reward: 9.50 | Avg loss: 0.1015 | Epsilon: 0.010\n",
            "Episode 1690/5000 | Reward: 3.20 | Avg loss: 0.1243 | Epsilon: 0.010\n",
            "Episode 1691/5000 | Reward: 4.50 | Avg loss: 0.1038 | Epsilon: 0.010\n",
            "Episode 1692/5000 | Reward: 6.30 | Avg loss: 0.1191 | Epsilon: 0.010\n",
            "Episode 1693/5000 | Reward: 7.00 | Avg loss: 0.1069 | Epsilon: 0.010\n",
            "Episode 1694/5000 | Reward: 6.70 | Avg loss: 0.1033 | Epsilon: 0.010\n",
            "Episode 1695/5000 | Reward: -0.60 | Avg loss: 0.0934 | Epsilon: 0.010\n",
            "Episode 1696/5000 | Reward: 1.40 | Avg loss: 0.1171 | Epsilon: 0.010\n",
            "Episode 1697/5000 | Reward: 3.10 | Avg loss: 0.1005 | Epsilon: 0.010\n",
            "Episode 1698/5000 | Reward: 2.90 | Avg loss: 0.0952 | Epsilon: 0.010\n",
            "Episode 1699/5000 | Reward: 5.20 | Avg loss: 0.1019 | Epsilon: 0.010\n",
            "Episode 1700/5000 | Reward: 3.10 | Avg loss: 0.0958 | Epsilon: 0.010\n",
            "Episode 1701/5000 | Reward: 3.10 | Avg loss: 0.1311 | Epsilon: 0.010\n",
            "Episode 1702/5000 | Reward: -0.90 | Avg loss: 0.0906 | Epsilon: 0.010\n",
            "Episode 1703/5000 | Reward: 2.90 | Avg loss: 0.1087 | Epsilon: 0.010\n",
            "Episode 1704/5000 | Reward: 0.50 | Avg loss: 0.1063 | Epsilon: 0.010\n",
            "Episode 1705/5000 | Reward: -0.90 | Avg loss: 0.1199 | Epsilon: 0.010\n",
            "Episode 1706/5000 | Reward: 2.60 | Avg loss: 0.1067 | Epsilon: 0.010\n",
            "Episode 1707/5000 | Reward: 13.80 | Avg loss: 0.1056 | Epsilon: 0.010\n",
            "Episode 1708/5000 | Reward: -1.00 | Avg loss: 0.0851 | Epsilon: 0.010\n",
            "Episode 1709/5000 | Reward: -0.40 | Avg loss: 0.0792 | Epsilon: 0.010\n",
            "Episode 1710/5000 | Reward: 6.40 | Avg loss: 0.0873 | Epsilon: 0.010\n",
            "Episode 1711/5000 | Reward: 2.80 | Avg loss: 0.1022 | Epsilon: 0.010\n",
            "Episode 1712/5000 | Reward: 3.10 | Avg loss: 0.0965 | Epsilon: 0.010\n",
            "Episode 1713/5000 | Reward: 2.40 | Avg loss: 0.0983 | Epsilon: 0.010\n",
            "Episode 1714/5000 | Reward: 2.80 | Avg loss: 0.0935 | Epsilon: 0.010\n",
            "Episode 1715/5000 | Reward: 11.30 | Avg loss: 0.1063 | Epsilon: 0.010\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 7: TRAINING LOOP\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "def train_dqn(agent, env, num_episodes):\n",
        "    \"\"\"\n",
        "    Main training loop.\n",
        "\n",
        "    Args:\n",
        "        agent: DQNAgent instance\n",
        "        env: Gymnasium environment\n",
        "        num_episodes: Number of episodes to train\n",
        "\n",
        "    Returns:\n",
        "        rewards: List of episode rewards\n",
        "        losses: List of training losses (avg per episode)\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    losses = []\n",
        "\n",
        "    epsilon = EPSILON_START\n",
        "    global_step = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0.0\n",
        "        episode_losses = []\n",
        "\n",
        "        while not done:\n",
        "            # --- Epsilon-greedy action selection ---\n",
        "            action = agent.select_action(state, epsilon)\n",
        "\n",
        "            # --- Interact with environment ---\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # --- Store experience in replay buffer ---\n",
        "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            # --- Train on a random batch from replay buffer ---\n",
        "            loss = agent.train_step()\n",
        "            if loss is not None:\n",
        "                episode_losses.append(loss)\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "            # --- Periodically update target network ---\n",
        "            if global_step % TARGET_UPDATE == 0:\n",
        "                agent.update_target_network()\n",
        "\n",
        "            # --- Decay epsilon (exploration) ---\n",
        "            epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
        "\n",
        "        # end of episode\n",
        "        rewards.append(episode_reward)\n",
        "        if episode_losses:\n",
        "            avg_loss = np.mean(episode_losses)\n",
        "        else:\n",
        "            avg_loss = 0.0\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode + 1}/{num_episodes} | \"\n",
        "            f\"Reward: {episode_reward:.2f} | \"\n",
        "            f\"Avg loss: {avg_loss:.4f} | \"\n",
        "            f\"Epsilon: {epsilon:.3f}\"\n",
        "        )\n",
        "\n",
        "    return rewards, losses\n",
        "\n",
        "#test\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "env = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "agent = DQNAgent(state_dim, action_dim, device=device)\n",
        "\n",
        "rewards, losses = train_dqn(agent, env, NUM_EPISODES)\n",
        "\n",
        "save_model(agent, MODEL_PATH)\n",
        "\n",
        "# NEW: save metrics (optional, but nice)\n",
        "np.save(\"rewards_v1.npy\", rewards)\n",
        "np.save(\"losses_v1.npy\", losses)\n",
        "\n",
        "# NEW: show learning curves\n",
        "plot_training_results(rewards, losses)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MlUQ5PBJ7nEi"
      },
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 9.5: VIDEO\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "LIDAR_OBS = True\n",
        "ENV_ID = 'FlappyBird-v0'\n",
        "\n",
        "MODEL_PATH = 'flappy_bird_dqn_v1.pth' # Must match the save path\n",
        "VIDEO_FOLDER = './dqn_v1_performance_video'\n",
        "\n",
        "# 1. Instantiate a new evaluation agent\n",
        "eval_agent = DQNAgent(state_dim, action_dim, device=device)\n",
        "\n",
        "# 2. Load the model\n",
        "eval_agent = load_model(eval_agent, MODEL_PATH)\n",
        "\n",
        "# 3. Create an evaluation environment (must be 'rgb_array' for video)\n",
        "eval_env = gym.make(\n",
        "    ENV_ID,\n",
        "    render_mode='rgb_array',\n",
        "    use_lidar=LIDAR_OBS\n",
        ")\n",
        "\n",
        "# 4. Record the video\n",
        "record_video(eval_env, eval_agent, VIDEO_FOLDER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nB9N0Bnng3D2"
      },
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 9: EVALUATION\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "def evaluate_agent(agent, env, num_episodes=10):\n",
        "    \"\"\"\n",
        "    Evaluate agent performance without exploration (epsilon=0).\n",
        "\n",
        "    Args:\n",
        "        agent: trained DQNAgent\n",
        "        env: evaluation environment (no video wrapper)\n",
        "        num_episodes: number of episodes to evaluate\n",
        "\n",
        "    Returns:\n",
        "        avg_reward: Average episode reward\n",
        "        avg_steps: Average number of steps survived\n",
        "        scores: List of individual episode rewards\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    steps_list = []\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        ep_steps = 0\n",
        "\n",
        "        while not done:\n",
        "            # Greedy action selection (no exploration)\n",
        "            action = agent.select_action(state, epsilon=0.0)\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            ep_steps += 1\n",
        "\n",
        "        scores.append(episode_reward)\n",
        "        steps_list.append(ep_steps)\n",
        "\n",
        "    avg_reward = float(np.mean(scores))\n",
        "    avg_steps = float(np.mean(steps_list))\n",
        "\n",
        "    return avg_reward, avg_steps, scores\n",
        "\n",
        "\n",
        "# --- RUN EVALUATION ---\n",
        "# Make a fresh evaluation environment\n",
        "eval_env2 = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
        "\n",
        "# Evaluate the trained model (eval_agent must be loaded in Section 9.5)\n",
        "avg_reward, avg_steps, scores = evaluate_agent(eval_agent, eval_env2, num_episodes=20)\n",
        "\n",
        "eval_env2.close()\n",
        "\n",
        "print(\"════════ Evaluation Results ════════\")\n",
        "print(\"Avg reward:\", avg_reward)\n",
        "print(\"Avg steps:\", avg_steps)\n",
        "print(\"Scores per episode:\", scores)\n",
        "print(\"════════════════════════════════════\")\n",
        "\n",
        "np.save(\"eval_scores_v1.npy\", scores)\n",
        "with open(\"eval_summary_v1.txt\", \"w\") as f:\n",
        "    f.write(f\"Avg reward: {avg_reward}\\n\")\n",
        "    f.write(f\"Avg steps: {avg_steps}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa-a2kIDbUro"
      },
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 10: EXPERIMENTS - VERSION 1 (BASIC DQN)\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "def run_experiment_v1(num_episodes=NUM_EPISODES):\n",
        "    print(\"════ Running Experiment V1: Basic DQN ════\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 1) Environment + agent\n",
        "    env = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    agent = DQNAgent(state_dim, action_dim, device=device)\n",
        "\n",
        "    # 2) Train\n",
        "    rewards, losses = train_dqn(agent, env, num_episodes)\n",
        "    env.close()\n",
        "\n",
        "    # 3) Save model + metrics\n",
        "    MODEL_PATH_V1 = \"flappy_bird_dqn_v1.pth\"\n",
        "    save_model(agent, MODEL_PATH_V1)\n",
        "\n",
        "    np.save(\"rewards_v1.npy\", rewards)\n",
        "    np.save(\"losses_v1.npy\", losses)\n",
        "\n",
        "    # 4) Plot learning curves\n",
        "    plot_training_results(rewards, losses)\n",
        "\n",
        "    # 5) Evaluation\n",
        "    eval_env = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
        "    avg_reward, avg_steps, scores = evaluate_agent(agent, eval_env, num_episodes=20)\n",
        "    eval_env.close()\n",
        "\n",
        "    np.save(\"eval_scores_v1.npy\", scores)\n",
        "    with open(\"eval_summary_v1.txt\", \"w\") as f:\n",
        "        f.write(f\"Avg reward: {avg_reward}\\n\")\n",
        "        f.write(f\"Avg steps: {avg_steps}\\n\")\n",
        "\n",
        "    print(\"════ V1 Evaluation ════\")\n",
        "    print(\"Avg reward:\", avg_reward)\n",
        "    print(\"Avg steps:\", avg_steps)\n",
        "    print(\"Scores:\", scores)\n",
        "\n",
        "    # 6) Record video\n",
        "    eval_env_video = gym.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=True)\n",
        "    VIDEO_FOLDER_V1 = \"./dqn_v1_video\"\n",
        "    record_video(eval_env_video, agent, VIDEO_FOLDER_V1)\n",
        "    eval_env_video.close()\n",
        "\n",
        "    print(\"✅ Experiment V1 completed.\")\n",
        "    return agent, rewards, losses\n",
        "\n",
        "# To actually run:\n",
        "agent_v1, rewards_v1, losses_v1 = run_experiment_v1()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQGk4nTEbVZB"
      },
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 11: EXPERIMENTS - VERSION 2 (IMPROVEMENTS: DOUBLE DQN)\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "class DoubleDQNAgent(DQNAgent):\n",
        "    \"\"\"\n",
        "    Extends the basic DQNAgent by implementing the Double DQN update rule.\n",
        "    Double DQN reduces overestimation bias and improves stability.\n",
        "    \"\"\"\n",
        "\n",
        "    def train_step(self):\n",
        "        # Don’t train until we have enough samples\n",
        "        if len(self.replay_buffer) < max(self.batch_size, self.min_buffer_size):\n",
        "            return None\n",
        "\n",
        "        # Sample from replay buffer\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "        states      = torch.tensor(states, dtype=torch.float32, device=self.device)\n",
        "        actions     = torch.tensor(actions, dtype=torch.long,   device=self.device).unsqueeze(1)\n",
        "        rewards     = torch.tensor(rewards, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float32, device=self.device)\n",
        "        dones       = torch.tensor(dones, dtype=torch.float32,  device=self.device).unsqueeze(1)\n",
        "\n",
        "        # Q(s,a) for current states\n",
        "        q_values = self.policy_net(states).gather(1, actions)\n",
        "\n",
        "        # ---------- DOUBLE DQN TARGET UPDATE ----------\n",
        "        with torch.no_grad():\n",
        "            # 1) Use policy net to select best next action\n",
        "            next_actions = self.policy_net(next_states).argmax(dim=1, keepdim=True)\n",
        "\n",
        "            # 2) Use target net to evaluate that action\n",
        "            next_q_target = self.target_net(next_states).gather(1, next_actions)\n",
        "\n",
        "            target_q_values = rewards + self.gamma * (1.0 - dones) * next_q_target\n",
        "\n",
        "        # Compute loss\n",
        "        loss = self.loss_fn(q_values, target_q_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "\n",
        "\n",
        "def run_experiment_v2(num_episodes=NUM_EPISODES):\n",
        "    \"\"\"\n",
        "    Run Experiment Version 2:\n",
        "    - Switch from standard DQN to Double DQN\n",
        "    - Train, evaluate, save metrics, and record video\n",
        "    \"\"\"\n",
        "    print(\"════ Running Experiment V2: Double DQN ════\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize environment\n",
        "    env = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    # Initialize improved agent\n",
        "    agent = DoubleDQNAgent(state_dim, action_dim, device=device)\n",
        "\n",
        "    # Train Double DQN agent\n",
        "    rewards, losses = train_dqn(agent, env, num_episodes)\n",
        "    env.close()\n",
        "\n",
        "    # Save model & training metrics\n",
        "    MODEL_PATH_V2 = \"flappy_bird_dqn_v2_double.pth\"\n",
        "    save_model(agent, MODEL_PATH_V2)\n",
        "\n",
        "    np.save(\"rewards_v2.npy\", rewards)\n",
        "    np.save(\"losses_v2.npy\", losses)\n",
        "\n",
        "    # Plot learning curves\n",
        "    plot_training_results(rewards, losses)\n",
        "\n",
        "    # Evaluate on fresh environment\n",
        "    eval_env = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
        "    avg_reward, avg_steps, scores = evaluate_agent(agent, eval_env, num_episodes=20)\n",
        "    eval_env.close()\n",
        "\n",
        "    # Save evaluation results\n",
        "    np.save(\"eval_scores_v2.npy\", scores)\n",
        "    with open(\"eval_summary_v2.txt\", \"w\") as f:\n",
        "        f.write(f\"Avg reward: {avg_reward}\\n\")\n",
        "        f.write(f\"Avg steps: {avg_steps}\\n\")\n",
        "\n",
        "    print(\"════ V2 Evaluation Results ════\")\n",
        "    print(\"Avg reward:\", avg_reward)\n",
        "    print(\"Avg steps:\", avg_steps)\n",
        "    print(\"Scores:\", scores)\n",
        "\n",
        "    # Record video of trained agent\n",
        "    eval_env_video = gym.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=True)\n",
        "    VIDEO_FOLDER_V2 = \"./dqn_v2_double_video\"\n",
        "    record_video(eval_env_video, agent, VIDEO_FOLDER_V2)\n",
        "    eval_env_video.close()\n",
        "\n",
        "    print(\"✅ Experiment V2 completed.\")\n",
        "    return agent, rewards, losses\n",
        "\n",
        "# To run this experiment:\n",
        "agent_v2, rewards_v2, losses_v2 = run_experiment_v2()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmI5Fo53bVdL"
      },
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 12: EXPERIMENTS - VERSION 3 (MORE IMPROVEMENTS)\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "def train_dqn_with_reward_shaping(agent, env, num_episodes,\n",
        "                                  step_reward=0.1,\n",
        "                                  epsilon_start=1.0,\n",
        "                                  epsilon_end=0.01,\n",
        "                                  epsilon_decay=0.997):\n",
        "    \"\"\"\n",
        "    Training loop with reward shaping and tuned epsilon decay.\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    losses = []\n",
        "\n",
        "    epsilon = epsilon_start\n",
        "    global_step = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0.0\n",
        "        episode_losses = []\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state, epsilon)\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # ---------- REWARD SHAPING ----------\n",
        "            shaped_reward = reward + step_reward  # small bonus for staying alive\n",
        "\n",
        "            agent.replay_buffer.push(state, action, shaped_reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += shaped_reward\n",
        "\n",
        "            loss = agent.train_step()\n",
        "            if loss is not None:\n",
        "                episode_losses.append(loss)\n",
        "\n",
        "            global_step += 1\n",
        "            if global_step % TARGET_UPDATE == 0:\n",
        "                agent.update_target_network()\n",
        "\n",
        "            epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
        "\n",
        "        rewards.append(episode_reward)\n",
        "        losses.append(np.mean(episode_losses) if episode_losses else 0.0)\n",
        "\n",
        "        print(\n",
        "            f\"[V3] Episode {episode+1}/{num_episodes} \"\n",
        "            f\"Reward: {episode_reward:.2f} \"\n",
        "            f\"Avg loss: {losses[-1]:.4f} \"\n",
        "            f\"Epsilon: {epsilon:.3f}\"\n",
        "        )\n",
        "\n",
        "    return rewards, losses\n",
        "\n",
        "\n",
        "def run_experiment_v3(num_episodes=NUM_EPISODES):\n",
        "    print(\"════ Running Experiment V3: Double DQN + Reward Shaping ════\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    env = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    # Use DoubleDQNAgent again (stacking improvements)\n",
        "    agent = DoubleDQNAgent(state_dim, action_dim, device=device)\n",
        "\n",
        "    rewards, losses = train_dqn_with_reward_shaping(\n",
        "        agent, env, num_episodes,\n",
        "        step_reward=0.01,\n",
        "        epsilon_start=1.0,\n",
        "        epsilon_end=0.05,\n",
        "        epsilon_decay=0.997\n",
        "    )\n",
        "    env.close()\n",
        "\n",
        "    MODEL_PATH_V3 = \"flappy_bird_dqn_v3_shaped.pth\"\n",
        "    save_model(agent, MODEL_PATH_V3)\n",
        "\n",
        "    np.save(\"rewards_v3.npy\", rewards)\n",
        "    np.save(\"losses_v3.npy\", losses)\n",
        "\n",
        "    plot_training_results(rewards, losses)\n",
        "\n",
        "    eval_env = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
        "    avg_reward, avg_steps, scores = evaluate_agent(agent, eval_env, num_episodes=20)\n",
        "    eval_env.close()\n",
        "\n",
        "    np.save(\"eval_scores_v3.npy\", scores)\n",
        "    with open(\"eval_summary_v3.txt\", \"w\") as f:\n",
        "        f.write(f\"Avg reward: {avg_reward}\\n\")\n",
        "        f.write(f\"Avg steps: {avg_steps}\\n\")\n",
        "\n",
        "    print(\"════ V3 Evaluation ════\")\n",
        "    print(\"Avg reward:\", avg_reward)\n",
        "    print(\"Avg steps:\", avg_steps)\n",
        "\n",
        "    eval_env_video = gym.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=True)\n",
        "    VIDEO_FOLDER_V3 = \"./dqn_v3_shaped_video\"\n",
        "    record_video(eval_env_video, agent, VIDEO_FOLDER_V3)\n",
        "    eval_env_video.close()\n",
        "\n",
        "    print(\"✅ Experiment V3 completed.\")\n",
        "    return agent, rewards, losses\n",
        "\n",
        "# Run:\n",
        "agent_v3, rewards_v3, losses_v3 = run_experiment_v3()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgoAOT8CbVfH"
      },
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 13: RESULTS & ANALYSIS\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "def compare_learning_curves():\n",
        "    \"\"\"Plot rewards of V1, V2, V3 on one graph.\"\"\"\n",
        "    rewards_v1 = np.load(\"rewards_v1.npy\")\n",
        "    rewards_v2 = np.load(\"rewards_v2.npy\")\n",
        "    rewards_v3 = np.load(\"rewards_v3.npy\")\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(rewards_v1, label=\"V1: Basic DQN\")\n",
        "    plt.plot(rewards_v2, label=\"V2: Double DQN\")\n",
        "    plt.plot(rewards_v3, label=\"V3: Double DQN + Reward shaping\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Episode reward\")\n",
        "    plt.title(\"Training reward comparison\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def print_eval_summaries():\n",
        "    \"\"\"Print textual comparison of evaluation results.\"\"\"\n",
        "    def load_summary(path):\n",
        "        with open(path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "        avg_reward = float(lines[0].split(\":\")[1])\n",
        "        avg_steps = float(lines[1].split(\":\")[1])\n",
        "        return avg_reward, avg_steps\n",
        "\n",
        "    avg_r1, avg_s1 = load_summary(\"eval_summary_v1.txt\")\n",
        "    avg_r2, avg_s2 = load_summary(\"eval_summary_v2.txt\")\n",
        "    avg_r3, avg_s3 = load_summary(\"eval_summary_v3.txt\")\n",
        "\n",
        "    print(\"════ FINAL COMPARISON ════\")\n",
        "    print(f\"V1 Basic DQN:       reward={avg_r1:.2f}, steps={avg_s1:.1f}\")\n",
        "    print(f\"V2 Double DQN:      reward={avg_r2:.2f}, steps={avg_s2:.1f}\")\n",
        "    print(f\"V3 Double+Shaping:  reward={avg_r3:.2f}, steps={avg_s3:.1f}\")\n",
        "\n",
        "# Side-by-side videos are already produced as different folders:\n",
        "#random_agent_video/, dqn_v1_video/, dqn_v2_double_video/, dqn_v3_shaped_video/\n",
        "\n",
        "# After running all experiments:\n",
        "compare_learning_curves()\n",
        "print_eval_summaries()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}