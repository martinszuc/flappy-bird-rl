{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuZQ28L7aul2"
   },
   "source": [
    "### MPDA - Flappy Bird Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4sWOZ6Ui-Xw"
   },
   "source": [
    "### PROGRESS:\n",
    "[Date] [Name]: [What was done]\n",
    "\n",
    "[30.11.] [mszuc]: [Team work templates, Define tasks for project]\n",
    "\n",
    "[1.12.] [xsocha02]: [Added random baseline agent + video/GIF]\n",
    "\n",
    "\n",
    "### BLOCKERS:\n",
    "None yet\n",
    "\n",
    "### DECISIONS:\n",
    "- Using flappy-bird-gymnasium package for environment\n",
    "- Starting with simple feature space (12 values)\n",
    "- PyTorch for neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ngo3x4cpa1xX",
    "outputId": "af2fe20e-0028-484d-f05a-c97bf424411d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flappy-bird-gymnasium in ./venv/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.11/site-packages (2.9.1)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.11/site-packages (0.24.1)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.11/site-packages (3.10.7)\n",
      "Requirement already satisfied: imageio in ./venv/lib/python3.11/site-packages (2.37.2)\n",
      "Requirement already satisfied: gymnasium in ./venv/lib/python3.11/site-packages (from flappy-bird-gymnasium) (1.2.2)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.11/site-packages (from flappy-bird-gymnasium) (2.3.5)\n",
      "Requirement already satisfied: pygame in ./venv/lib/python3.11/site-packages (from flappy-bird-gymnasium) (2.6.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./venv/lib/python3.11/site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./venv/lib/python3.11/site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.11/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.11/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.11/site-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.11/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./venv/lib/python3.11/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./venv/lib/python3.11/site-packages (from gymnasium->flappy-bird-gymnasium) (3.1.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./venv/lib/python3.11/site-packages (from gymnasium->flappy-bird-gymnasium) (0.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mszuc/school/fekt-pda-project/venv/lib/python3.11/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 0: SETUP & INSTALLATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "!pip install flappy-bird-gymnasium torch torchvision matplotlib imageio\n",
    "\n",
    "import gymnasium as gym\n",
    "import flappy_bird_gymnasium\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import imageio\n",
    "# Note: moviepy removed - using imageio for GIF creation instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wkKS4C_L-qUU",
    "outputId": "11cdc356-165b-4205-9806-82ee433eeac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment loaded!\n",
      "Observation space: Box(0.0, 1.0, (180,), float64)\n",
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# Test environment loads\n",
    "env = gym.make('FlappyBird-v0', render_mode='rgb_array', use_lidar=True)\n",
    "print(f\"âœ… Environment loaded!\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "zNg5Rd4p9Kla",
    "outputId": "2b8fb7f5-9466-4c43-bbda-809ca4ee6f38"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mszuc/school/fekt-pda-project/venv/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        ...,\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202]],\n",
       "\n",
       "       [[ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        ...,\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202]],\n",
       "\n",
       "       [[ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        ...,\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]],\n",
       "\n",
       "       [[222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]],\n",
       "\n",
       "       [[222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]]], shape=(512, 288, 3), dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Render test\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "FL-jlTqCUPUl",
    "outputId": "1ad87458-4d14-4cab-e3a0-bbfe7be44633"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mszuc/school/fekt-pda-project/venv/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        ...,\n",
       "        [ 78, 192, 202],\n",
       "        [ 84,  56,  71],\n",
       "        [ 84,  56,  71]],\n",
       "\n",
       "       [[ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        ...,\n",
       "        [ 78, 192, 202],\n",
       "        [ 84,  56,  71],\n",
       "        [ 84,  56,  71]],\n",
       "\n",
       "       [[ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        ...,\n",
       "        [ 78, 192, 202],\n",
       "        [ 84,  56,  71],\n",
       "        [ 84,  56,  71]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]],\n",
       "\n",
       "       [[222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]],\n",
       "\n",
       "       [[222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]]], shape=(512, 288, 3), dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = 0\n",
    "env.step(action)\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 552
    },
    "id": "qmbVKtMpUSZM",
    "outputId": "7a925cce-e1a1-494e-87ff-05d4775c9553"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        ...,\n",
       "        [132, 169,  68],\n",
       "        [148, 183,  81],\n",
       "        [148, 183,  81]],\n",
       "\n",
       "       [[ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        ...,\n",
       "        [132, 169,  68],\n",
       "        [148, 183,  81],\n",
       "        [148, 183,  81]],\n",
       "\n",
       "       [[ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        ...,\n",
       "        [132, 170,  69],\n",
       "        [148, 183,  81],\n",
       "        [148, 183,  81]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]],\n",
       "\n",
       "       [[222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]],\n",
       "\n",
       "       [[222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]]], shape=(512, 288, 3), dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = 1\n",
    "env.step(action)\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bEXl_IsAa2Jw"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVfjqgGjaP4P"
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 1: HYPERPARAMETERS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# FIXED: Lower learning rate for stability (was 1e-3)\n",
    "LEARNING_RATE = 5e-4\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "# FIXED: Epsilon decay is now applied PER EPISODE, not per step\n",
    "# With 0.995, after 500 episodes: 0.995^500 â‰ˆ 0.08 (good exploration)\n",
    "EPSILON_DECAY = 0.995\n",
    "BATCH_SIZE = 64\n",
    "MEMORY_SIZE = 10000\n",
    "# FIXED: More frequent target updates for stability (was 100)\n",
    "TARGET_UPDATE = 50\n",
    "NUM_EPISODES = 1000  # Reduced for faster iteration, increase for final training\n",
    "MODEL_PATH = 'flappy_bird_dqn_v1.pth' # Must match the save path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eKw9LqZ9bJM6",
    "outputId": "cfe6a2f9-15e3-42c5-d63d-569f0c1db7c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (180,)\n",
      "Info: {'score': 0}\n",
      "Last 10 values: [0.87077653 0.86325566 0.85355176 0.84422975 0.83749654 0.83099074\n",
      " 0.82471771 0.81868282 0.81289137 0.80734859]\n",
      "After action: [0.92584822 0.91578435 0.90606844 0.8967118  0.88772578 0.88123633\n",
      " 0.87496653 0.86892114 0.86310487 0.85752239]\n",
      "After action: [0.97139963 0.96137938 0.95169132 0.9423457  0.93335279 0.92684578\n",
      " 0.91849489 0.91248114 0.90668591 0.90111343]\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 2: ENVIRONMENT EXPLORATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def explore_environment():\n",
    "    \"\"\"\n",
    "    Understand state/action space of Flappy Bird.\n",
    "    Prints shapes, runs random episode.\n",
    "    \"\"\"\n",
    "    # TODO: Implement exploration\n",
    "    observation, info = env.reset()\n",
    "    print(f\"Observation shape: {observation.shape}\")\n",
    "    print(f\"Info: {info}\")\n",
    "\n",
    "    print(f\"Last 10 values: {observation[-10:]}\")\n",
    "\n",
    "    # jump\n",
    "    action = 1\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # bird jumped up => lower values increased\n",
    "    print(f\"After action: {observation[-10:]}\")\n",
    "\n",
    "    # don't jump\n",
    "    action = 0\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # still rising\n",
    "    print(f\"After action: {observation[-10:]}\")\n",
    "    action = 0\n",
    "\n",
    "env = gym.make('FlappyBird-v0', render_mode='rgb_array', use_lidar=True)\n",
    "explore_environment()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sp1Iss8rbNNd",
    "outputId": "065c36f3-7058-4c1e-c881-867e3e46ce88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: -7.079999999999998\n",
      "Average steps: 50.0\n",
      "Videos saved in /content/random_agent_video/\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "got an unexpected keyword argument 'program'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# convert to gif\u001b[39;00m\n\u001b[32m     44\u001b[39m video = VideoFileClip(\u001b[33m\"\u001b[39m\u001b[33mrandom_agent_video/rl-video-episode-0.mp4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mvideo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_gif\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrandom_agent.gif\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mffmpeg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGIF saved in /content/random_agent.gif\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# When randomly choosing actions, bird almost always flies up to the top of the screen\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/school/fekt-pda-project/venv/lib/python3.11/site-packages/decorator.py:234\u001b[39m, in \u001b[36mdecorate.<locals>.fun\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfun\u001b[39m(*args, **kw):\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         args, kw = \u001b[43mfix\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, *(extras + args), **kw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/school/fekt-pda-project/venv/lib/python3.11/site-packages/decorator.py:204\u001b[39m, in \u001b[36mfix\u001b[39m\u001b[34m(args, kwargs, sig)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfix\u001b[39m(args, kwargs, sig):\n\u001b[32m    201\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[33;03m    Fix args and kwargs to be consistent with the signature\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     ba = \u001b[43msig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m     ba.apply_defaults()  \u001b[38;5;66;03m# needed for test_dan_schult\u001b[39;00m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ba.args, ba.kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/inspect.py:3195\u001b[39m, in \u001b[36mSignature.bind\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m   3191\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[32m   3192\u001b[39m \u001b[33;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[32m   3193\u001b[39m \u001b[33;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[32m   3194\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/inspect.py:3184\u001b[39m, in \u001b[36mSignature._bind\u001b[39m\u001b[34m(self, args, kwargs, partial)\u001b[39m\n\u001b[32m   3182\u001b[39m         arguments[kwargs_param.name] = kwargs\n\u001b[32m   3183\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3184\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   3185\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mgot an unexpected keyword argument \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m   3186\u001b[39m                 arg=\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n\u001b[32m   3188\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_arguments_cls(\u001b[38;5;28mself\u001b[39m, arguments)\n",
      "\u001b[31mTypeError\u001b[39m: got an unexpected keyword argument 'program'"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 3: RANDOM BASELINE AGENT\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def random_agent_baseline(env, num_episodes=10):\n",
    "    \"\"\"\n",
    "    Random agent for baseline performance.\n",
    "\n",
    "    Returns:\n",
    "        avg_score: Average score over episodes\n",
    "        avg_steps: Average survival time\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    steps = []\n",
    "    for episode in range(num_episodes):\n",
    "        observation, info = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        steps_taken = 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            score += reward\n",
    "            done = terminated or truncated\n",
    "            steps_taken += 1\n",
    "        scores.append(score)\n",
    "        steps.append(steps_taken)\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "    avg_steps = np.mean(steps)\n",
    "\n",
    "    return avg_score, avg_steps\n",
    "\n",
    "env = gym.make('FlappyBird-v0', render_mode='rgb_array', use_lidar=True)\n",
    "env = RecordVideo(env, 'random_agent_video', episode_trigger=lambda x: True)\n",
    "avg_score, avg_steps = random_agent_baseline(env)\n",
    "env.close()\n",
    "\n",
    "print(f\"Average score: {avg_score}\")\n",
    "print(f\"Average steps: {avg_steps}\")\n",
    "print(\"Videos saved in /content/random_agent_video/\")\n",
    "\n",
    "# convert to gif (using imageio instead of moviepy for better compatibility)\n",
    "import imageio\n",
    "reader = imageio.get_reader(\"random_agent_video/rl-video-episode-0.mp4\")\n",
    "frames = [frame for frame in reader]\n",
    "imageio.mimsave(\"random_agent.gif\", frames, fps=30)\n",
    "reader.close()\n",
    "print(\"GIF saved as random_agent.gif\")\n",
    "\n",
    "# When randomly choosing actions, bird almost always flies up to the top of the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3hgeel4bNPf",
    "outputId": "36a49e08-ac17-4ef7-c91e-50bd74f24114"
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 4: DQN NETWORK\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"\n",
    "        Deep Q-Network.\n",
    "\n",
    "        Args:\n",
    "            state_dim: Dimension of state space (12 for features, 180 for LIDAR)\n",
    "            action_dim: Number of actions (2: nothing/flap)\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # 2â€“3 fully connected layers with ReLU\n",
    "        hidden1 = 128\n",
    "        hidden2 = 128\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: State tensor [batch_size, state_dim]\n",
    "\n",
    "        Returns:\n",
    "            Q-values: [batch_size, action_dim]\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Model Save / Load\n",
    "# -----------------------------------------------------------pabad\n",
    "\n",
    "def save_dqn(model, path):\n",
    "    \"\"\"\n",
    "    Save DQN model weights to file.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "\n",
    "def load_dqn(path, state_dim, action_dim, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Load DQN model weights from file.\n",
    "\n",
    "    Returns:\n",
    "        model: DQN instance with loaded weights (in eval mode)\n",
    "    \"\"\"\n",
    "    model = DQN(state_dim, action_dim).to(device)\n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "#test\n",
    "env = gym.make('FlappyBird-v0', use_lidar=True)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "model = DQN(state_dim, action_dim)\n",
    "test_state = torch.randn(1, state_dim)   # fake input\n",
    "q_values = model(test_state)\n",
    "\n",
    "print(\"State dim:\", state_dim)\n",
    "print(\"Action dim:\", action_dim)\n",
    "print(\"Q-values:\", q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HhL0sLWlbNRb",
    "outputId": "89178983-e552-4bea-c98d-be285fd66a8f"
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 5: EXPERIENCE REPLAY BUFFER\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        Experience replay buffer.\n",
    "\n",
    "        Args:\n",
    "            capacity: Maximum number of experiences to store\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)   # auto-removes old items\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store one experience into the buffer.\"\"\"\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a random batch of experiences.\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Unzip the batch\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert to NumPy arrays (PyTorch will convert later)\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states),\n",
    "            np.array(dones, dtype=np.float32),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return current size of the buffer.\"\"\"\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "#test\n",
    "buf = ReplayBuffer(100)\n",
    "\n",
    "buf.push([1,2,3], 1, 0.5, [1,2,4], False)\n",
    "buf.push([4,5,6], 0, 1.0, [4,5,7], True)\n",
    "\n",
    "print(\"Buffer size:\", len(buf))\n",
    "batch = buf.sample(2)\n",
    "\n",
    "print(\"Sampled states:\", batch[0])\n",
    "print(\"Sampled actions:\", batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kyWpBiJzbVv4",
    "outputId": "718c4d8b-a51a-40a3-f511-9eaf288e6784"
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 6: DQN AGENT\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, device=None):\n",
    "        \"\"\"\n",
    "        DQN Agent with target network.\n",
    "\n",
    "        Args:\n",
    "            state_dim: State space dimension\n",
    "            action_dim: Action space dimension\n",
    "        \"\"\"\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # Device (CPU / GPU)\n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = device\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.gamma = 0.99           # discount factor\n",
    "        self.batch_size = 64\n",
    "        self.learning_rate = LEARNING_RATE  # Use global hyperparameter\n",
    "        self.replay_capacity = 50_000\n",
    "        self.min_buffer_size = 1_000  # start training only after this many transitions\n",
    "\n",
    "        # Networks\n",
    "        self.policy_net = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_net = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()  # target net in eval mode\n",
    "\n",
    "        # Optimizer & loss\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(self.replay_capacity)\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Epsilon-greedy action selection.\n",
    "\n",
    "        Args:\n",
    "            state: Current state (np.array shape [state_dim])\n",
    "            epsilon: Exploration probability\n",
    "\n",
    "        Returns:\n",
    "            action: 0 or 1\n",
    "        \"\"\"\n",
    "        # Explore\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "\n",
    "        # Exploit\n",
    "        state_tensor = torch.tensor(\n",
    "            state, dtype=torch.float32, device=self.device\n",
    "        ).unsqueeze(0)  # [1, state_dim]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)  # [1, action_dim]\n",
    "            action = q_values.argmax(dim=1).item()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def train_step(self):\n",
    "        \"\"\"\n",
    "        Sample batch and update network.\n",
    "\n",
    "        Returns:\n",
    "            loss_value: float or None (if not enough data)\n",
    "        \"\"\"\n",
    "        # Don't train until buffer has enough samples\n",
    "        if len(self.replay_buffer) < max(self.batch_size, self.min_buffer_size):\n",
    "            return None\n",
    "\n",
    "        # Sample batch from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        states      = torch.tensor(states, dtype=torch.float32, device=self.device)\n",
    "        actions     = torch.tensor(actions, dtype=torch.long,   device=self.device).unsqueeze(1)   # [B,1]\n",
    "        rewards     = torch.tensor(rewards, dtype=torch.float32, device=self.device).unsqueeze(1) # [B,1]\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32, device=self.device)\n",
    "        dones       = torch.tensor(dones, dtype=torch.float32,  device=self.device).unsqueeze(1)  # [B,1]\n",
    "\n",
    "        # Current Q-values from policy network for taken actions\n",
    "        q_values = self.policy_net(states).gather(1, actions)  # [B,1]\n",
    "\n",
    "        # Target Q-values using target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(dim=1, keepdim=True)[0]  # [B,1]\n",
    "            target_q_values = rewards + self.gamma * (1.0 - dones) * next_q_values\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # FIXED: Enable gradient clipping for training stability\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from policy to target network.\"\"\"\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "env = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_dim, action_dim)\n",
    "\n",
    "state, _ = env.reset()\n",
    "action = agent.select_action(state, epsilon=0.5)\n",
    "print(\"Sampled action:\", action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2waCE8JgvU9"
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 8: HELPER FUNCTIONS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def moving_average(data, window=50):\n",
    "    \"\"\"Calculate moving average for smoother visualization.\"\"\"\n",
    "    if len(data) < window:\n",
    "        return data\n",
    "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "def plot_training_results(rewards, losses):\n",
    "    \"\"\"Plot reward curves and training losses with moving averages.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Rewards plot\n",
    "    axes[0].plot(rewards, alpha=0.3, label='Raw rewards')\n",
    "    if len(rewards) >= 50:\n",
    "        ma_rewards = moving_average(rewards, 50)\n",
    "        axes[0].plot(range(49, len(rewards)), ma_rewards, label='Moving avg (50 ep)', linewidth=2)\n",
    "    axes[0].set_xlabel('Episode')\n",
    "    axes[0].set_ylabel('Reward')\n",
    "    axes[0].set_title('Training Rewards')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Loss plot\n",
    "    axes[1].plot(losses, alpha=0.3, label='Raw loss')\n",
    "    if len(losses) >= 50:\n",
    "        ma_losses = moving_average(losses, 50)\n",
    "        axes[1].plot(range(49, len(losses)), ma_losses, label='Moving avg (50 ep)', linewidth=2)\n",
    "    axes[1].set_xlabel('Episode')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].set_title('Training Losses')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\\\nğŸ“Š Training Summary:\")\n",
    "    print(f\"   Final 100 episodes - Avg Reward: {np.mean(rewards[-100:]):.2f}, Max: {np.max(rewards[-100:]):.2f}\")\n",
    "    print(f\"   Best episode reward: {np.max(rewards):.2f}\")\n",
    "\n",
    "def save_model(agent, filepath):\n",
    "    \"\"\"Save model weights (agent's primary Q-Network: policy_net).\"\"\"\n",
    "    torch.save(agent.policy_net.state_dict(), filepath)\n",
    "    print(f\"âœ… Model saved to {filepath}\")\n",
    "\n",
    "def load_model(agent, filepath):\n",
    "    \"\"\"Load model weights into agent's primary Q-Network: policy_net.\"\"\"\n",
    "    state_dict = torch.load(filepath, map_location=agent.device)\n",
    "    agent.policy_net.load_state_dict(state_dict)\n",
    "    agent.policy_net.eval() # Set network to evaluation mode after loading\n",
    "    print(f\"âœ… Model loaded from {filepath}\")\n",
    "    return agent\n",
    "\n",
    "def record_video(env, agent, filename):\n",
    "    \"\"\"Record agent gameplay video.\"\"\"\n",
    "    # Set a seed for consistent visualization\n",
    "    SEED = 42\n",
    "\n",
    "    # 1. Wrap the environment for recording.\n",
    "    video_env = RecordVideo(env, filename, episode_trigger=lambda x: True, name_prefix='eval_run')\n",
    "\n",
    "    # 2. Run an episode\n",
    "    state, _ = video_env.reset(seed=SEED)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "\n",
    "    while not done:\n",
    "        # *** FIX: Using agent.select_action and epsilon=0.0 ***\n",
    "        # Action selection uses the trained agent's greedy policy\n",
    "        action = agent.select_action(state, epsilon=0.0)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = video_env.step(action)\n",
    "        done = terminated or truncated\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "\n",
    "    video_env.close()\n",
    "    print(f\"âœ… Video recorded in the '{filename}' directory. Total Reward: {total_reward:.2f}, Steps: {step}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MFs3iP3vQ9gm",
    "outputId": "67ce326f-0860-4fe5-ec14-213f88f1b37b"
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 7: TRAINING LOOP\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def train_dqn(agent, env, num_episodes):\n",
    "    \"\"\"\n",
    "    Main training loop.\n",
    "\n",
    "    Args:\n",
    "        agent: DQNAgent instance\n",
    "        env: Gymnasium environment\n",
    "        num_episodes: Number of episodes to train\n",
    "\n",
    "    Returns:\n",
    "        rewards: List of episode rewards\n",
    "        losses: List of training losses (avg per episode)\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    losses = []\n",
    "\n",
    "    epsilon = EPSILON_START\n",
    "    global_step = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0.0\n",
    "        episode_losses = []\n",
    "\n",
    "        while not done:\n",
    "            # --- Epsilon-greedy action selection ---\n",
    "            action = agent.select_action(state, epsilon)\n",
    "\n",
    "            # --- Interact with environment ---\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # --- Store experience in replay buffer ---\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            # --- Train on a random batch from replay buffer ---\n",
    "            loss = agent.train_step()\n",
    "            if loss is not None:\n",
    "                episode_losses.append(loss)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            # --- Periodically update target network ---\n",
    "            if global_step % TARGET_UPDATE == 0:\n",
    "                agent.update_target_network()\n",
    "\n",
    "        # end of episode\n",
    "        rewards.append(episode_reward)\n",
    "        \n",
    "        # FIXED: Decay epsilon ONCE PER EPISODE (was inside while loop - too fast!)\n",
    "        # This ensures proper exploration throughout training\n",
    "        epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
    "        if episode_losses:\n",
    "            avg_loss = np.mean(episode_losses)\n",
    "        else:\n",
    "            avg_loss = 0.0\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode + 1}/{num_episodes} | \"\n",
    "            f\"Reward: {episode_reward:.2f} | \"\n",
    "            f\"Avg loss: {avg_loss:.4f} | \"\n",
    "            f\"Epsilon: {epsilon:.3f}\"\n",
    "        )\n",
    "\n",
    "    return rewards, losses\n",
    "\n",
    "#test\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_dim, action_dim, device=device)\n",
    "\n",
    "rewards, losses = train_dqn(agent, env, NUM_EPISODES)\n",
    "\n",
    "save_model(agent, MODEL_PATH)\n",
    "\n",
    "# NEW: save metrics (optional, but nice)\n",
    "np.save(\"rewards_v1.npy\", rewards)\n",
    "np.save(\"losses_v1.npy\", losses)\n",
    "\n",
    "# NEW: show learning curves\n",
    "plot_training_results(rewards, losses)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MlUQ5PBJ7nEi",
    "outputId": "ba4f621b-57aa-4c35-b606-16536d044eab"
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 9.5: VIDEO\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "LIDAR_OBS = True\n",
    "ENV_ID = 'FlappyBird-v0'\n",
    "\n",
    "MODEL_PATH = 'flappy_bird_dqn_v1.pth' # Must match the save path\n",
    "VIDEO_FOLDER = './dqn_v1_performance_video'\n",
    "\n",
    "# 1. Instantiate a new evaluation agent\n",
    "eval_agent = DQNAgent(state_dim, action_dim, device=device)\n",
    "\n",
    "# 2. Load the model\n",
    "eval_agent = load_model(eval_agent, MODEL_PATH)\n",
    "\n",
    "# 3. Create an evaluation environment (must be 'rgb_array' for video)\n",
    "eval_env = gym.make(\n",
    "    ENV_ID,\n",
    "    render_mode='rgb_array',\n",
    "    use_lidar=LIDAR_OBS\n",
    ")\n",
    "\n",
    "# 4. Record the video\n",
    "record_video(eval_env, eval_agent, VIDEO_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nB9N0Bnng3D2",
    "outputId": "d88dcc79-7fc5-4f00-d723-db039982ea68"
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 9: EVALUATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def evaluate_agent(agent, env, num_episodes=10):\n",
    "    \"\"\"\n",
    "    Evaluate agent performance without exploration (epsilon=0).\n",
    "\n",
    "    Args:\n",
    "        agent: trained DQNAgent\n",
    "        env: evaluation environment (no video wrapper)\n",
    "        num_episodes: number of episodes to evaluate\n",
    "\n",
    "    Returns:\n",
    "        avg_reward: Average episode reward\n",
    "        avg_steps: Average number of steps survived\n",
    "        scores: List of individual episode rewards\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    steps_list = []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        ep_steps = 0\n",
    "\n",
    "        while not done:\n",
    "            # Greedy action selection (no exploration)\n",
    "            action = agent.select_action(state, epsilon=0.0)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            ep_steps += 1\n",
    "\n",
    "        scores.append(episode_reward)\n",
    "        steps_list.append(ep_steps)\n",
    "\n",
    "    avg_reward = float(np.mean(scores))\n",
    "    avg_steps = float(np.mean(steps_list))\n",
    "\n",
    "    return avg_reward, avg_steps, scores\n",
    "\n",
    "\n",
    "# --- RUN EVALUATION ---\n",
    "# Make a fresh evaluation environment\n",
    "eval_env2 = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
    "\n",
    "# Evaluate the trained model (eval_agent must be loaded in Section 9.5)\n",
    "avg_reward, avg_steps, scores = evaluate_agent(eval_agent, eval_env2, num_episodes=20)\n",
    "\n",
    "eval_env2.close()\n",
    "\n",
    "print(\"â•â•â•â•â•â•â•â• Evaluation Results â•â•â•â•â•â•â•â•\")\n",
    "print(\"Avg reward:\", avg_reward)\n",
    "print(\"Avg steps:\", avg_steps)\n",
    "print(\"Scores per episode:\", scores)\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "\n",
    "np.save(\"eval_scores_v1.npy\", scores)\n",
    "with open(\"eval_summary_v1.txt\", \"w\") as f:\n",
    "    f.write(f\"Avg reward: {avg_reward}\\n\")\n",
    "    f.write(f\"Avg steps: {avg_steps}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aa-a2kIDbUro",
    "outputId": "ecb5b302-22c9-4585-bcf6-1ad8b1902e13"
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 10: EXPERIMENTS - VERSION 1 (BASIC DQN)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def run_experiment_v1(num_episodes=NUM_EPISODES):\n",
    "    print(\"â•â•â•â• Running Experiment V1: Basic DQN â•â•â•â•\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1) Environment + agent\n",
    "    env = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_dim, action_dim, device=device)\n",
    "\n",
    "    # 2) Train\n",
    "    rewards, losses = train_dqn(agent, env, num_episodes)\n",
    "    env.close()\n",
    "\n",
    "    # 3) Save model + metrics\n",
    "    MODEL_PATH_V1 = \"flappy_bird_dqn_v1.pth\"\n",
    "    save_model(agent, MODEL_PATH_V1)\n",
    "\n",
    "    np.save(\"rewards_v1.npy\", rewards)\n",
    "    np.save(\"losses_v1.npy\", losses)\n",
    "\n",
    "    # 4) Plot learning curves\n",
    "    plot_training_results(rewards, losses)\n",
    "\n",
    "    # 5) Evaluation\n",
    "    eval_env = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
    "    avg_reward, avg_steps, scores = evaluate_agent(agent, eval_env, num_episodes=20)\n",
    "    eval_env.close()\n",
    "\n",
    "    np.save(\"eval_scores_v1.npy\", scores)\n",
    "    with open(\"eval_summary_v1.txt\", \"w\") as f:\n",
    "        f.write(f\"Avg reward: {avg_reward}\\n\")\n",
    "        f.write(f\"Avg steps: {avg_steps}\\n\")\n",
    "\n",
    "    print(\"â•â•â•â• V1 Evaluation â•â•â•â•\")\n",
    "    print(\"Avg reward:\", avg_reward)\n",
    "    print(\"Avg steps:\", avg_steps)\n",
    "    print(\"Scores:\", scores)\n",
    "\n",
    "    # 6) Record video\n",
    "    eval_env_video = gym.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=True)\n",
    "    VIDEO_FOLDER_V1 = \"./dqn_v1_video\"\n",
    "    record_video(eval_env_video, agent, VIDEO_FOLDER_V1)\n",
    "    eval_env_video.close()\n",
    "\n",
    "    print(\"âœ… Experiment V1 completed.\")\n",
    "    return agent, rewards, losses\n",
    "\n",
    "# To actually run:\n",
    "agent_v1, rewards_v1, losses_v1 = run_experiment_v1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dQGk4nTEbVZB",
    "outputId": "7fc424c4-557f-4381-d322-92d18f29edf0"
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 11: EXPERIMENTS - VERSION 2 (IMPROVEMENTS: DOUBLE DQN)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class DoubleDQNAgent(DQNAgent):\n",
    "    \"\"\"\n",
    "    Extends the basic DQNAgent by implementing the Double DQN update rule.\n",
    "    Double DQN reduces overestimation bias and improves stability.\n",
    "    \"\"\"\n",
    "\n",
    "    def train_step(self):\n",
    "        # Donâ€™t train until we have enough samples\n",
    "        if len(self.replay_buffer) < max(self.batch_size, self.min_buffer_size):\n",
    "            return None\n",
    "\n",
    "        # Sample from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        states      = torch.tensor(states, dtype=torch.float32, device=self.device)\n",
    "        actions     = torch.tensor(actions, dtype=torch.long,   device=self.device).unsqueeze(1)\n",
    "        rewards     = torch.tensor(rewards, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32, device=self.device)\n",
    "        dones       = torch.tensor(dones, dtype=torch.float32,  device=self.device).unsqueeze(1)\n",
    "\n",
    "        # Q(s,a) for current states\n",
    "        q_values = self.policy_net(states).gather(1, actions)\n",
    "\n",
    "        # ---------- DOUBLE DQN TARGET UPDATE ----------\n",
    "        with torch.no_grad():\n",
    "            # 1) Use policy net to select best next action\n",
    "            next_actions = self.policy_net(next_states).argmax(dim=1, keepdim=True)\n",
    "\n",
    "            # 2) Use target net to evaluate that action\n",
    "            next_q_target = self.target_net(next_states).gather(1, next_actions)\n",
    "\n",
    "            target_q_values = rewards + self.gamma * (1.0 - dones) * next_q_target\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # FIXED: Enable gradient clipping for training stability\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "\n",
    "def run_experiment_v2(num_episodes=NUM_EPISODES):\n",
    "    \"\"\"\n",
    "    Run Experiment Version 2:\n",
    "    - Switch from standard DQN to Double DQN\n",
    "    - Train, evaluate, save metrics, and record video\n",
    "    \"\"\"\n",
    "    print(\"â•â•â•â• Running Experiment V2: Double DQN â•â•â•â•\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize environment\n",
    "    env = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # Initialize improved agent\n",
    "    agent = DoubleDQNAgent(state_dim, action_dim, device=device)\n",
    "\n",
    "    # Train Double DQN agent\n",
    "    rewards, losses = train_dqn(agent, env, num_episodes)\n",
    "    env.close()\n",
    "\n",
    "    # Save model & training metrics\n",
    "    MODEL_PATH_V2 = \"flappy_bird_dqn_v2_double.pth\"\n",
    "    save_model(agent, MODEL_PATH_V2)\n",
    "\n",
    "    np.save(\"rewards_v2.npy\", rewards)\n",
    "    np.save(\"losses_v2.npy\", losses)\n",
    "\n",
    "    # Plot learning curves\n",
    "    plot_training_results(rewards, losses)\n",
    "\n",
    "    # Evaluate on fresh environment\n",
    "    eval_env = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
    "    avg_reward, avg_steps, scores = evaluate_agent(agent, eval_env, num_episodes=20)\n",
    "    eval_env.close()\n",
    "\n",
    "    # Save evaluation results\n",
    "    np.save(\"eval_scores_v2.npy\", scores)\n",
    "    with open(\"eval_summary_v2.txt\", \"w\") as f:\n",
    "        f.write(f\"Avg reward: {avg_reward}\\n\")\n",
    "        f.write(f\"Avg steps: {avg_steps}\\n\")\n",
    "\n",
    "    print(\"â•â•â•â• V2 Evaluation Results â•â•â•â•\")\n",
    "    print(\"Avg reward:\", avg_reward)\n",
    "    print(\"Avg steps:\", avg_steps)\n",
    "    print(\"Scores:\", scores)\n",
    "\n",
    "    # Record video of trained agent\n",
    "    eval_env_video = gym.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=True)\n",
    "    VIDEO_FOLDER_V2 = \"./dqn_v2_double_video\"\n",
    "    record_video(eval_env_video, agent, VIDEO_FOLDER_V2)\n",
    "    eval_env_video.close()\n",
    "\n",
    "    print(\"âœ… Experiment V2 completed.\")\n",
    "    return agent, rewards, losses\n",
    "\n",
    "# To run this experiment:\n",
    "agent_v2, rewards_v2, losses_v2 = run_experiment_v2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nmI5Fo53bVdL",
    "outputId": "d2e4d8c9-810d-44a4-b000-77a0c1f8a063"
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 12: EXPERIMENTS - VERSION 3 (MORE IMPROVEMENTS)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def train_dqn_with_reward_shaping(agent, env, num_episodes,\n",
    "                                  step_reward=0.1,\n",
    "                                  epsilon_start=1.0,\n",
    "                                  epsilon_end=0.01,\n",
    "                                  epsilon_decay=0.997):\n",
    "    \"\"\"\n",
    "    Training loop with reward shaping and tuned epsilon decay.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    losses = []\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "    global_step = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0.0\n",
    "        episode_losses = []\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_action(state, epsilon)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # ---------- REWARD SHAPING ----------\n",
    "            shaped_reward = reward + step_reward  # small bonus for staying alive\n",
    "\n",
    "            agent.replay_buffer.push(state, action, shaped_reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += shaped_reward\n",
    "\n",
    "            loss = agent.train_step()\n",
    "            if loss is not None:\n",
    "                episode_losses.append(loss)\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % TARGET_UPDATE == 0:\n",
    "                agent.update_target_network()\n",
    "\n",
    "        rewards.append(episode_reward)\n",
    "        \n",
    "        # FIXED: Decay epsilon ONCE PER EPISODE (was inside while loop)\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "        losses.append(np.mean(episode_losses) if episode_losses else 0.0)\n",
    "\n",
    "        print(\n",
    "            f\"[V3] Episode {episode+1}/{num_episodes} \"\n",
    "            f\"Reward: {episode_reward:.2f} \"\n",
    "            f\"Avg loss: {losses[-1]:.4f} \"\n",
    "            f\"Epsilon: {epsilon:.3f}\"\n",
    "        )\n",
    "\n",
    "    return rewards, losses\n",
    "\n",
    "\n",
    "def run_experiment_v3(num_episodes=NUM_EPISODES):\n",
    "    print(\"â•â•â•â• Running Experiment V3: Double DQN + Reward Shaping â•â•â•â•\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    env = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # Use DoubleDQNAgent again (stacking improvements)\n",
    "    agent = DoubleDQNAgent(state_dim, action_dim, device=device)\n",
    "\n",
    "    rewards, losses = train_dqn_with_reward_shaping(\n",
    "        agent, env, num_episodes,\n",
    "        step_reward=0.01,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.05,\n",
    "        epsilon_decay=0.997\n",
    "    )\n",
    "    env.close()\n",
    "\n",
    "    MODEL_PATH_V3 = \"flappy_bird_dqn_v3_shaped.pth\"\n",
    "    save_model(agent, MODEL_PATH_V3)\n",
    "\n",
    "    np.save(\"rewards_v3.npy\", rewards)\n",
    "    np.save(\"losses_v3.npy\", losses)\n",
    "\n",
    "    plot_training_results(rewards, losses)\n",
    "\n",
    "    eval_env = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
    "    avg_reward, avg_steps, scores = evaluate_agent(agent, eval_env, num_episodes=20)\n",
    "    eval_env.close()\n",
    "\n",
    "    np.save(\"eval_scores_v3.npy\", scores)\n",
    "    with open(\"eval_summary_v3.txt\", \"w\") as f:\n",
    "        f.write(f\"Avg reward: {avg_reward}\\n\")\n",
    "        f.write(f\"Avg steps: {avg_steps}\\n\")\n",
    "\n",
    "    print(\"â•â•â•â• V3 Evaluation â•â•â•â•\")\n",
    "    print(\"Avg reward:\", avg_reward)\n",
    "    print(\"Avg steps:\", avg_steps)\n",
    "\n",
    "    eval_env_video = gym.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=True)\n",
    "    VIDEO_FOLDER_V3 = \"./dqn_v3_shaped_video\"\n",
    "    record_video(eval_env_video, agent, VIDEO_FOLDER_V3)\n",
    "    eval_env_video.close()\n",
    "\n",
    "    print(\"âœ… Experiment V3 completed.\")\n",
    "    return agent, rewards, losses\n",
    "\n",
    "# Run:\n",
    "agent_v3, rewards_v3, losses_v3 = run_experiment_v3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "id": "vgoAOT8CbVfH",
    "outputId": "15d6672b-0516-4158-bfc0-df6ea0304d5d"
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 13: RESULTS & ANALYSIS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def compare_learning_curves():\n",
    "    \"\"\"Plot rewards of V1, V2, V3 on one graph with moving averages.\"\"\"\n",
    "    rewards_v1 = np.load(\"rewards_v1.npy\")\n",
    "    rewards_v2 = np.load(\"rewards_v2.npy\")\n",
    "    rewards_v3 = np.load(\"rewards_v3.npy\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    # Raw rewards (transparent)\n",
    "    axes[0].plot(rewards_v1, alpha=0.2, color='blue')\n",
    "    axes[0].plot(rewards_v2, alpha=0.2, color='orange')\n",
    "    axes[0].plot(rewards_v3, alpha=0.2, color='green')\n",
    "    \n",
    "    # Moving averages (solid lines)\n",
    "    window = 50\n",
    "    if len(rewards_v1) >= window:\n",
    "        ma_v1 = moving_average(rewards_v1, window)\n",
    "        axes[0].plot(range(window-1, len(rewards_v1)), ma_v1, label=\"V1: Basic DQN\", linewidth=2, color='blue')\n",
    "    if len(rewards_v2) >= window:\n",
    "        ma_v2 = moving_average(rewards_v2, window)\n",
    "        axes[0].plot(range(window-1, len(rewards_v2)), ma_v2, label=\"V2: Double DQN\", linewidth=2, color='orange')\n",
    "    if len(rewards_v3) >= window:\n",
    "        ma_v3 = moving_average(rewards_v3, window)\n",
    "        axes[0].plot(range(window-1, len(rewards_v3)), ma_v3, label=\"V3: Double DQN + Shaping\", linewidth=2, color='green')\n",
    "    \n",
    "    axes[0].set_xlabel(\"Episode\")\n",
    "    axes[0].set_ylabel(\"Episode reward\")\n",
    "    axes[0].set_title(\"Training Reward Comparison (Moving Avg)\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Random baseline')\n",
    "    \n",
    "    # Bar chart comparison\n",
    "    versions = ['V0\\nRandom', 'V1\\nBasic DQN', 'V2\\nDouble DQN', 'V3\\nDouble+Shaping']\n",
    "    final_rewards = [\n",
    "        -7.9,  # Random baseline\n",
    "        np.mean(rewards_v1[-100:]),\n",
    "        np.mean(rewards_v2[-100:]),\n",
    "        np.mean(rewards_v3[-100:])\n",
    "    ]\n",
    "    colors = ['red', 'blue', 'orange', 'green']\n",
    "    bars = axes[1].bar(versions, final_rewards, color=colors, alpha=0.7)\n",
    "    axes[1].set_ylabel(\"Average Reward (last 100 episodes)\")\n",
    "    axes[1].set_title(\"Final Performance Comparison\")\n",
    "    axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, final_rewards):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                    f'{val:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_eval_summaries():\n",
    "    \"\"\"Print textual comparison of evaluation results.\"\"\"\n",
    "    def load_summary(path):\n",
    "        with open(path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        avg_reward = float(lines[0].split(\":\")[1])\n",
    "        avg_steps = float(lines[1].split(\":\")[1])\n",
    "        return avg_reward, avg_steps\n",
    "\n",
    "    avg_r1, avg_s1 = load_summary(\"eval_summary_v1.txt\")\n",
    "    avg_r2, avg_s2 = load_summary(\"eval_summary_v2.txt\")\n",
    "    avg_r3, avg_s3 = load_summary(\"eval_summary_v3.txt\")\n",
    "\n",
    "    print(\"â•â•â•â• FINAL COMPARISON â•â•â•â•\")\n",
    "    print(f\"V1 Basic DQN:       reward={avg_r1:.2f}, steps={avg_s1:.1f}\")\n",
    "    print(f\"V2 Double DQN:      reward={avg_r2:.2f}, steps={avg_s2:.1f}\")\n",
    "    print(f\"V3 Double+Shaping:  reward={avg_r3:.2f}, steps={avg_s3:.1f}\")\n",
    "\n",
    "# Side-by-side videos are already produced as different folders:\n",
    "#random_agent_video/, dqn_v1_video/, dqn_v2_double_video/, dqn_v3_shaped_video/\n",
    "\n",
    "# After running all experiments:\n",
    "compare_learning_curves()\n",
    "print_eval_summaries()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
