{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### MPDA - Flappy Bird Reinforcement Learning\n"
      ],
      "metadata": {
        "id": "QuZQ28L7aul2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PROGRESS:\n",
        "[Date] [Name]: [What was done]\n",
        "\n",
        "[30.11.] [mszuc]: [Team work templates, Define tasks for project]\n",
        "\n",
        "[1.12.] [xsocha02]: [Added random baseline agent + video/GIF]\n",
        "\n",
        "\n",
        "### BLOCKERS:\n",
        "None yet\n",
        "\n",
        "### DECISIONS:\n",
        "- Using flappy-bird-gymnasium package for environment\n",
        "- Starting with simple feature space (12 values)\n",
        "- PyTorch for neural networks"
      ],
      "metadata": {
        "id": "z4sWOZ6Ui-Xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 0: SETUP & INSTALLATION\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "!pip install flappy-bird-gymnasium torch torchvision matplotlib imageio\n",
        "\n",
        "import gymnasium as gym\n",
        "import flappy_bird_gymnasium\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import random\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import imageio\n",
        "from moviepy.video.io.VideoFileClip import VideoFileClip\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngo3x4cpa1xX",
        "outputId": "9627147b-bb5e-4152-efd2-959e4f5454dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flappy-bird-gymnasium in /usr/local/lib/python3.12/dist-packages (0.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (2.37.2)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (from flappy-bird-gymnasium) (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from flappy-bird-gymnasium) (2.0.2)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.12/dist-packages (from flappy-bird-gymnasium) (2.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->flappy-bird-gymnasium) (3.1.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium->flappy-bird-gymnasium) (0.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test environment loads\n",
        "env = gym.make('FlappyBird-v0', render_mode='rgb_array', use_lidar=True)\n",
        "print(f\"✅ Environment loaded!\")\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "print(f\"Action space: {env.action_space}\")"
      ],
      "metadata": {
        "id": "wkKS4C_L-qUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "643f81ed-9ee6-42da-85a4-20368c8c3675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Environment loaded!\n",
            "Observation space: Box(0.0, 1.0, (180,), float64)\n",
            "Action space: Discrete(2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Render test\n",
        "env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "id": "zNg5Rd4p9Kla",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "outputId": "ec6531d7-e5df-4a2b-9a45-1c0e2f2add6d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-8c83f582-ee6a-47a8-9158-521787e89b86\" class=\"ndarray_repr\"><pre>ndarray (512, 288, 3) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,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\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-8c83f582-ee6a-47a8-9158-521787e89b86 button').onclick = (e) => {\n",
              "        document.querySelector('#id-8c83f582-ee6a-47a8-9158-521787e89b86').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-8c83f582-ee6a-47a8-9158-521787e89b86 button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action = 0\n",
        "env.step(action)\n",
        "\n",
        "env.render()"
      ],
      "metadata": {
        "id": "FL-jlTqCUPUl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "outputId": "7f320a5b-8fb6-407f-e275-1559a48214aa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 84,  56,  71],\n",
              "        [ 84,  56,  71]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 84,  56,  71],\n",
              "        [ 84,  56,  71]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 84,  56,  71],\n",
              "        [ 84,  56,  71]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-20f8cfee-4eff-428f-b9d7-8908c8500fee\" class=\"ndarray_repr\"><pre>ndarray (512, 288, 3) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,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\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 84,  56,  71],\n",
              "        [ 84,  56,  71]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 84,  56,  71],\n",
              "        [ 84,  56,  71]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [ 78, 192, 202],\n",
              "        [ 84,  56,  71],\n",
              "        [ 84,  56,  71]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-20f8cfee-4eff-428f-b9d7-8908c8500fee button').onclick = (e) => {\n",
              "        document.querySelector('#id-20f8cfee-4eff-428f-b9d7-8908c8500fee').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-20f8cfee-4eff-428f-b9d7-8908c8500fee button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action = 1\n",
        "env.step(action)\n",
        "\n",
        "env.render()"
      ],
      "metadata": {
        "id": "qmbVKtMpUSZM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "outputId": "9249ba8b-6e7e-4d07-d075-6be093e14385",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [132, 169,  68],\n",
              "        [148, 183,  81],\n",
              "        [148, 183,  81]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [132, 169,  68],\n",
              "        [148, 183,  81],\n",
              "        [148, 183,  81]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [132, 169,  69],\n",
              "        [148, 184,  80],\n",
              "        [148, 184,  80]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-d422219c-dbfd-4ba4-8250-a31888b745f1\" class=\"ndarray_repr\"><pre>ndarray (512, 288, 3) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASAAAAIACAIAAACtpSZ8AAAeX0lEQVR4nO3dW4wk133f8W9199x2lhTJHZLiRWQSwTCsC0UiUGRBsRInACMDgh6CJLQsC4GAPPhBlk0pD4QjwQkiJAIcxYCixED8wCAWweglT0ZCEDAoC5AYXaxEpChKcQiLS2q5FHeWFHfn3l0nD6eqq2Z6Znd2t/5z6f1+QM729vRUV03t//xOnVNVXXzka99GUhdmH/kccP/DI+CmG28Aeoe8RtJUGxz2CkjTpwAoEiaYFMoEk7qWA4wCE0wKZYJJIRJggkmhTDCpaylBFWEmmBTIBJM6VwBFASaYFMoEk7pWgKOI0gEwwaSOFTm8HEWUoplgUgxHEaVoJpjUOc/kkA6ECSZ1rqj+N8GkUCaY1LGUz+TwnhxSNBNMClE4iihFM8GkIN5VSgpmgkkdK0jUY4kmmBTIBJM6lrweTDoYJpjUtXxDKcAEk0KZYFIMz0WUoplgUseK1p8mmBTIBJNCFMljMCmYCSZ1LKUEJI/BpGgmmNQ1z+SQDoYJJnUtgfdFlA6ACSYFcR5MCmaCSR3LR19+RrMUzgSTulZ9SDOYYFIoE0zqXAHV+RwmmBTIBJM6lu+L6KerSOFMMCmG96aXoplgUseqy8G8J4cUzQSTupbyF+fBpGAmmNS15pYcJpgUyQSTuuY9OaSDYYJJHauuBvNMDimaCSZ1rXVTDhNMCmSCSR2rAsxjMCmaCSZ1LF/R7Nn0UjgTTOpYgfdFlA6ECSZ1LBXeF1E6ECaYFMNRRCmaBSYFssCkQB6DSR3Ld6UvHEWUoplgUte8q5R0MEwwqWv5zr7Og0nRTDCpY6k6FdFRRCmYCSbF8K5SUjQTTOpa9flgjiJKwUwwqXMJP11FOgAmmNSxgoL6zhwmmBTIBJO6ls/k8FxEKZoJJnUtJerPuTTBpEAmmNSxPH7o2fRSOBNMCuExmBTOBJM6lmfAPBdRCmeCSZ0r8K5S0gGwwKRAFpgUyGMwqWOpmgMDE0wKZYJJHctXNDsPJoUzwaSOpdZXE0wKZIJJXfMTLqWDYYJJQbyiWQpmgkkdK1p/mGBSIBNM6li+Esw7+0rhTDCpY/mOiH66ihTOBJM6ls9C9Gx6KZwJJnWsqE6ndxRRCmaCSR1L+fDLUUQpmgkmdez7P3xh/NgEkwKZYFIHZh/53PhxeuIXxo9NMCmQCSZ1pvjQX+54xgSTAplgUmc+99u/CczNzgCzszOYYFIoE0zqTL6Wuf5sFc9FlIKZYFJnigLqK8HyOfUmmBTIBJM6kyYem2BSIBNM6kyRdj5jgkmBTDCpM6m6IyLgvemleCaY1KE0/oLzYFI0E0zqTPuTwTwGk8KZYFJnzr9xETgxPwvMz81igkmhTDCpM1/+2A93PGOCSYGKj3zt24e9DtLUMsGkQBaYFMgCkwJZYFIgC0wKZIFJgSwwKZAFJgWywKRAFpgUyAKTAllgUiALTApkgUmBLDApkAUmBbLApEAWmBTIApMCWWBSIAtMCmSBSYEsMCmQBSYFssCkQBaYFMgCkwJZYFIgC0wKZIFJgSwwKZAFJgWywKRAFpgUyAKTAllgUiALTApkgUmBLDApkAUmBbLApEAWmBTIApMCWWBSIAtMCmSBSYEsMCmQBSYFssCkQBaYFMgCkwJZYFIgC0wKZIFJgSwwKZAFJgWywKRAFpgUyAKTAllgUiALTApkgUmBLDApkAUmBbLApEAWmBTIApMCWWBSIAtMCmSBSYGuowKbeeSzM4989rDXQteXwWGvQLjJopp55LNbX/j8oayMrjfFR7727cNehxCTdfX41/8U+OgHP5z/uvWFz1tpijZtCbZXJzBXV/vBuNKkOFNVYO3qevzRB/ODj37iyb1e//jX/7QdaKHrpuvTVA1y5CJ5/NEHx9WVjVNrbFxXk9+SOjQ9CZbjq11al8gu6WBMQ4LtGH8vf/J4/u+x31/Oz7xw7pkXzj2z14/nEHMEXxGmocD26hlewo4RRSnINAzTzzzy2XZplT95fPz4Y//qVH7w+f/+b4C3L9236xJ2VJoDHurKsT8G29E53PHdx35/eVxjQO4otstsMsS+ypMPPQKWmbowJQk2fjw+7hprF1jOMeCz//D3drzsqzQjIg/xIBaYunCMC2zXczVyIk2WGdsrra1dWlhd6tRx7SJum1NuzWXlGuv9/acnKy0/HpfZjrqiLi2pQ8e1wLJdp4kvO3e8a2RJEY7lMH01p7yPkzB2dAvbf32IB/N/42e+ypP5P5wWU0eOd4Lt6YUvPf7og7ueyfFVntwRWZN9RQ/A1JUpLTD46Cee3DHU0T76yjVmX1HRjmWB5Uu5PvrBD1fzy2//VPO9F77E9rMQz35rY8ePP8SDly4trxNTV45lge3ihS+1//b4ow+e+ep/OfutbS/54ofOAA890dTSZGRZV+rWcZ0H2zEIMXn1Vy6nS/jME3eOH1tXCnJcE2xcEuNKu4qLU6wrRTuuCbZDO9Aum13AZ5640+rSAZiSAqOusUtXV7tbiAmmeMe1i3gJ7SrK9bajrrC0dFCmJ8HGxt3FPJrf/pZ1pQM2hQnGRCFZVzosU5hg0tFxLE/2lY6L6ewiSgds9pHP7fq8CSYFMsGkznzysXcAJ+Zngfm5WUwwKZQJJnXmlptOArNzs8DczAATTAplgkmdSe0/EphgUigTTOpQMf6Sv5pgUiATTOpMkRJQeAwmHQwTTOpMKnY+Y4JJgUwwqTPFxGMTTApkgkmdSQnGJ3I4DyZFM8GkzhRFQXMkVmCCSaFMMKkz//o/fGXHMyaYFMgEkzqTnviFHc+YYFIg7+wrdSbfHbH40F+OnzHBpEAeg0kde8873g7cfONJTDAplAkmday+ojlhgkmhTDCpY9V1zYXnIkrBTDCpYznA8oGYCSYFMsGkjqXq7oiOIkrBTDCpY/m65uQoohTNBJM61v6UMBNMCmSCSUEcRZSCmWBS11Lz0ASTAplgukpF9XHExfjxtg8XyXdpr+7VnsaPrwftX4YJJgUywXQFclL18h3YewUw1wOYLQD6rVeOANhMABtlAaQyAWWa/jSrEhswwaRQl0kw+9lH2cHsncnUOtkHWOwXwCffcQLo9wD6RfP2o5SAUQnw5R+uAisjgIuj6U+zorUbTDAp0C4JZj/7KLu6vbOZ905KQFnC/jItv0uvVwAnBgA39AvgU+88Acz0CmCml18J0Gu13GX+mgA+8+5FYKtMwJeeWwUujABWhwVQtv7NTB8TTApUJdj12c8+LkeY1753/tMPV4H1EmC1hMv1OPJ79fsFcPMA4OF3LQJz/QKYrd6rtVbVb7KRKMbLHBQJmO0V1Gm2PkrAF59dAX4+bNakvQ7HZe/sJlFfFWaCSYEG11s/+3CPMPffKtfzKB3tnfsWgWGZgK29ehx1HIzf8dSgAD797hPAfL/9Xrun1rYtbW9v/jeWAPpFok7a/G/mC8+uACutHCuO/N65dGbma5mdB5PCDRYGBXW7+DvvOkF3/eyNUQL+8AcrwOvDAqpm5+Bz7HCPMK8uM4dlAQwK6GLv5BUse3m7YLcex/qoWYfF1jsuVO94+dTa8zfQ+j3kdJolAaPqGK9Z3saoABaO/N6ZzEwm3iWvqwkmBSp+9zt/AfzWO08A89v69FfWYrU+UKKq6dzXz+NF//7ZVWB5mIDR6OCOx67tCDOvJ+Nt2X6EmYDVIex2hHntmfmfn1+l/s1f+97JJvfRqJolAximBDz6/Crwz96xSJ2T15Jde8m/1fyO+V/IH/1glTolfusdR33v1JmZqJOzOuL65/8CeODhEfAWPx9Milb8ZP1H1PU6uKp2cVK7pdwsE7A2TMC/e3YFOL8FMCpLrna0Zz+6mMlptmKyvd9+hAl1Mlfvfs2Zmb/m73W7d/baurL1TF63fqfvuNe7D1vJkJ/vF83Xo7l32pn55eea7D33qd8DHvh0Cdx0wyImmBSqeGXrx81fOm2r2m1MblFWhgn4t3neYyu3Wzsj7NrnItrZdS0zOXtty+QR5h8+twq8udWsytWO+zXvMikiSdrvS+vdW4ceIe/Yft9r396D3zvtzBy2ehyP/OrvAA88XAJveYvHYFKwQS+sgSpaf+TcyK1Fbj9WRwC9PFMBdDEXUc089Jrjrpxdea6v25mcXtE88+l3nQD+6LnV8euvbtyv/S4Hqf2GB/nuXW3vwe+d9rkp1em8rRzOfSs/XUUKF35PjipV8vx9L1GP2PzBMyvAz1sVvngN8/cro+Y1N/YBHn73IvVxV7czOe1kzkvO7eXvvntx/JqIcT/tx8HsnV2yt9j5/apfdoVLlnQFDuiuUkVrZiMfid0yU1CfdZZ98hrO3/9yq4e90IP6DLq8nIgMabeUAwpg0J94jdl1SA5577RGwk0wKdBBJRhQH4nN9RLwyXctUmdR1dK0Uu6Kz9+/bzF/mzr3to0OBW/XwY/7aT8Oee9U/S9JYQ70zr5F65hqNveJWwm2bZ5hryW0X7PXXMTE0qSDVFTnGzkPJgU72ATLX/ecPdipmhxvne0x+WOp/ffW/P1Ri64r3pb8+EhuSzZNW9TxtrR7WN2tpKSdjuinq9RnLqfmK7D9mqW29tn3/fopjkZLOU3bkk3TFoVsS+unTDAp0FFNsNY1Nj9+/Q1g2Lri9ezKTTteX1+BC/DBuwvq1uUojCJO07Zk07RFEduSirxkRxGlYCEJlvZxq41ij+ar3SfOd2vILcptcz3q9ubMSgIeuOXC+AfyVWTPnL9x/JquOvjTtC201ufSjssWHf1tMcGkQIN8X4EOZiry7HX9B7C5vgHb5hZSPRMGzC7Mjh+3x2Fybziv1ddfLoBRugk4fbGkaQ9KoKAEzmyMgDvnB2y/U0KRmi244i2apm2Zvi06wtuyjfNgUrRBvp4q12B1P6b8nX23K+1+8ObaRFtS3XOvYPvdVXt5oqFo2qH2daDte/fcf/Ob1C1Wr2gCt08C7l7oMZ61AOp2pXoqXxO97y2apm2Zvi06+ttSLb267VnCBJNCDf7sJajrLI/u/527m8fte07t0lduDeFsa1Fa7UfZaiH+1/n51g81LUT7qrD89AfuahZfFAk4uz6kbkXOrA+BuxYGwJnVErhtrqC+Q91TL+c1T8BMUex3i6ZpW6Zvi47JttTv1zw2waRAg1PzrwN3zfeBZ9+4kfqeTWWrRem1rrzqtdqV3FudbFGqrmluUWgqPn99T55VoDn2y53YM+slcHb15nphlV5rTXZpp1t/ya3XrSfeAO6eHwA/eOOGS2xRv92eHfNtce8chW2pXlg0f5hgUqDB2ZUbgVdXC+q7XHz9p823+1UVJuBX7oZxGbd6sXu2KAnqkZZqrKb6bgLObgyBO+f7O16fP8MitcZ5sjvmm7bgjoXejudzK5h/4tWVtwCvrRTAoHepLfpg7v23Nuj4bot75yhsS/O9etjSBJMCDe462RyDtXvAdaEW1GdebaxuAqPWHQtn5md3LK7dQjx9fp66XamWRgK+//rJ8eNXVwGGqen27nVy2SvrJXDXQg94Za0E7syP10vgttl+vQDuPPE68LaFvEXpElu0vrZJfYfXuYWZY70t7p2jsC3/o3nzKnZNMCnQoF+UQK/Xo662Is9k07Qr9dWdibrNuHRdtq+xec8tb46XWY3nFM1r8rMvrY2oe8NnV26h7j3XbUzT1PQmvrYVrTn4omg9vvQWsUdTdhy3xb1zqNtSf7dZmAkmBRr0qvajBM6ul8DdCwPqWr8jH5u1WoX9nAFXvzpRz4XnefFX17eAO/K8+HoJ3J6vvQG2t17rqxtAydx4mXe2xnbuXBjseH7Uai3y+vW3zcrvtUWXd3y2xb1z+NtSKZp1M8GkQIPJtqJs/yVNPLNv1X18273tbbPjean98TO91jv+1cYakNIsdQ+7f/mmeZtyj8dXt0XTtC3ZNG3R0dkWWkum/V6SImy7J8e2Oez8eDLg9tvVr5cz19yEvpoXTwC3z80A3zt/A+PZiWrJiXqOvNcaBTqzUQJvy+c4rw2p+8e5h33b3MTHP+1/i/btGGyLe+cIbEt9bVse3ZAUZluC5bnq9rz1HXvUa/uOCOmSzc4rGyPqsZ08L/7W+d74+exv5rOeW9ecvrxeAj9bu2nH0sqJr+MV2vnMfrao3d4f923ZtgD3zuFsS/WSas09m14KNkitkZXUrszJSs3PFPnynQSsrq6Mv1lUM/HVpTCM6zgxfqZ57fZrePLIz5n1LepWp2QEjMohMEojxvMYCeCO+SZ4b8+vLxP1rPwwjVrPlEBKJUBqRpO2bVIqgM219V1/QbnVzOuQvw5bS66X3/pdFQl4ZX0TeOtc3pZmHfLS9ne+9hW6wr2zj4VVr9xr75R575D3Tn59uccWHdbeofnaWmbE3tklXVszZpJCDMrUA8pUALfPz1C3x7fP98ePU6vtqUq4yG1GU879Io/b9IGS3DYk4La55j5yb52foW5R3jqX73PQaoEYUl+LupWGwEbaAjbTEHh5tQTuXZgBzm4k6vOdX14dUd8pYaPcBDbSBrCe+sCp2QIYlkPq8Z/qrgzb2pudbU/e3lFZAsNUApslwKnZ0Xh9Ts0VwFaZ370HvHQx5+0WsJU2ga3UA7bSCNgsN4FRGlIfIdyzMABeWR+x2/nakybH3Mpq75RXtHcSvfF3J0feqj7CXB84s5JzYPe9s573TrkF/HQzAfeemAFezUdT84n6/IzbZnvj38DB7p285AJ46WLeii73Tu7x5TS+dbZZz+ocSM+ml6INzqzeDNw5fwF45vxJ4P5TF4DvLd8A3Hfzm8BG2bRYZdoC/uTlp6hbsmymGAAfv/tX89+AjdEM8N3lReC9SxeB7y2fBO67+QLw7XMnqdvFvOSbZ4fAoy9+A9hIJZBHf569OADeefJB4Lb5NeD0yk3A0uybwE8ungRunD0PfP/iU8Do4pC6Lz9X9ICP3fsB4NX1AnjbfL52qARGKdEMU+W+eNOD30hDYLMcAd+/8BTw3QtDxldb0QN+/Z4PAD9dT8BqmgNWynXgf579BjDKLVz1GxkA9596P7BRJmCrhPFVTNX65NfndErjtcrtYD3mBnBmLc/J5GS4sr1TMANslQPg9PomcO9CH3h1PadNAk6vjYCN0TywkTZ33Tu5xX7uYg/4e7e/D9gYJWCzzF/71H2K9XyV18WngK0D3Dv/5J4PAN9bvgko08ZV752UjxtT8/phOWJcF2XeU81YZX306DyYFKz4jz96FvhbSxeAZ1+/AXjglp8D31k+Cbzr5jeAp5cXgB9ffALYYhNYWCqBIU0bOWAArJ3rAb00C/ziyV8DZugDv7y0Avyf8zcA77nl58A3z52kPnL40cUngLLYAOaXcl+5aS369IHN5R4wX8wAH7vn/eP3fezFp4F1toDZU7llbdqSAb3xz84wC3z8nr8NPHP+FuCDp5o2L99/PI+J5SOrR09/DVhlAxicar5Lq41cOZe728151rmtPbGUgFQ0R0E9+sDWcgHMMQD+6b2/AgyKPtAvmiOQfJzzF8s3AHed/Dn1kefPNkbAXQt94MXVLeDWOYBvnlsEnt/33ukzC/zSyQ8B9yy+Cfz16sh2CNw61wP+am0LOH3xRuD5C08Ao2Jr172Tf8MbywVwYnLvnH4aWE9DYOZUoh6BjN47+d/MyjmAVOYjT8bvfvIK986gaMYptvJ9F5cXgVtPnAO+9rNvASM2gAuf/N/AAw+XwFtuXMQEk0IVX3z+u8D7li5Q94zzvEceXfmTF78JrLAJzJ8aAaOi+dqeNyvoAf2yD9Vo1dprfWCWWeATf+19AEUPePz008BKOQS2GAIlQ+pWPx+VtadqekUP6Ke8tB6wstyM2CzcUozXZ7PVZud5ifzJFzlt+swA6+f6wHwxB7zzhgeBD9yyATz20p+Pl5BHL3MPe26JZsmttarOuU7NNa8rr+V3L4H5pV7z+jyalKBOkrwtw9xeFnPAb9zzfuC7528E/u/FPxv/bv/u7e+lTphBqzXMe+e/nv4GsJqa9L7M3kl9oJf6wPq5HrBQzAAfv/f91NcEPnb6W8Bq6whnWO0ddt07/SoxBsAsfWDtXLN35k4VwJBmOeXB752U02zryvbOubx3ZoDfvPf9wEsbJfDnP/sOsFY2I5ZDtqj/9a7+xnPA/Q+XwM0mmBSt+Nh3PgfMF1C3kb9+z3uBr5x+mrp3u8YWdZ+1Oldgr7PCyuYsjc3XAAr6wGzRpwowFk/1gK2qVcszNgDr5zahOhNifmlAc4dUqO/ZUDVSZXOu9MVzm8DcUk7fEiiKHrB+brhzNVMPWFyaAwbFANhYBhgUAItLua3Nsz153Uqalo9d16peMuOtzo83zg2B+Vt3foBoqr5bAr2q5R5Q9/XzMhdPDajb+LXlBMznIwF6wEP3vA/4yovfBAanErDBFjDaz95pnUveTwPqI+SN881418JSAWzmuTWa+bTq93k97J3XRkCR074YUP/rZeJf76j1+1lb3gJ6v/3/aBLsJCaYFGowu9Sc5ZXn+P/4dJ5FKYGF1BzhjF+0++PqiXq0Zjx3wRAYLM1Sjwit95pZlHy2R25uFm5rzh1ZX26OpuZPDYBy251TW8cY1ckmJbCx3Jz9PLc0YLzo6kdzm7cG5JZl8dbmbnsrVeOec6B1jHfrYNe1Wjg1qBezrR/fPlUttc6lyKmVzS31xqtWFCPqT6DK1ovmiGKwlJ/LY4w94I9fvNa9k59eO7dGnSeLSzPUe2etKKmj7rrbOwlg7lQeF23OtOwtzVJn4+S/3vaabNC8YxpviaQgg83eFtvvkjM4BfUlM7kPnc3dOhi/pmotJtrIqvrPlcDcrU31VqNbPajv5JqX0992Z/BEdRBXtYv5+dxfzz3stqrVKVqPq/VstRpp5+vbW1H2RuNtZx9rtdBaq7Xl4Xg9i1aLtcsatuTsqt6zirBE/ftpr0k2qr6W46c72zs5RYsezdgjXLd7J2fX0s7fT37LMvcR9vGvt37YPDbBpECD9j10qp5lL1FfnjPXapk28jxPD+q2YVI1OtSS20iaL7tcjVTULx2/Jvez21eUpXY/PrejrVGg1PpjY7mZj5prr2f77OaJ1mjSLmvVS0B7oK71YbxsnG+2vd0SV6s2sW6TrT6T6xO8d9rvcn3vnZLtqVvdV+MK//XWD/Oaey6iFGz3po4dvXmgbpOqOt2jMNut8nqrrVpojRql1vL3lNuPiT56tQ4Tcx3tpW0bWWrNt1Rt6j5axz1Wp/4ckF4zF7T+WjPit+c8TOuJPUe9qt9bPkfu8uvm3plYnWvcOz1gvnV8eE2/H6DejyaYFGjPBFs/P2R77W7rJU+0MVVz1kvUsw3Vz7Za69yK7L+mc3+93ZtvdYn3XgeadahasuVh/fI9e9L7UbWUQD1y1T7mqe8vu491y6Ne1/D7ce9M6mDvdPT7aW+pCSYF2iXBcs21++t7tUltk6M622ZOrrB1rNbh1mtdhzzelfv9k7NMV6n1OcIL29awuMTiu/r9uHcu41D3zjbJYzAp2LYEy3U8Ofpxpa3Kbu3BfpfW2TpUffqdP3fNLWQHa3h1vx/3zr6WDBzG3tnGMzmkg7HLMdi1tyLXvrSu1qHbbYlY8lXmzyG9e7frMH17p/qpyXFdSRH2nAeTdHXy5z3kM11MMCmQCSYF8UwOKZgJJoXw01WkcCaY1LE8flh4LqIUzQSTOtfcf9IEkwKZYFLnivEXE0wKVPyjX/4Hh70O0tQywaRAxcujPzjsdZCmlgkmBSo+89R947/80i++87I/8PyPn7vsa1yOy3E5mQkmBarmwY5CrbsclzN9yzHBpECDo1PrLsflTN9yTDAp0GXORTyObYbLcTlHZzkmmBRozwQ7vm2Gy3E5R2c5JpgUaJcEO+5thstxOUdnOSaYFGhbgk1Hm+FyXM7RWY4JJgUq/vG//BuHvQ7S1DLBpEDF8//tw4e9DtLUMsGkQIO5+3/tsNdBmlommBTIApMCWWBSIAtMCmSBSYEsMCmQBSYFssCkQBaYFMgCkwJZYFIgC0wKZIFJgSwwKZAFJgWywKRAFpgUyAKTAllgUiALTApkgUmBLDApkAUmBbLApEAWmBTIApMCWWBSIAtMCmSBSYEsMCmQBSYFssCkQBaYFMgCkwJZYFIgC0wKZIFJgSwwKZAFJgWywKRAFpgUyAKTAllgUiALTApkgUmBLDApkAUmBbLApEAWmBTIApMCWWBSIAtMCmSBSYEsMCmQBSYFssCkQBaYFMgCkwJZYFIgC0wKZIFJgSwwKZAFJgWywKRAFpgUyAKTAllgUiALTApkgUmBLDApkAUmBbLApEAWmBTIApMCWWBSIAtMCmSBSYEsMCmQBSYFssCkQBaYFMgCkwJZYFIgC0wKZIFJgf4/k42dyXISZeAAAAAASUVORK5CYII=\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [132, 169,  68],\n",
              "        [148, 183,  81],\n",
              "        [148, 183,  81]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [132, 169,  68],\n",
              "        [148, 183,  81],\n",
              "        [148, 183,  81]],\n",
              "\n",
              "       [[ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        [ 78, 192, 202],\n",
              "        ...,\n",
              "        [132, 169,  69],\n",
              "        [148, 184,  80],\n",
              "        [148, 184,  80]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]],\n",
              "\n",
              "       [[222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        ...,\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149],\n",
              "        [222, 216, 149]]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-d422219c-dbfd-4ba4-8250-a31888b745f1 button').onclick = (e) => {\n",
              "        document.querySelector('#id-d422219c-dbfd-4ba4-8250-a31888b745f1').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-d422219c-dbfd-4ba4-8250-a31888b745f1 button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.close()"
      ],
      "metadata": {
        "id": "bEXl_IsAa2Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 1: HYPERPARAMETERS\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "GAMMA = 0.99\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_END = 0.01\n",
        "EPSILON_DECAY = 0.995\n",
        "BATCH_SIZE = 64\n",
        "MEMORY_SIZE = 10000\n",
        "TARGET_UPDATE = 100\n",
        "NUM_EPISODES = 1000"
      ],
      "metadata": {
        "id": "dVfjqgGjaP4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 2: ENVIRONMENT EXPLORATION\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "def explore_environment():\n",
        "    \"\"\"\n",
        "    Understand state/action space of Flappy Bird.\n",
        "    Prints shapes, runs random episode.\n",
        "    \"\"\"\n",
        "    # TODO: Implement exploration\n",
        "    observation, info = env.reset()\n",
        "    print(f\"Observation shape: {observation.shape}\")\n",
        "    print(f\"Info: {info}\")\n",
        "\n",
        "    print(f\"Last 10 values: {observation[-10:]}\")\n",
        "\n",
        "    # jump\n",
        "    action = 1\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    # bird jumped up => lower values increased\n",
        "    print(f\"After action: {observation[-10:]}\")\n",
        "\n",
        "    # don't jump\n",
        "    action = 0\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    # still rising\n",
        "    print(f\"After action: {observation[-10:]}\")\n",
        "    action = 0\n",
        "\n",
        "env = gym.make('FlappyBird-v0', render_mode='rgb_array', use_lidar=True)\n",
        "explore_environment()\n",
        "env.close()"
      ],
      "metadata": {
        "id": "eKw9LqZ9bJM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d572b6-6454-4608-aa5c-454d47bfdbf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation shape: (180,)\n",
            "Info: {'score': 0}\n",
            "Last 10 values: [0.87077653 0.86325566 0.85355176 0.84422975 0.83749654 0.83099074\n",
            " 0.82471771 0.81868282 0.81289137 0.80734859]\n",
            "After action: [0.92584822 0.91578435 0.90606844 0.8967118  0.88772578 0.88123633\n",
            " 0.87496653 0.86892114 0.86310487 0.85752239]\n",
            "After action: [0.97139963 0.96137938 0.95169132 0.9423457  0.93335279 0.92684578\n",
            " 0.91849489 0.91248114 0.90668591 0.90111343]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 3: RANDOM BASELINE AGENT\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "def random_agent_baseline(env, num_episodes=10):\n",
        "    \"\"\"\n",
        "    Random agent for baseline performance.\n",
        "\n",
        "    Returns:\n",
        "        avg_score: Average score over episodes\n",
        "        avg_steps: Average survival time\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    steps = []\n",
        "    for episode in range(num_episodes):\n",
        "        observation, info = env.reset()\n",
        "        done = False\n",
        "        score = 0\n",
        "        steps_taken = 0\n",
        "        while not done:\n",
        "            action = env.action_space.sample()\n",
        "            observation, reward, terminated, truncated, info = env.step(action)\n",
        "            score += reward\n",
        "            done = terminated or truncated\n",
        "            steps_taken += 1\n",
        "        scores.append(score)\n",
        "        steps.append(steps_taken)\n",
        "\n",
        "    avg_score = np.mean(scores)\n",
        "    avg_steps = np.mean(steps)\n",
        "\n",
        "    return avg_score, avg_steps\n",
        "\n",
        "env = gym.make('FlappyBird-v0', render_mode='rgb_array', use_lidar=True)\n",
        "env = RecordVideo(env, 'random_agent_video', episode_trigger=lambda x: True)\n",
        "avg_score, avg_steps = random_agent_baseline(env)\n",
        "env.close()\n",
        "\n",
        "print(f\"Average score: {avg_score}\")\n",
        "print(f\"Average steps: {avg_steps}\")\n",
        "print(\"Videos saved in /content/random_agent_video/\")\n",
        "\n",
        "# convert to gif\n",
        "video = VideoFileClip(\"random_agent_video/rl-video-episode-0.mp4\")\n",
        "video.write_gif(\"random_agent.gif\", fps=30, program='ffmpeg')\n",
        "print(\"GIF saved in /content/random_agent.gif\")\n",
        "\n",
        "# When randomly choosing actions, bird almost always flies up to the top of the screen"
      ],
      "metadata": {
        "id": "sp1Iss8rbNNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37ee7710-c2f0-4203-d71c-920abf4a1054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/random_agent_video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average score: -7.679999999999998\n",
            "Average steps: 50.0\n",
            "Videos saved in /content/random_agent_video/\n",
            "MoviePy - Building file  random_agent.gif\n",
            "MoviePy - - Generating GIF frames.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - - File ready: random_agent.gif.\n",
            "GIF saved in /content/random_agent.gif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 4: DQN NETWORK\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        \"\"\"\n",
        "        Deep Q-Network.\n",
        "\n",
        "        Args:\n",
        "            state_dim: Dimension of state space (12 for features, 180 for LIDAR)\n",
        "            action_dim: Number of actions (2: nothing/flap)\n",
        "        \"\"\"\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        # 2–3 fully connected layers with ReLU\n",
        "        hidden1 = 128\n",
        "        hidden2 = 128\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden1, hidden2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden2, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: State tensor [batch_size, state_dim]\n",
        "\n",
        "        Returns:\n",
        "            Q-values: [batch_size, action_dim]\n",
        "        \"\"\"\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Model Save / Load\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def save_dqn(model, path):\n",
        "    \"\"\"\n",
        "    Save DQN model weights to file.\n",
        "    \"\"\"\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "\n",
        "def load_dqn(path, state_dim, action_dim, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Load DQN model weights from file.\n",
        "\n",
        "    Returns:\n",
        "        model: DQN instance with loaded weights (in eval mode)\n",
        "    \"\"\"\n",
        "    model = DQN(state_dim, action_dim).to(device)\n",
        "    state_dict = torch.load(path, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "#test\n",
        "env = gym.make('FlappyBird-v0', use_lidar=True)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "model = DQN(state_dim, action_dim)\n",
        "test_state = torch.randn(1, state_dim)   # fake input\n",
        "q_values = model(test_state)\n",
        "\n",
        "print(\"State dim:\", state_dim)\n",
        "print(\"Action dim:\", action_dim)\n",
        "print(\"Q-values:\", q_values)"
      ],
      "metadata": {
        "id": "V3hgeel4bNPf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e89357d-7bfc-4831-dc13-525e27b13957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State dim: 180\n",
            "Action dim: 2\n",
            "Q-values: tensor([[ 0.0668, -0.1172]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 5: EXPERIENCE REPLAY BUFFER\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        \"\"\"\n",
        "        Experience replay buffer.\n",
        "\n",
        "        Args:\n",
        "            capacity: Maximum number of experiences to store\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "        self.buffer = deque(maxlen=capacity)   # auto-removes old items\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Store one experience into the buffer.\"\"\"\n",
        "        experience = (state, action, reward, next_state, done)\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample a random batch of experiences.\"\"\"\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "\n",
        "        # Unzip the batch\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        # Convert to NumPy arrays (PyTorch will convert later)\n",
        "        return (\n",
        "            np.array(states),\n",
        "            np.array(actions),\n",
        "            np.array(rewards, dtype=np.float32),\n",
        "            np.array(next_states),\n",
        "            np.array(dones, dtype=np.float32),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return current size of the buffer.\"\"\"\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "#test\n",
        "buf = ReplayBuffer(100)\n",
        "\n",
        "buf.push([1,2,3], 1, 0.5, [1,2,4], False)\n",
        "buf.push([4,5,6], 0, 1.0, [4,5,7], True)\n",
        "\n",
        "print(\"Buffer size:\", len(buf))\n",
        "batch = buf.sample(2)\n",
        "\n",
        "print(\"Sampled states:\", batch[0])\n",
        "print(\"Sampled actions:\", batch[1])"
      ],
      "metadata": {
        "id": "HhL0sLWlbNRb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03264388-c5a8-47c5-aa86-1cb521027271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buffer size: 2\n",
            "Sampled states: [[4 5 6]\n",
            " [1 2 3]]\n",
            "Sampled actions: [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 6: DQN AGENT\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, device=None):\n",
        "        \"\"\"\n",
        "        DQN Agent with target network.\n",
        "\n",
        "        Args:\n",
        "            state_dim: State space dimension\n",
        "            action_dim: Action space dimension\n",
        "        \"\"\"\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "        # Device (CPU / GPU)\n",
        "        if device is None:\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.device = device\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.gamma = 0.99           # discount factor\n",
        "        self.batch_size = 64\n",
        "        self.learning_rate = 1e-3\n",
        "        self.replay_capacity = 50_000\n",
        "        self.min_buffer_size = 1_000  # start training only after this many transitions\n",
        "\n",
        "        # Networks\n",
        "        self.policy_net = DQN(state_dim, action_dim).to(self.device)\n",
        "        self.target_net = DQN(state_dim, action_dim).to(self.device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()  # target net in eval mode\n",
        "\n",
        "        # Optimizer & loss\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "        # Replay buffer\n",
        "        self.replay_buffer = ReplayBuffer(self.replay_capacity)\n",
        "\n",
        "    def select_action(self, state, epsilon):\n",
        "        \"\"\"\n",
        "        Epsilon-greedy action selection.\n",
        "\n",
        "        Args:\n",
        "            state: Current state (np.array shape [state_dim])\n",
        "            epsilon: Exploration probability\n",
        "\n",
        "        Returns:\n",
        "            action: 0 or 1\n",
        "        \"\"\"\n",
        "        # Explore\n",
        "        if random.random() < epsilon:\n",
        "            return random.randrange(self.action_dim)\n",
        "\n",
        "        # Exploit\n",
        "        state_tensor = torch.tensor(\n",
        "            state, dtype=torch.float32, device=self.device\n",
        "        ).unsqueeze(0)  # [1, state_dim]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_values = self.policy_net(state_tensor)  # [1, action_dim]\n",
        "            action = q_values.argmax(dim=1).item()\n",
        "\n",
        "        return action\n",
        "\n",
        "    def train_step(self):\n",
        "        \"\"\"\n",
        "        Sample batch and update network.\n",
        "\n",
        "        Returns:\n",
        "            loss_value: float or None (if not enough data)\n",
        "        \"\"\"\n",
        "        # Don't train until buffer has enough samples\n",
        "        if len(self.replay_buffer) < max(self.batch_size, self.min_buffer_size):\n",
        "            return None\n",
        "\n",
        "        # Sample batch from replay buffer\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "        # Convert to tensors\n",
        "        states      = torch.tensor(states, dtype=torch.float32, device=self.device)\n",
        "        actions     = torch.tensor(actions, dtype=torch.long,   device=self.device).unsqueeze(1)   # [B,1]\n",
        "        rewards     = torch.tensor(rewards, dtype=torch.float32, device=self.device).unsqueeze(1) # [B,1]\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float32, device=self.device)\n",
        "        dones       = torch.tensor(dones, dtype=torch.float32,  device=self.device).unsqueeze(1)  # [B,1]\n",
        "\n",
        "        # Current Q-values from policy network for taken actions\n",
        "        q_values = self.policy_net(states).gather(1, actions)  # [B,1]\n",
        "\n",
        "        # Target Q-values using target network\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_net(next_states).max(dim=1, keepdim=True)[0]  # [B,1]\n",
        "            target_q_values = rewards + self.gamma * (1.0 - dones) * next_q_values\n",
        "\n",
        "        # Compute loss\n",
        "        loss = self.loss_fn(q_values, target_q_values)\n",
        "\n",
        "        # Backpropagation\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # (Optional) gradient clipping:\n",
        "        # torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"Copy weights from policy to target network.\"\"\"\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "env = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "agent = DQNAgent(state_dim, action_dim)\n",
        "\n",
        "state, _ = env.reset()\n",
        "action = agent.select_action(state, epsilon=0.5)\n",
        "print(\"Sampled action:\", action)\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "kyWpBiJzbVv4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1a2ba71-5d6b-464f-b453-be02afc2e466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled action: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFs3iP3vQ9gm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a94b1c22-7113-4630-91c3-7dc1439a6890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/1000 | Reward: -7.50 | Avg loss: 0.0000 | Epsilon: 0.778\n",
            "Episode 2/1000 | Reward: -6.30 | Avg loss: 0.0000 | Epsilon: 0.606\n",
            "Episode 3/1000 | Reward: -0.90 | Avg loss: 0.0000 | Epsilon: 0.471\n",
            "Episode 4/1000 | Reward: -4.50 | Avg loss: 0.0000 | Epsilon: 0.367\n",
            "Episode 5/1000 | Reward: -0.90 | Avg loss: 0.0000 | Epsilon: 0.286\n",
            "Episode 6/1000 | Reward: 0.90 | Avg loss: 0.0000 | Epsilon: 0.222\n",
            "Episode 7/1000 | Reward: -0.90 | Avg loss: 0.0000 | Epsilon: 0.173\n",
            "Episode 8/1000 | Reward: 1.40 | Avg loss: 0.0000 | Epsilon: 0.144\n",
            "Episode 9/1000 | Reward: -0.90 | Avg loss: 0.0000 | Epsilon: 0.112\n",
            "Episode 10/1000 | Reward: -2.00 | Avg loss: 0.0000 | Epsilon: 0.084\n",
            "Episode 11/1000 | Reward: 0.90 | Avg loss: 0.0000 | Epsilon: 0.065\n",
            "Episode 12/1000 | Reward: 0.80 | Avg loss: 0.0000 | Epsilon: 0.056\n",
            "Episode 13/1000 | Reward: -0.90 | Avg loss: 0.0000 | Epsilon: 0.044\n",
            "Episode 14/1000 | Reward: 0.80 | Avg loss: 0.0000 | Epsilon: 0.037\n",
            "Episode 15/1000 | Reward: 0.80 | Avg loss: 0.0000 | Epsilon: 0.032\n",
            "Episode 16/1000 | Reward: 1.70 | Avg loss: 0.0000 | Epsilon: 0.026\n",
            "Episode 17/1000 | Reward: -0.40 | Avg loss: 0.0000 | Epsilon: 0.020\n",
            "Episode 18/1000 | Reward: 0.80 | Avg loss: 0.0000 | Epsilon: 0.018\n",
            "Episode 19/1000 | Reward: 0.80 | Avg loss: 0.0000 | Epsilon: 0.015\n",
            "Episode 20/1000 | Reward: 0.80 | Avg loss: 0.0000 | Epsilon: 0.013\n",
            "Episode 21/1000 | Reward: 0.80 | Avg loss: 0.0000 | Epsilon: 0.011\n",
            "Episode 22/1000 | Reward: 0.80 | Avg loss: 0.0000 | Epsilon: 0.010\n",
            "Episode 23/1000 | Reward: 0.80 | Avg loss: 0.0000 | Epsilon: 0.010\n",
            "Episode 24/1000 | Reward: 0.80 | Avg loss: 0.0000 | Epsilon: 0.010\n",
            "Episode 25/1000 | Reward: -0.90 | Avg loss: 0.0509 | Epsilon: 0.010\n",
            "Episode 26/1000 | Reward: -0.90 | Avg loss: 0.0227 | Epsilon: 0.010\n",
            "Episode 27/1000 | Reward: -0.90 | Avg loss: 0.0197 | Epsilon: 0.010\n",
            "Episode 28/1000 | Reward: -0.90 | Avg loss: 0.0120 | Epsilon: 0.010\n",
            "Episode 29/1000 | Reward: -0.90 | Avg loss: 0.0230 | Epsilon: 0.010\n",
            "Episode 30/1000 | Reward: -0.90 | Avg loss: 0.0142 | Epsilon: 0.010\n",
            "Episode 31/1000 | Reward: -0.90 | Avg loss: 0.0236 | Epsilon: 0.010\n",
            "Episode 32/1000 | Reward: -6.90 | Avg loss: 0.0198 | Epsilon: 0.010\n",
            "Episode 33/1000 | Reward: -0.90 | Avg loss: 0.0247 | Epsilon: 0.010\n",
            "Episode 34/1000 | Reward: -0.90 | Avg loss: 0.0188 | Epsilon: 0.010\n",
            "Episode 35/1000 | Reward: -0.90 | Avg loss: 0.0285 | Epsilon: 0.010\n",
            "Episode 36/1000 | Reward: -0.90 | Avg loss: 0.0231 | Epsilon: 0.010\n",
            "Episode 37/1000 | Reward: -4.50 | Avg loss: 0.0280 | Epsilon: 0.010\n",
            "Episode 38/1000 | Reward: -0.90 | Avg loss: 0.0253 | Epsilon: 0.010\n",
            "Episode 39/1000 | Reward: -5.70 | Avg loss: 0.0304 | Epsilon: 0.010\n",
            "Episode 40/1000 | Reward: -0.90 | Avg loss: 0.0249 | Epsilon: 0.010\n",
            "Episode 41/1000 | Reward: -0.90 | Avg loss: 0.0331 | Epsilon: 0.010\n",
            "Episode 42/1000 | Reward: -0.90 | Avg loss: 0.0239 | Epsilon: 0.010\n",
            "Episode 43/1000 | Reward: -0.90 | Avg loss: 0.0491 | Epsilon: 0.010\n",
            "Episode 44/1000 | Reward: -0.90 | Avg loss: 0.0413 | Epsilon: 0.010\n",
            "Episode 45/1000 | Reward: -4.80 | Avg loss: 0.0535 | Epsilon: 0.010\n",
            "Episode 46/1000 | Reward: 0.60 | Avg loss: 0.0532 | Epsilon: 0.010\n",
            "Episode 47/1000 | Reward: -3.30 | Avg loss: 0.0388 | Epsilon: 0.010\n",
            "Episode 48/1000 | Reward: -0.90 | Avg loss: 0.0448 | Epsilon: 0.010\n",
            "Episode 49/1000 | Reward: -0.90 | Avg loss: 0.0487 | Epsilon: 0.010\n",
            "Episode 50/1000 | Reward: -2.70 | Avg loss: 0.0477 | Epsilon: 0.010\n",
            "Episode 51/1000 | Reward: -0.90 | Avg loss: 0.0626 | Epsilon: 0.010\n",
            "Episode 52/1000 | Reward: -2.60 | Avg loss: 0.0560 | Epsilon: 0.010\n",
            "Episode 53/1000 | Reward: -0.90 | Avg loss: 0.0467 | Epsilon: 0.010\n",
            "Episode 54/1000 | Reward: -1.00 | Avg loss: 0.0702 | Epsilon: 0.010\n",
            "Episode 55/1000 | Reward: 0.80 | Avg loss: 0.0594 | Epsilon: 0.010\n",
            "Episode 56/1000 | Reward: -2.20 | Avg loss: 0.0617 | Epsilon: 0.010\n",
            "Episode 57/1000 | Reward: -2.50 | Avg loss: 0.0701 | Epsilon: 0.010\n",
            "Episode 58/1000 | Reward: 0.30 | Avg loss: 0.0873 | Epsilon: 0.010\n",
            "Episode 59/1000 | Reward: -1.10 | Avg loss: 0.0632 | Epsilon: 0.010\n",
            "Episode 60/1000 | Reward: 0.20 | Avg loss: 0.0611 | Epsilon: 0.010\n",
            "Episode 61/1000 | Reward: 0.30 | Avg loss: 0.0778 | Epsilon: 0.010\n",
            "Episode 62/1000 | Reward: -0.30 | Avg loss: 0.0630 | Epsilon: 0.010\n",
            "Episode 63/1000 | Reward: 1.70 | Avg loss: 0.0673 | Epsilon: 0.010\n",
            "Episode 64/1000 | Reward: 1.20 | Avg loss: 0.0721 | Epsilon: 0.010\n",
            "Episode 65/1000 | Reward: -0.90 | Avg loss: 0.0646 | Epsilon: 0.010\n",
            "Episode 66/1000 | Reward: -0.90 | Avg loss: 0.0690 | Epsilon: 0.010\n",
            "Episode 67/1000 | Reward: 0.90 | Avg loss: 0.0669 | Epsilon: 0.010\n",
            "Episode 68/1000 | Reward: 0.40 | Avg loss: 0.0697 | Epsilon: 0.010\n",
            "Episode 69/1000 | Reward: 2.10 | Avg loss: 0.0544 | Epsilon: 0.010\n",
            "Episode 70/1000 | Reward: -0.90 | Avg loss: 0.0742 | Epsilon: 0.010\n",
            "Episode 71/1000 | Reward: 0.30 | Avg loss: 0.0490 | Epsilon: 0.010\n",
            "Episode 72/1000 | Reward: -0.90 | Avg loss: 0.0554 | Epsilon: 0.010\n",
            "Episode 73/1000 | Reward: -0.50 | Avg loss: 0.0488 | Epsilon: 0.010\n",
            "Episode 74/1000 | Reward: 2.70 | Avg loss: 0.0535 | Epsilon: 0.010\n",
            "Episode 75/1000 | Reward: 2.90 | Avg loss: 0.0484 | Epsilon: 0.010\n",
            "Episode 76/1000 | Reward: -1.90 | Avg loss: 0.0535 | Epsilon: 0.010\n",
            "Episode 77/1000 | Reward: 0.50 | Avg loss: 0.0544 | Epsilon: 0.010\n",
            "Episode 78/1000 | Reward: -0.90 | Avg loss: 0.0424 | Epsilon: 0.010\n",
            "Episode 79/1000 | Reward: -0.40 | Avg loss: 0.0445 | Epsilon: 0.010\n",
            "Episode 80/1000 | Reward: -3.50 | Avg loss: 0.0438 | Epsilon: 0.010\n",
            "Episode 81/1000 | Reward: 0.30 | Avg loss: 0.0685 | Epsilon: 0.010\n",
            "Episode 82/1000 | Reward: 0.60 | Avg loss: 0.0563 | Epsilon: 0.010\n",
            "Episode 83/1000 | Reward: -1.60 | Avg loss: 0.0501 | Epsilon: 0.010\n",
            "Episode 84/1000 | Reward: -0.90 | Avg loss: 0.0610 | Epsilon: 0.010\n",
            "Episode 85/1000 | Reward: 0.00 | Avg loss: 0.0600 | Epsilon: 0.010\n",
            "Episode 86/1000 | Reward: 2.70 | Avg loss: 0.0554 | Epsilon: 0.010\n",
            "Episode 87/1000 | Reward: 2.10 | Avg loss: 0.0536 | Epsilon: 0.010\n",
            "Episode 88/1000 | Reward: 2.80 | Avg loss: 0.0497 | Epsilon: 0.010\n",
            "Episode 89/1000 | Reward: 0.20 | Avg loss: 0.0475 | Epsilon: 0.010\n",
            "Episode 90/1000 | Reward: -0.90 | Avg loss: 0.0494 | Epsilon: 0.010\n",
            "Episode 91/1000 | Reward: 1.10 | Avg loss: 0.0608 | Epsilon: 0.010\n",
            "Episode 92/1000 | Reward: -0.90 | Avg loss: 0.0522 | Epsilon: 0.010\n",
            "Episode 93/1000 | Reward: 0.30 | Avg loss: 0.0512 | Epsilon: 0.010\n",
            "Episode 94/1000 | Reward: 2.10 | Avg loss: 0.0508 | Epsilon: 0.010\n",
            "Episode 95/1000 | Reward: 1.50 | Avg loss: 0.0408 | Epsilon: 0.010\n",
            "Episode 96/1000 | Reward: 2.50 | Avg loss: 0.0561 | Epsilon: 0.010\n",
            "Episode 97/1000 | Reward: -0.90 | Avg loss: 0.0533 | Epsilon: 0.010\n",
            "Episode 98/1000 | Reward: 2.80 | Avg loss: 0.0606 | Epsilon: 0.010\n",
            "Episode 99/1000 | Reward: 0.50 | Avg loss: 0.0433 | Epsilon: 0.010\n",
            "Episode 100/1000 | Reward: -0.90 | Avg loss: 0.0517 | Epsilon: 0.010\n",
            "Episode 101/1000 | Reward: 0.20 | Avg loss: 0.0602 | Epsilon: 0.010\n",
            "Episode 102/1000 | Reward: -1.10 | Avg loss: 0.0731 | Epsilon: 0.010\n",
            "Episode 103/1000 | Reward: 2.70 | Avg loss: 0.0570 | Epsilon: 0.010\n",
            "Episode 104/1000 | Reward: 0.00 | Avg loss: 0.0470 | Epsilon: 0.010\n",
            "Episode 105/1000 | Reward: -0.10 | Avg loss: 0.0468 | Epsilon: 0.010\n",
            "Episode 106/1000 | Reward: 2.60 | Avg loss: 0.0483 | Epsilon: 0.010\n",
            "Episode 107/1000 | Reward: 0.90 | Avg loss: 0.0467 | Epsilon: 0.010\n",
            "Episode 108/1000 | Reward: 3.00 | Avg loss: 0.0539 | Epsilon: 0.010\n",
            "Episode 109/1000 | Reward: 1.50 | Avg loss: 0.0452 | Epsilon: 0.010\n",
            "Episode 110/1000 | Reward: 1.80 | Avg loss: 0.0416 | Epsilon: 0.010\n",
            "Episode 111/1000 | Reward: 2.60 | Avg loss: 0.0607 | Epsilon: 0.010\n",
            "Episode 112/1000 | Reward: 1.60 | Avg loss: 0.0598 | Epsilon: 0.010\n",
            "Episode 113/1000 | Reward: 2.30 | Avg loss: 0.0576 | Epsilon: 0.010\n",
            "Episode 114/1000 | Reward: 2.90 | Avg loss: 0.0519 | Epsilon: 0.010\n",
            "Episode 115/1000 | Reward: -8.70 | Avg loss: 0.0584 | Epsilon: 0.010\n",
            "Episode 116/1000 | Reward: 0.30 | Avg loss: 0.0560 | Epsilon: 0.010\n",
            "Episode 117/1000 | Reward: 0.60 | Avg loss: 0.0657 | Epsilon: 0.010\n",
            "Episode 118/1000 | Reward: 2.70 | Avg loss: 0.0498 | Epsilon: 0.010\n",
            "Episode 119/1000 | Reward: -0.30 | Avg loss: 0.0549 | Epsilon: 0.010\n",
            "Episode 120/1000 | Reward: 2.10 | Avg loss: 0.0649 | Epsilon: 0.010\n",
            "Episode 121/1000 | Reward: 2.90 | Avg loss: 0.0671 | Epsilon: 0.010\n",
            "Episode 122/1000 | Reward: 2.40 | Avg loss: 0.0587 | Epsilon: 0.010\n",
            "Episode 123/1000 | Reward: 0.70 | Avg loss: 0.0552 | Epsilon: 0.010\n",
            "Episode 124/1000 | Reward: -0.90 | Avg loss: 0.0551 | Epsilon: 0.010\n",
            "Episode 125/1000 | Reward: 3.00 | Avg loss: 0.0580 | Epsilon: 0.010\n",
            "Episode 126/1000 | Reward: 1.70 | Avg loss: 0.0597 | Epsilon: 0.010\n",
            "Episode 127/1000 | Reward: 2.80 | Avg loss: 0.0578 | Epsilon: 0.010\n",
            "Episode 128/1000 | Reward: 2.70 | Avg loss: 0.0575 | Epsilon: 0.010\n",
            "Episode 129/1000 | Reward: 2.80 | Avg loss: 0.0516 | Epsilon: 0.010\n",
            "Episode 130/1000 | Reward: 0.30 | Avg loss: 0.0620 | Epsilon: 0.010\n",
            "Episode 131/1000 | Reward: 2.70 | Avg loss: 0.0496 | Epsilon: 0.010\n",
            "Episode 132/1000 | Reward: 1.50 | Avg loss: 0.0525 | Epsilon: 0.010\n",
            "Episode 133/1000 | Reward: 2.10 | Avg loss: 0.0413 | Epsilon: 0.010\n",
            "Episode 134/1000 | Reward: 2.80 | Avg loss: 0.0483 | Epsilon: 0.010\n",
            "Episode 135/1000 | Reward: 0.90 | Avg loss: 0.0474 | Epsilon: 0.010\n",
            "Episode 136/1000 | Reward: 2.90 | Avg loss: 0.0482 | Epsilon: 0.010\n",
            "Episode 137/1000 | Reward: -0.70 | Avg loss: 0.0555 | Epsilon: 0.010\n",
            "Episode 138/1000 | Reward: 2.10 | Avg loss: 0.0443 | Epsilon: 0.010\n",
            "Episode 139/1000 | Reward: 1.80 | Avg loss: 0.0467 | Epsilon: 0.010\n",
            "Episode 140/1000 | Reward: 2.80 | Avg loss: 0.0402 | Epsilon: 0.010\n",
            "Episode 141/1000 | Reward: 2.40 | Avg loss: 0.0423 | Epsilon: 0.010\n",
            "Episode 142/1000 | Reward: 2.60 | Avg loss: 0.0383 | Epsilon: 0.010\n",
            "Episode 143/1000 | Reward: 0.70 | Avg loss: 0.0411 | Epsilon: 0.010\n",
            "Episode 144/1000 | Reward: -0.90 | Avg loss: 0.0441 | Epsilon: 0.010\n",
            "Episode 145/1000 | Reward: -1.40 | Avg loss: 0.0594 | Epsilon: 0.010\n",
            "Episode 146/1000 | Reward: -0.90 | Avg loss: 0.0530 | Epsilon: 0.010\n",
            "Episode 147/1000 | Reward: 2.90 | Avg loss: 0.0504 | Epsilon: 0.010\n",
            "Episode 148/1000 | Reward: 0.30 | Avg loss: 0.0576 | Epsilon: 0.010\n",
            "Episode 149/1000 | Reward: 6.90 | Avg loss: 0.0498 | Epsilon: 0.010\n",
            "Episode 150/1000 | Reward: 3.10 | Avg loss: 0.0505 | Epsilon: 0.010\n",
            "Episode 151/1000 | Reward: 1.50 | Avg loss: 0.0520 | Epsilon: 0.010\n",
            "Episode 152/1000 | Reward: 1.80 | Avg loss: 0.0546 | Epsilon: 0.010\n",
            "Episode 153/1000 | Reward: 3.80 | Avg loss: 0.0561 | Epsilon: 0.010\n",
            "Episode 154/1000 | Reward: 3.20 | Avg loss: 0.0590 | Epsilon: 0.010\n",
            "Episode 155/1000 | Reward: 3.10 | Avg loss: 0.0590 | Epsilon: 0.010\n",
            "Episode 156/1000 | Reward: -0.90 | Avg loss: 0.0622 | Epsilon: 0.010\n",
            "Episode 157/1000 | Reward: 2.20 | Avg loss: 0.0532 | Epsilon: 0.010\n",
            "Episode 158/1000 | Reward: 3.50 | Avg loss: 0.0447 | Epsilon: 0.010\n",
            "Episode 159/1000 | Reward: 3.10 | Avg loss: 0.0417 | Epsilon: 0.010\n",
            "Episode 160/1000 | Reward: 2.90 | Avg loss: 0.0462 | Epsilon: 0.010\n",
            "Episode 161/1000 | Reward: -0.90 | Avg loss: 0.0409 | Epsilon: 0.010\n",
            "Episode 162/1000 | Reward: 2.20 | Avg loss: 0.0435 | Epsilon: 0.010\n",
            "Episode 163/1000 | Reward: 2.90 | Avg loss: 0.0410 | Epsilon: 0.010\n",
            "Episode 164/1000 | Reward: 2.80 | Avg loss: 0.0345 | Epsilon: 0.010\n",
            "Episode 165/1000 | Reward: 3.30 | Avg loss: 0.0390 | Epsilon: 0.010\n",
            "Episode 166/1000 | Reward: -0.90 | Avg loss: 0.0366 | Epsilon: 0.010\n",
            "Episode 167/1000 | Reward: 0.20 | Avg loss: 0.0353 | Epsilon: 0.010\n",
            "Episode 168/1000 | Reward: 2.70 | Avg loss: 0.0394 | Epsilon: 0.010\n",
            "Episode 169/1000 | Reward: 0.20 | Avg loss: 0.0383 | Epsilon: 0.010\n",
            "Episode 170/1000 | Reward: 2.40 | Avg loss: 0.0398 | Epsilon: 0.010\n",
            "Episode 171/1000 | Reward: 3.00 | Avg loss: 0.0321 | Epsilon: 0.010\n",
            "Episode 172/1000 | Reward: 4.30 | Avg loss: 0.0327 | Epsilon: 0.010\n",
            "Episode 173/1000 | Reward: 3.00 | Avg loss: 0.0339 | Epsilon: 0.010\n",
            "Episode 174/1000 | Reward: 3.10 | Avg loss: 0.0367 | Epsilon: 0.010\n",
            "Episode 175/1000 | Reward: 2.80 | Avg loss: 0.0403 | Epsilon: 0.010\n",
            "Episode 176/1000 | Reward: 1.10 | Avg loss: 0.0424 | Epsilon: 0.010\n",
            "Episode 177/1000 | Reward: 3.10 | Avg loss: 0.0376 | Epsilon: 0.010\n",
            "Episode 178/1000 | Reward: 2.90 | Avg loss: 0.0408 | Epsilon: 0.010\n",
            "Episode 179/1000 | Reward: 2.60 | Avg loss: 0.0378 | Epsilon: 0.010\n",
            "Episode 180/1000 | Reward: 2.70 | Avg loss: 0.0391 | Epsilon: 0.010\n",
            "Episode 181/1000 | Reward: 3.00 | Avg loss: 0.0437 | Epsilon: 0.010\n",
            "Episode 182/1000 | Reward: 2.50 | Avg loss: 0.0377 | Epsilon: 0.010\n",
            "Episode 183/1000 | Reward: 2.20 | Avg loss: 0.0415 | Epsilon: 0.010\n",
            "Episode 184/1000 | Reward: -2.30 | Avg loss: 0.0481 | Epsilon: 0.010\n",
            "Episode 185/1000 | Reward: 3.20 | Avg loss: 0.0353 | Epsilon: 0.010\n",
            "Episode 186/1000 | Reward: 1.50 | Avg loss: 0.0484 | Epsilon: 0.010\n",
            "Episode 187/1000 | Reward: -0.30 | Avg loss: 0.0492 | Epsilon: 0.010\n",
            "Episode 188/1000 | Reward: 1.50 | Avg loss: 0.0470 | Epsilon: 0.010\n",
            "Episode 189/1000 | Reward: 2.70 | Avg loss: 0.0429 | Epsilon: 0.010\n",
            "Episode 190/1000 | Reward: -0.90 | Avg loss: 0.0474 | Epsilon: 0.010\n",
            "Episode 191/1000 | Reward: 2.30 | Avg loss: 0.0452 | Epsilon: 0.010\n",
            "Episode 192/1000 | Reward: -0.90 | Avg loss: 0.0438 | Epsilon: 0.010\n",
            "Episode 193/1000 | Reward: 3.00 | Avg loss: 0.0530 | Epsilon: 0.010\n",
            "Episode 194/1000 | Reward: 0.20 | Avg loss: 0.0515 | Epsilon: 0.010\n",
            "Episode 195/1000 | Reward: 3.10 | Avg loss: 0.0547 | Epsilon: 0.010\n",
            "Episode 196/1000 | Reward: 0.30 | Avg loss: 0.0486 | Epsilon: 0.010\n",
            "Episode 197/1000 | Reward: 2.00 | Avg loss: 0.0463 | Epsilon: 0.010\n",
            "Episode 198/1000 | Reward: -0.90 | Avg loss: 0.0465 | Epsilon: 0.010\n",
            "Episode 199/1000 | Reward: 2.20 | Avg loss: 0.0542 | Epsilon: 0.010\n",
            "Episode 200/1000 | Reward: 2.90 | Avg loss: 0.0470 | Epsilon: 0.010\n",
            "Episode 201/1000 | Reward: 2.10 | Avg loss: 0.0497 | Epsilon: 0.010\n",
            "Episode 202/1000 | Reward: -0.90 | Avg loss: 0.0536 | Epsilon: 0.010\n",
            "Episode 203/1000 | Reward: 0.10 | Avg loss: 0.0530 | Epsilon: 0.010\n",
            "Episode 204/1000 | Reward: 2.40 | Avg loss: 0.0676 | Epsilon: 0.010\n",
            "Episode 205/1000 | Reward: 2.80 | Avg loss: 0.0463 | Epsilon: 0.010\n",
            "Episode 206/1000 | Reward: 3.00 | Avg loss: 0.0570 | Epsilon: 0.010\n",
            "Episode 207/1000 | Reward: 2.90 | Avg loss: 0.0503 | Epsilon: 0.010\n",
            "Episode 208/1000 | Reward: 2.50 | Avg loss: 0.0581 | Epsilon: 0.010\n",
            "Episode 209/1000 | Reward: 3.40 | Avg loss: 0.0674 | Epsilon: 0.010\n",
            "Episode 210/1000 | Reward: 0.60 | Avg loss: 0.0531 | Epsilon: 0.010\n",
            "Episode 211/1000 | Reward: 4.40 | Avg loss: 0.0597 | Epsilon: 0.010\n",
            "Episode 212/1000 | Reward: 2.60 | Avg loss: 0.0578 | Epsilon: 0.010\n",
            "Episode 213/1000 | Reward: 2.10 | Avg loss: 0.0663 | Epsilon: 0.010\n",
            "Episode 214/1000 | Reward: 3.00 | Avg loss: 0.0485 | Epsilon: 0.010\n",
            "Episode 215/1000 | Reward: 2.90 | Avg loss: 0.0779 | Epsilon: 0.010\n",
            "Episode 216/1000 | Reward: -0.90 | Avg loss: 0.0507 | Epsilon: 0.010\n",
            "Episode 217/1000 | Reward: 2.20 | Avg loss: 0.0484 | Epsilon: 0.010\n",
            "Episode 218/1000 | Reward: 3.00 | Avg loss: 0.0493 | Epsilon: 0.010\n",
            "Episode 219/1000 | Reward: 2.80 | Avg loss: 0.0531 | Epsilon: 0.010\n",
            "Episode 220/1000 | Reward: 3.00 | Avg loss: 0.0496 | Epsilon: 0.010\n",
            "Episode 221/1000 | Reward: 4.80 | Avg loss: 0.0532 | Epsilon: 0.010\n",
            "Episode 222/1000 | Reward: -0.90 | Avg loss: 0.0429 | Epsilon: 0.010\n",
            "Episode 223/1000 | Reward: -0.90 | Avg loss: 0.0556 | Epsilon: 0.010\n",
            "Episode 224/1000 | Reward: 2.20 | Avg loss: 0.0574 | Epsilon: 0.010\n",
            "Episode 225/1000 | Reward: 1.30 | Avg loss: 0.0558 | Epsilon: 0.010\n",
            "Episode 226/1000 | Reward: -0.90 | Avg loss: 0.0607 | Epsilon: 0.010\n",
            "Episode 227/1000 | Reward: 1.30 | Avg loss: 0.0702 | Epsilon: 0.010\n",
            "Episode 228/1000 | Reward: 2.40 | Avg loss: 0.0585 | Epsilon: 0.010\n",
            "Episode 229/1000 | Reward: 1.90 | Avg loss: 0.0591 | Epsilon: 0.010\n",
            "Episode 230/1000 | Reward: 2.90 | Avg loss: 0.0705 | Epsilon: 0.010\n",
            "Episode 231/1000 | Reward: 2.10 | Avg loss: 0.0683 | Epsilon: 0.010\n",
            "Episode 232/1000 | Reward: 2.90 | Avg loss: 0.0628 | Epsilon: 0.010\n",
            "Episode 233/1000 | Reward: 2.80 | Avg loss: 0.0608 | Epsilon: 0.010\n",
            "Episode 234/1000 | Reward: 2.90 | Avg loss: 0.0625 | Epsilon: 0.010\n",
            "Episode 235/1000 | Reward: 2.40 | Avg loss: 0.0562 | Epsilon: 0.010\n",
            "Episode 236/1000 | Reward: 0.60 | Avg loss: 0.0669 | Epsilon: 0.010\n",
            "Episode 237/1000 | Reward: 3.80 | Avg loss: 0.0678 | Epsilon: 0.010\n",
            "Episode 238/1000 | Reward: 3.10 | Avg loss: 0.0634 | Epsilon: 0.010\n",
            "Episode 239/1000 | Reward: 3.10 | Avg loss: 0.0568 | Epsilon: 0.010\n",
            "Episode 240/1000 | Reward: 2.90 | Avg loss: 0.0613 | Epsilon: 0.010\n",
            "Episode 241/1000 | Reward: 2.80 | Avg loss: 0.0629 | Epsilon: 0.010\n",
            "Episode 242/1000 | Reward: 3.40 | Avg loss: 0.0596 | Epsilon: 0.010\n",
            "Episode 243/1000 | Reward: 1.30 | Avg loss: 0.0628 | Epsilon: 0.010\n",
            "Episode 244/1000 | Reward: 5.40 | Avg loss: 0.0659 | Epsilon: 0.010\n",
            "Episode 245/1000 | Reward: 3.10 | Avg loss: 0.0735 | Epsilon: 0.010\n",
            "Episode 246/1000 | Reward: 3.00 | Avg loss: 0.0627 | Epsilon: 0.010\n",
            "Episode 247/1000 | Reward: 0.10 | Avg loss: 0.0526 | Epsilon: 0.010\n",
            "Episode 248/1000 | Reward: 1.60 | Avg loss: 0.0459 | Epsilon: 0.010\n",
            "Episode 249/1000 | Reward: -0.30 | Avg loss: 0.0481 | Epsilon: 0.010\n",
            "Episode 250/1000 | Reward: 4.80 | Avg loss: 0.0489 | Epsilon: 0.010\n",
            "Episode 251/1000 | Reward: 2.20 | Avg loss: 0.0503 | Epsilon: 0.010\n",
            "Episode 252/1000 | Reward: 3.80 | Avg loss: 0.0561 | Epsilon: 0.010\n",
            "Episode 253/1000 | Reward: 2.60 | Avg loss: 0.0502 | Epsilon: 0.010\n",
            "Episode 254/1000 | Reward: 3.10 | Avg loss: 0.0643 | Epsilon: 0.010\n",
            "Episode 255/1000 | Reward: 2.70 | Avg loss: 0.0648 | Epsilon: 0.010\n",
            "Episode 256/1000 | Reward: 2.80 | Avg loss: 0.0654 | Epsilon: 0.010\n",
            "Episode 257/1000 | Reward: -0.40 | Avg loss: 0.0602 | Epsilon: 0.010\n",
            "Episode 258/1000 | Reward: 1.10 | Avg loss: 0.0604 | Epsilon: 0.010\n",
            "Episode 259/1000 | Reward: 2.40 | Avg loss: 0.0650 | Epsilon: 0.010\n",
            "Episode 260/1000 | Reward: 2.00 | Avg loss: 0.0618 | Epsilon: 0.010\n",
            "Episode 261/1000 | Reward: 2.90 | Avg loss: 0.0596 | Epsilon: 0.010\n",
            "Episode 262/1000 | Reward: -0.90 | Avg loss: 0.0516 | Epsilon: 0.010\n",
            "Episode 263/1000 | Reward: 2.40 | Avg loss: 0.0496 | Epsilon: 0.010\n",
            "Episode 264/1000 | Reward: 2.70 | Avg loss: 0.0561 | Epsilon: 0.010\n",
            "Episode 265/1000 | Reward: -0.10 | Avg loss: 0.0480 | Epsilon: 0.010\n",
            "Episode 266/1000 | Reward: 2.80 | Avg loss: 0.0497 | Epsilon: 0.010\n",
            "Episode 267/1000 | Reward: 3.00 | Avg loss: 0.0485 | Epsilon: 0.010\n",
            "Episode 268/1000 | Reward: -0.90 | Avg loss: 0.0561 | Epsilon: 0.010\n",
            "Episode 269/1000 | Reward: -1.00 | Avg loss: 0.0511 | Epsilon: 0.010\n",
            "Episode 270/1000 | Reward: 2.90 | Avg loss: 0.0411 | Epsilon: 0.010\n",
            "Episode 271/1000 | Reward: 2.70 | Avg loss: 0.0461 | Epsilon: 0.010\n",
            "Episode 272/1000 | Reward: -0.90 | Avg loss: 0.0527 | Epsilon: 0.010\n",
            "Episode 273/1000 | Reward: 3.90 | Avg loss: 0.0425 | Epsilon: 0.010\n",
            "Episode 274/1000 | Reward: 0.40 | Avg loss: 0.0461 | Epsilon: 0.010\n",
            "Episode 275/1000 | Reward: 2.30 | Avg loss: 0.0449 | Epsilon: 0.010\n",
            "Episode 276/1000 | Reward: -0.90 | Avg loss: 0.0439 | Epsilon: 0.010\n",
            "Episode 277/1000 | Reward: 1.10 | Avg loss: 0.0471 | Epsilon: 0.010\n",
            "Episode 278/1000 | Reward: -0.90 | Avg loss: 0.0571 | Epsilon: 0.010\n",
            "Episode 279/1000 | Reward: 2.90 | Avg loss: 0.0571 | Epsilon: 0.010\n",
            "Episode 280/1000 | Reward: -1.90 | Avg loss: 0.0476 | Epsilon: 0.010\n",
            "Episode 281/1000 | Reward: 2.80 | Avg loss: 0.0528 | Epsilon: 0.010\n",
            "Episode 282/1000 | Reward: 5.20 | Avg loss: 0.0648 | Epsilon: 0.010\n",
            "Episode 283/1000 | Reward: 1.50 | Avg loss: 0.0642 | Epsilon: 0.010\n",
            "Episode 284/1000 | Reward: 3.10 | Avg loss: 0.0536 | Epsilon: 0.010\n",
            "Episode 285/1000 | Reward: 5.70 | Avg loss: 0.0583 | Epsilon: 0.010\n",
            "Episode 286/1000 | Reward: 3.00 | Avg loss: 0.0613 | Epsilon: 0.010\n",
            "Episode 287/1000 | Reward: 0.50 | Avg loss: 0.0586 | Epsilon: 0.010\n",
            "Episode 288/1000 | Reward: 2.40 | Avg loss: 0.0651 | Epsilon: 0.010\n",
            "Episode 289/1000 | Reward: 8.90 | Avg loss: 0.0571 | Epsilon: 0.010\n",
            "Episode 290/1000 | Reward: 3.70 | Avg loss: 0.0546 | Epsilon: 0.010\n",
            "Episode 291/1000 | Reward: 0.80 | Avg loss: 0.0612 | Epsilon: 0.010\n",
            "Episode 292/1000 | Reward: 2.10 | Avg loss: 0.0651 | Epsilon: 0.010\n",
            "Episode 293/1000 | Reward: 0.10 | Avg loss: 0.0618 | Epsilon: 0.010\n",
            "Episode 294/1000 | Reward: 2.40 | Avg loss: 0.0646 | Epsilon: 0.010\n",
            "Episode 295/1000 | Reward: 2.80 | Avg loss: 0.0626 | Epsilon: 0.010\n",
            "Episode 296/1000 | Reward: -0.40 | Avg loss: 0.0697 | Epsilon: 0.010\n",
            "Episode 297/1000 | Reward: 2.20 | Avg loss: 0.0758 | Epsilon: 0.010\n",
            "Episode 298/1000 | Reward: 3.40 | Avg loss: 0.0742 | Epsilon: 0.010\n",
            "Episode 299/1000 | Reward: 0.40 | Avg loss: 0.0652 | Epsilon: 0.010\n",
            "Episode 300/1000 | Reward: -0.80 | Avg loss: 0.0738 | Epsilon: 0.010\n",
            "Episode 301/1000 | Reward: -0.90 | Avg loss: 0.0847 | Epsilon: 0.010\n",
            "Episode 302/1000 | Reward: 3.10 | Avg loss: 0.1085 | Epsilon: 0.010\n",
            "Episode 303/1000 | Reward: -0.90 | Avg loss: 0.0708 | Epsilon: 0.010\n",
            "Episode 304/1000 | Reward: -0.10 | Avg loss: 0.0802 | Epsilon: 0.010\n",
            "Episode 305/1000 | Reward: 2.50 | Avg loss: 0.0771 | Epsilon: 0.010\n",
            "Episode 306/1000 | Reward: 2.70 | Avg loss: 0.1047 | Epsilon: 0.010\n",
            "Episode 307/1000 | Reward: 2.50 | Avg loss: 0.0788 | Epsilon: 0.010\n",
            "Episode 308/1000 | Reward: -2.40 | Avg loss: 0.0640 | Epsilon: 0.010\n",
            "Episode 309/1000 | Reward: 1.40 | Avg loss: 0.0727 | Epsilon: 0.010\n",
            "Episode 310/1000 | Reward: 3.10 | Avg loss: 0.0733 | Epsilon: 0.010\n",
            "Episode 311/1000 | Reward: 2.90 | Avg loss: 0.0840 | Epsilon: 0.010\n",
            "Episode 312/1000 | Reward: 5.40 | Avg loss: 0.0634 | Epsilon: 0.010\n",
            "Episode 313/1000 | Reward: 1.80 | Avg loss: 0.0718 | Epsilon: 0.010\n",
            "Episode 314/1000 | Reward: 0.10 | Avg loss: 0.0648 | Epsilon: 0.010\n",
            "Episode 315/1000 | Reward: 3.10 | Avg loss: 0.0534 | Epsilon: 0.010\n",
            "Episode 316/1000 | Reward: 3.80 | Avg loss: 0.0656 | Epsilon: 0.010\n",
            "Episode 317/1000 | Reward: 2.50 | Avg loss: 0.0617 | Epsilon: 0.010\n",
            "Episode 318/1000 | Reward: 1.40 | Avg loss: 0.0585 | Epsilon: 0.010\n",
            "Episode 319/1000 | Reward: -0.30 | Avg loss: 0.0639 | Epsilon: 0.010\n",
            "Episode 320/1000 | Reward: 3.00 | Avg loss: 0.0638 | Epsilon: 0.010\n",
            "Episode 321/1000 | Reward: 2.10 | Avg loss: 0.0628 | Epsilon: 0.010\n",
            "Episode 322/1000 | Reward: 2.90 | Avg loss: 0.0690 | Epsilon: 0.010\n",
            "Episode 323/1000 | Reward: 2.60 | Avg loss: 0.0623 | Epsilon: 0.010\n",
            "Episode 324/1000 | Reward: 2.60 | Avg loss: 0.0578 | Epsilon: 0.010\n",
            "Episode 325/1000 | Reward: 2.70 | Avg loss: 0.0583 | Epsilon: 0.010\n",
            "Episode 326/1000 | Reward: 2.50 | Avg loss: 0.0540 | Epsilon: 0.010\n",
            "Episode 327/1000 | Reward: 2.50 | Avg loss: 0.0684 | Epsilon: 0.010\n",
            "Episode 328/1000 | Reward: -0.90 | Avg loss: 0.0509 | Epsilon: 0.010\n",
            "Episode 329/1000 | Reward: 2.40 | Avg loss: 0.0544 | Epsilon: 0.010\n",
            "Episode 330/1000 | Reward: 1.30 | Avg loss: 0.0563 | Epsilon: 0.010\n",
            "Episode 331/1000 | Reward: 2.50 | Avg loss: 0.0484 | Epsilon: 0.010\n",
            "Episode 332/1000 | Reward: -0.90 | Avg loss: 0.0546 | Epsilon: 0.010\n",
            "Episode 333/1000 | Reward: 4.90 | Avg loss: 0.0553 | Epsilon: 0.010\n",
            "Episode 334/1000 | Reward: 3.00 | Avg loss: 0.0520 | Epsilon: 0.010\n",
            "Episode 335/1000 | Reward: -0.90 | Avg loss: 0.0609 | Epsilon: 0.010\n",
            "Episode 336/1000 | Reward: -2.10 | Avg loss: 0.0610 | Epsilon: 0.010\n",
            "Episode 337/1000 | Reward: -1.10 | Avg loss: 0.0600 | Epsilon: 0.010\n",
            "Episode 338/1000 | Reward: -0.80 | Avg loss: 0.0614 | Epsilon: 0.010\n",
            "Episode 339/1000 | Reward: -0.20 | Avg loss: 0.0572 | Epsilon: 0.010\n",
            "Episode 340/1000 | Reward: -0.90 | Avg loss: 0.0772 | Epsilon: 0.010\n",
            "Episode 341/1000 | Reward: 3.30 | Avg loss: 0.0633 | Epsilon: 0.010\n",
            "Episode 342/1000 | Reward: -0.90 | Avg loss: 0.0733 | Epsilon: 0.010\n",
            "Episode 343/1000 | Reward: 6.20 | Avg loss: 0.0688 | Epsilon: 0.010\n",
            "Episode 344/1000 | Reward: -0.90 | Avg loss: 0.0816 | Epsilon: 0.010\n",
            "Episode 345/1000 | Reward: 5.40 | Avg loss: 0.0737 | Epsilon: 0.010\n",
            "Episode 346/1000 | Reward: 0.40 | Avg loss: 0.0726 | Epsilon: 0.010\n",
            "Episode 347/1000 | Reward: -0.40 | Avg loss: 0.0807 | Epsilon: 0.010\n",
            "Episode 348/1000 | Reward: 2.90 | Avg loss: 0.0752 | Epsilon: 0.010\n",
            "Episode 349/1000 | Reward: 2.90 | Avg loss: 0.0705 | Epsilon: 0.010\n",
            "Episode 350/1000 | Reward: 0.60 | Avg loss: 0.0825 | Epsilon: 0.010\n",
            "Episode 351/1000 | Reward: 0.90 | Avg loss: 0.0721 | Epsilon: 0.010\n",
            "Episode 352/1000 | Reward: 3.60 | Avg loss: 0.0683 | Epsilon: 0.010\n",
            "Episode 353/1000 | Reward: 3.10 | Avg loss: 0.0661 | Epsilon: 0.010\n",
            "Episode 354/1000 | Reward: 10.40 | Avg loss: 0.0705 | Epsilon: 0.010\n",
            "Episode 355/1000 | Reward: -0.90 | Avg loss: 0.0781 | Epsilon: 0.010\n",
            "Episode 356/1000 | Reward: -1.20 | Avg loss: 0.0764 | Epsilon: 0.010\n",
            "Episode 357/1000 | Reward: 12.20 | Avg loss: 0.0739 | Epsilon: 0.010\n",
            "Episode 358/1000 | Reward: 3.00 | Avg loss: 0.0705 | Epsilon: 0.010\n",
            "Episode 359/1000 | Reward: 2.10 | Avg loss: 0.0671 | Epsilon: 0.010\n",
            "Episode 360/1000 | Reward: 3.00 | Avg loss: 0.0626 | Epsilon: 0.010\n",
            "Episode 361/1000 | Reward: 2.10 | Avg loss: 0.0558 | Epsilon: 0.010\n",
            "Episode 362/1000 | Reward: 4.70 | Avg loss: 0.0711 | Epsilon: 0.010\n",
            "Episode 363/1000 | Reward: 5.80 | Avg loss: 0.0691 | Epsilon: 0.010\n",
            "Episode 364/1000 | Reward: 2.50 | Avg loss: 0.0591 | Epsilon: 0.010\n",
            "Episode 365/1000 | Reward: 3.00 | Avg loss: 0.0703 | Epsilon: 0.010\n",
            "Episode 366/1000 | Reward: 3.30 | Avg loss: 0.0634 | Epsilon: 0.010\n",
            "Episode 367/1000 | Reward: 1.60 | Avg loss: 0.0509 | Epsilon: 0.010\n",
            "Episode 368/1000 | Reward: 4.00 | Avg loss: 0.0586 | Epsilon: 0.010\n",
            "Episode 369/1000 | Reward: -0.90 | Avg loss: 0.0452 | Epsilon: 0.010\n",
            "Episode 370/1000 | Reward: 1.80 | Avg loss: 0.0547 | Epsilon: 0.010\n",
            "Episode 371/1000 | Reward: 0.20 | Avg loss: 0.0578 | Epsilon: 0.010\n",
            "Episode 372/1000 | Reward: 2.30 | Avg loss: 0.0567 | Epsilon: 0.010\n",
            "Episode 373/1000 | Reward: 0.30 | Avg loss: 0.0484 | Epsilon: 0.010\n",
            "Episode 374/1000 | Reward: 3.10 | Avg loss: 0.0547 | Epsilon: 0.010\n",
            "Episode 375/1000 | Reward: -0.90 | Avg loss: 0.0571 | Epsilon: 0.010\n",
            "Episode 376/1000 | Reward: -1.30 | Avg loss: 0.0615 | Epsilon: 0.010\n",
            "Episode 377/1000 | Reward: 3.00 | Avg loss: 0.0508 | Epsilon: 0.010\n",
            "Episode 378/1000 | Reward: 2.30 | Avg loss: 0.0528 | Epsilon: 0.010\n",
            "Episode 379/1000 | Reward: -0.90 | Avg loss: 0.0496 | Epsilon: 0.010\n",
            "Episode 380/1000 | Reward: 2.80 | Avg loss: 0.0547 | Epsilon: 0.010\n",
            "Episode 381/1000 | Reward: -0.90 | Avg loss: 0.0585 | Epsilon: 0.010\n",
            "Episode 382/1000 | Reward: -0.90 | Avg loss: 0.0597 | Epsilon: 0.010\n",
            "Episode 383/1000 | Reward: 3.00 | Avg loss: 0.0455 | Epsilon: 0.010\n",
            "Episode 384/1000 | Reward: -0.90 | Avg loss: 0.0569 | Epsilon: 0.010\n",
            "Episode 385/1000 | Reward: -0.90 | Avg loss: 0.0503 | Epsilon: 0.010\n",
            "Episode 386/1000 | Reward: -0.90 | Avg loss: 0.0470 | Epsilon: 0.010\n",
            "Episode 387/1000 | Reward: 2.90 | Avg loss: 0.0472 | Epsilon: 0.010\n",
            "Episode 388/1000 | Reward: 5.30 | Avg loss: 0.0459 | Epsilon: 0.010\n",
            "Episode 389/1000 | Reward: 2.60 | Avg loss: 0.0566 | Epsilon: 0.010\n",
            "Episode 390/1000 | Reward: -0.90 | Avg loss: 0.0512 | Epsilon: 0.010\n",
            "Episode 391/1000 | Reward: -0.90 | Avg loss: 0.0607 | Epsilon: 0.010\n",
            "Episode 392/1000 | Reward: -0.90 | Avg loss: 0.0543 | Epsilon: 0.010\n",
            "Episode 393/1000 | Reward: 3.10 | Avg loss: 0.0622 | Epsilon: 0.010\n",
            "Episode 394/1000 | Reward: 1.40 | Avg loss: 0.0537 | Epsilon: 0.010\n",
            "Episode 395/1000 | Reward: 2.80 | Avg loss: 0.0550 | Epsilon: 0.010\n",
            "Episode 396/1000 | Reward: 2.60 | Avg loss: 0.0554 | Epsilon: 0.010\n",
            "Episode 397/1000 | Reward: 6.50 | Avg loss: 0.0646 | Epsilon: 0.010\n",
            "Episode 398/1000 | Reward: -0.10 | Avg loss: 0.0503 | Epsilon: 0.010\n",
            "Episode 399/1000 | Reward: 2.80 | Avg loss: 0.0517 | Epsilon: 0.010\n",
            "Episode 400/1000 | Reward: -0.90 | Avg loss: 0.0434 | Epsilon: 0.010\n",
            "Episode 401/1000 | Reward: 5.10 | Avg loss: 0.0512 | Epsilon: 0.010\n",
            "Episode 402/1000 | Reward: 2.60 | Avg loss: 0.0475 | Epsilon: 0.010\n",
            "Episode 403/1000 | Reward: -0.90 | Avg loss: 0.0447 | Epsilon: 0.010\n",
            "Episode 404/1000 | Reward: -1.20 | Avg loss: 0.0456 | Epsilon: 0.010\n",
            "Episode 405/1000 | Reward: -0.90 | Avg loss: 0.0440 | Epsilon: 0.010\n",
            "Episode 406/1000 | Reward: -0.40 | Avg loss: 0.0577 | Epsilon: 0.010\n",
            "Episode 407/1000 | Reward: -0.90 | Avg loss: 0.0506 | Epsilon: 0.010\n",
            "Episode 408/1000 | Reward: -0.90 | Avg loss: 0.0452 | Epsilon: 0.010\n",
            "Episode 409/1000 | Reward: -0.90 | Avg loss: 0.0518 | Epsilon: 0.010\n",
            "Episode 410/1000 | Reward: -0.90 | Avg loss: 0.0463 | Epsilon: 0.010\n",
            "Episode 411/1000 | Reward: 2.90 | Avg loss: 0.0485 | Epsilon: 0.010\n",
            "Episode 412/1000 | Reward: -0.90 | Avg loss: 0.0601 | Epsilon: 0.010\n",
            "Episode 413/1000 | Reward: 3.20 | Avg loss: 0.0549 | Epsilon: 0.010\n",
            "Episode 414/1000 | Reward: 2.90 | Avg loss: 0.0535 | Epsilon: 0.010\n",
            "Episode 415/1000 | Reward: 1.50 | Avg loss: 0.0647 | Epsilon: 0.010\n",
            "Episode 416/1000 | Reward: -0.10 | Avg loss: 0.0635 | Epsilon: 0.010\n",
            "Episode 417/1000 | Reward: 3.00 | Avg loss: 0.0523 | Epsilon: 0.010\n",
            "Episode 418/1000 | Reward: -0.90 | Avg loss: 0.0512 | Epsilon: 0.010\n",
            "Episode 419/1000 | Reward: -1.30 | Avg loss: 0.0580 | Epsilon: 0.010\n",
            "Episode 420/1000 | Reward: 7.60 | Avg loss: 0.0636 | Epsilon: 0.010\n",
            "Episode 421/1000 | Reward: 2.40 | Avg loss: 0.0645 | Epsilon: 0.010\n",
            "Episode 422/1000 | Reward: 6.10 | Avg loss: 0.0578 | Epsilon: 0.010\n",
            "Episode 423/1000 | Reward: 2.90 | Avg loss: 0.0635 | Epsilon: 0.010\n",
            "Episode 424/1000 | Reward: 6.10 | Avg loss: 0.0626 | Epsilon: 0.010\n",
            "Episode 425/1000 | Reward: 3.20 | Avg loss: 0.0697 | Epsilon: 0.010\n",
            "Episode 426/1000 | Reward: -0.60 | Avg loss: 0.0752 | Epsilon: 0.010\n",
            "Episode 427/1000 | Reward: 3.10 | Avg loss: 0.0718 | Epsilon: 0.010\n",
            "Episode 428/1000 | Reward: 7.10 | Avg loss: 0.0805 | Epsilon: 0.010\n",
            "Episode 429/1000 | Reward: 0.90 | Avg loss: 0.0807 | Epsilon: 0.010\n",
            "Episode 430/1000 | Reward: 3.40 | Avg loss: 0.0722 | Epsilon: 0.010\n",
            "Episode 431/1000 | Reward: 3.00 | Avg loss: 0.0699 | Epsilon: 0.010\n",
            "Episode 432/1000 | Reward: 3.20 | Avg loss: 0.0790 | Epsilon: 0.010\n",
            "Episode 433/1000 | Reward: 2.90 | Avg loss: 0.0678 | Epsilon: 0.010\n",
            "Episode 434/1000 | Reward: -0.90 | Avg loss: 0.0749 | Epsilon: 0.010\n",
            "Episode 435/1000 | Reward: 2.40 | Avg loss: 0.0710 | Epsilon: 0.010\n",
            "Episode 436/1000 | Reward: -0.90 | Avg loss: 0.0676 | Epsilon: 0.010\n",
            "Episode 437/1000 | Reward: 1.50 | Avg loss: 0.0691 | Epsilon: 0.010\n",
            "Episode 438/1000 | Reward: 2.60 | Avg loss: 0.0597 | Epsilon: 0.010\n",
            "Episode 439/1000 | Reward: 9.60 | Avg loss: 0.0615 | Epsilon: 0.010\n",
            "Episode 440/1000 | Reward: 2.50 | Avg loss: 0.0650 | Epsilon: 0.010\n",
            "Episode 441/1000 | Reward: 2.80 | Avg loss: 0.0637 | Epsilon: 0.010\n",
            "Episode 442/1000 | Reward: 0.40 | Avg loss: 0.0704 | Epsilon: 0.010\n",
            "Episode 443/1000 | Reward: 1.60 | Avg loss: 0.0703 | Epsilon: 0.010\n",
            "Episode 444/1000 | Reward: 0.60 | Avg loss: 0.0568 | Epsilon: 0.010\n",
            "Episode 445/1000 | Reward: 3.80 | Avg loss: 0.0535 | Epsilon: 0.010\n",
            "Episode 446/1000 | Reward: 5.00 | Avg loss: 0.0562 | Epsilon: 0.010\n",
            "Episode 447/1000 | Reward: 2.60 | Avg loss: 0.0619 | Epsilon: 0.010\n",
            "Episode 448/1000 | Reward: 2.80 | Avg loss: 0.0611 | Epsilon: 0.010\n",
            "Episode 449/1000 | Reward: 9.90 | Avg loss: 0.0557 | Epsilon: 0.010\n",
            "Episode 450/1000 | Reward: 2.10 | Avg loss: 0.0611 | Epsilon: 0.010\n",
            "Episode 451/1000 | Reward: 2.10 | Avg loss: 0.0657 | Epsilon: 0.010\n",
            "Episode 452/1000 | Reward: 3.10 | Avg loss: 0.0595 | Epsilon: 0.010\n",
            "Episode 453/1000 | Reward: 7.70 | Avg loss: 0.0563 | Epsilon: 0.010\n",
            "Episode 454/1000 | Reward: 2.00 | Avg loss: 0.0520 | Epsilon: 0.010\n",
            "Episode 455/1000 | Reward: -2.30 | Avg loss: 0.0494 | Epsilon: 0.010\n",
            "Episode 456/1000 | Reward: 2.60 | Avg loss: 0.0499 | Epsilon: 0.010\n",
            "Episode 457/1000 | Reward: 1.30 | Avg loss: 0.0554 | Epsilon: 0.010\n",
            "Episode 458/1000 | Reward: 2.20 | Avg loss: 0.0567 | Epsilon: 0.010\n",
            "Episode 459/1000 | Reward: 4.50 | Avg loss: 0.0555 | Epsilon: 0.010\n",
            "Episode 460/1000 | Reward: 2.80 | Avg loss: 0.0559 | Epsilon: 0.010\n",
            "Episode 461/1000 | Reward: 2.50 | Avg loss: 0.0597 | Epsilon: 0.010\n",
            "Episode 462/1000 | Reward: -0.90 | Avg loss: 0.0759 | Epsilon: 0.010\n",
            "Episode 463/1000 | Reward: -1.90 | Avg loss: 0.0708 | Epsilon: 0.010\n",
            "Episode 464/1000 | Reward: 0.90 | Avg loss: 0.0624 | Epsilon: 0.010\n",
            "Episode 465/1000 | Reward: -0.90 | Avg loss: 0.0618 | Epsilon: 0.010\n",
            "Episode 466/1000 | Reward: 0.90 | Avg loss: 0.0667 | Epsilon: 0.010\n",
            "Episode 467/1000 | Reward: 2.10 | Avg loss: 0.0641 | Epsilon: 0.010\n",
            "Episode 468/1000 | Reward: -0.10 | Avg loss: 0.0529 | Epsilon: 0.010\n",
            "Episode 469/1000 | Reward: 2.20 | Avg loss: 0.0572 | Epsilon: 0.010\n",
            "Episode 470/1000 | Reward: 1.90 | Avg loss: 0.0590 | Epsilon: 0.010\n",
            "Episode 471/1000 | Reward: 3.00 | Avg loss: 0.0638 | Epsilon: 0.010\n",
            "Episode 472/1000 | Reward: 0.40 | Avg loss: 0.0656 | Epsilon: 0.010\n",
            "Episode 473/1000 | Reward: -0.80 | Avg loss: 0.0738 | Epsilon: 0.010\n",
            "Episode 474/1000 | Reward: 1.50 | Avg loss: 0.0728 | Epsilon: 0.010\n",
            "Episode 475/1000 | Reward: 2.50 | Avg loss: 0.0680 | Epsilon: 0.010\n",
            "Episode 476/1000 | Reward: 2.30 | Avg loss: 0.0826 | Epsilon: 0.010\n",
            "Episode 477/1000 | Reward: -0.90 | Avg loss: 0.0823 | Epsilon: 0.010\n",
            "Episode 478/1000 | Reward: -0.90 | Avg loss: 0.0920 | Epsilon: 0.010\n",
            "Episode 479/1000 | Reward: 2.30 | Avg loss: 0.0986 | Epsilon: 0.010\n",
            "Episode 480/1000 | Reward: 4.20 | Avg loss: 0.0831 | Epsilon: 0.010\n",
            "Episode 481/1000 | Reward: 16.00 | Avg loss: 0.1028 | Epsilon: 0.010\n",
            "Episode 482/1000 | Reward: 9.60 | Avg loss: 0.0978 | Epsilon: 0.010\n",
            "Episode 483/1000 | Reward: 1.00 | Avg loss: 0.1137 | Epsilon: 0.010\n",
            "Episode 484/1000 | Reward: -0.90 | Avg loss: 0.1011 | Epsilon: 0.010\n",
            "Episode 485/1000 | Reward: -0.90 | Avg loss: 0.1430 | Epsilon: 0.010\n",
            "Episode 486/1000 | Reward: 2.00 | Avg loss: 0.0951 | Epsilon: 0.010\n",
            "Episode 487/1000 | Reward: 0.30 | Avg loss: 0.1101 | Epsilon: 0.010\n",
            "Episode 488/1000 | Reward: 3.10 | Avg loss: 0.1137 | Epsilon: 0.010\n",
            "Episode 489/1000 | Reward: 2.30 | Avg loss: 0.1149 | Epsilon: 0.010\n",
            "Episode 490/1000 | Reward: 1.30 | Avg loss: 0.1194 | Epsilon: 0.010\n",
            "Episode 491/1000 | Reward: 2.10 | Avg loss: 0.1113 | Epsilon: 0.010\n",
            "Episode 492/1000 | Reward: 2.10 | Avg loss: 0.0957 | Epsilon: 0.010\n",
            "Episode 493/1000 | Reward: 4.50 | Avg loss: 0.1020 | Epsilon: 0.010\n",
            "Episode 494/1000 | Reward: 2.70 | Avg loss: 0.1061 | Epsilon: 0.010\n",
            "Episode 495/1000 | Reward: -0.90 | Avg loss: 0.0914 | Epsilon: 0.010\n",
            "Episode 496/1000 | Reward: 2.70 | Avg loss: 0.1011 | Epsilon: 0.010\n",
            "Episode 497/1000 | Reward: -1.10 | Avg loss: 0.0760 | Epsilon: 0.010\n",
            "Episode 498/1000 | Reward: 3.30 | Avg loss: 0.0658 | Epsilon: 0.010\n",
            "Episode 499/1000 | Reward: -0.60 | Avg loss: 0.0750 | Epsilon: 0.010\n",
            "Episode 500/1000 | Reward: 2.60 | Avg loss: 0.0885 | Epsilon: 0.010\n",
            "Episode 501/1000 | Reward: -0.90 | Avg loss: 0.0804 | Epsilon: 0.010\n",
            "Episode 502/1000 | Reward: -0.60 | Avg loss: 0.0729 | Epsilon: 0.010\n",
            "Episode 503/1000 | Reward: -0.90 | Avg loss: 0.0823 | Epsilon: 0.010\n",
            "Episode 504/1000 | Reward: 2.70 | Avg loss: 0.0810 | Epsilon: 0.010\n",
            "Episode 505/1000 | Reward: 2.90 | Avg loss: 0.0820 | Epsilon: 0.010\n",
            "Episode 506/1000 | Reward: 4.70 | Avg loss: 0.0890 | Epsilon: 0.010\n",
            "Episode 507/1000 | Reward: 1.10 | Avg loss: 0.1020 | Epsilon: 0.010\n",
            "Episode 508/1000 | Reward: 3.20 | Avg loss: 0.0815 | Epsilon: 0.010\n",
            "Episode 509/1000 | Reward: 0.50 | Avg loss: 0.0721 | Epsilon: 0.010\n",
            "Episode 510/1000 | Reward: -7.50 | Avg loss: 0.0945 | Epsilon: 0.010\n",
            "Episode 511/1000 | Reward: 3.70 | Avg loss: 0.0695 | Epsilon: 0.010\n",
            "Episode 512/1000 | Reward: 4.60 | Avg loss: 0.0703 | Epsilon: 0.010\n",
            "Episode 513/1000 | Reward: 3.80 | Avg loss: 0.0885 | Epsilon: 0.010\n",
            "Episode 514/1000 | Reward: 3.20 | Avg loss: 0.0698 | Epsilon: 0.010\n",
            "Episode 515/1000 | Reward: 2.90 | Avg loss: 0.0792 | Epsilon: 0.010\n",
            "Episode 516/1000 | Reward: 3.40 | Avg loss: 0.0742 | Epsilon: 0.010\n",
            "Episode 517/1000 | Reward: 2.80 | Avg loss: 0.0845 | Epsilon: 0.010\n",
            "Episode 518/1000 | Reward: 0.50 | Avg loss: 0.0766 | Epsilon: 0.010\n",
            "Episode 519/1000 | Reward: 3.40 | Avg loss: 0.0731 | Epsilon: 0.010\n",
            "Episode 520/1000 | Reward: 1.00 | Avg loss: 0.0734 | Epsilon: 0.010\n",
            "Episode 521/1000 | Reward: 2.40 | Avg loss: 0.0728 | Epsilon: 0.010\n",
            "Episode 522/1000 | Reward: 3.10 | Avg loss: 0.0808 | Epsilon: 0.010\n",
            "Episode 523/1000 | Reward: 3.10 | Avg loss: 0.0712 | Epsilon: 0.010\n",
            "Episode 524/1000 | Reward: -2.40 | Avg loss: 0.0832 | Epsilon: 0.010\n",
            "Episode 525/1000 | Reward: -5.60 | Avg loss: 0.0790 | Epsilon: 0.010\n",
            "Episode 526/1000 | Reward: 2.90 | Avg loss: 0.0660 | Epsilon: 0.010\n",
            "Episode 527/1000 | Reward: 2.80 | Avg loss: 0.0748 | Epsilon: 0.010\n",
            "Episode 528/1000 | Reward: 3.00 | Avg loss: 0.0731 | Epsilon: 0.010\n",
            "Episode 529/1000 | Reward: 6.50 | Avg loss: 0.0822 | Epsilon: 0.010\n",
            "Episode 530/1000 | Reward: 2.70 | Avg loss: 0.0734 | Epsilon: 0.010\n",
            "Episode 531/1000 | Reward: 1.50 | Avg loss: 0.0672 | Epsilon: 0.010\n",
            "Episode 532/1000 | Reward: 3.10 | Avg loss: 0.0696 | Epsilon: 0.010\n",
            "Episode 533/1000 | Reward: 2.20 | Avg loss: 0.0755 | Epsilon: 0.010\n",
            "Episode 534/1000 | Reward: 3.10 | Avg loss: 0.0732 | Epsilon: 0.010\n",
            "Episode 535/1000 | Reward: -0.90 | Avg loss: 0.0807 | Epsilon: 0.010\n",
            "Episode 536/1000 | Reward: 2.70 | Avg loss: 0.0805 | Epsilon: 0.010\n",
            "Episode 537/1000 | Reward: 2.20 | Avg loss: 0.0968 | Epsilon: 0.010\n",
            "Episode 538/1000 | Reward: -0.30 | Avg loss: 0.0891 | Epsilon: 0.010\n",
            "Episode 539/1000 | Reward: 3.50 | Avg loss: 0.0966 | Epsilon: 0.010\n",
            "Episode 540/1000 | Reward: -0.30 | Avg loss: 0.0791 | Epsilon: 0.010\n",
            "Episode 541/1000 | Reward: 2.50 | Avg loss: 0.0805 | Epsilon: 0.010\n",
            "Episode 542/1000 | Reward: 3.30 | Avg loss: 0.0801 | Epsilon: 0.010\n",
            "Episode 543/1000 | Reward: 5.30 | Avg loss: 0.0908 | Epsilon: 0.010\n",
            "Episode 544/1000 | Reward: 3.20 | Avg loss: 0.0922 | Epsilon: 0.010\n",
            "Episode 545/1000 | Reward: 2.10 | Avg loss: 0.0774 | Epsilon: 0.010\n",
            "Episode 546/1000 | Reward: 3.10 | Avg loss: 0.0885 | Epsilon: 0.010\n",
            "Episode 547/1000 | Reward: 3.00 | Avg loss: 0.0867 | Epsilon: 0.010\n",
            "Episode 548/1000 | Reward: 2.80 | Avg loss: 0.0861 | Epsilon: 0.010\n",
            "Episode 549/1000 | Reward: 3.60 | Avg loss: 0.0856 | Epsilon: 0.010\n",
            "Episode 550/1000 | Reward: 2.50 | Avg loss: 0.0995 | Epsilon: 0.010\n",
            "Episode 551/1000 | Reward: 3.10 | Avg loss: 0.0703 | Epsilon: 0.010\n",
            "Episode 552/1000 | Reward: 2.10 | Avg loss: 0.0873 | Epsilon: 0.010\n",
            "Episode 553/1000 | Reward: 3.00 | Avg loss: 0.0780 | Epsilon: 0.010\n",
            "Episode 554/1000 | Reward: 2.00 | Avg loss: 0.0722 | Epsilon: 0.010\n",
            "Episode 555/1000 | Reward: 15.20 | Avg loss: 0.0809 | Epsilon: 0.010\n",
            "Episode 556/1000 | Reward: 2.80 | Avg loss: 0.0756 | Epsilon: 0.010\n",
            "Episode 557/1000 | Reward: 1.50 | Avg loss: 0.0800 | Epsilon: 0.010\n",
            "Episode 558/1000 | Reward: 2.70 | Avg loss: 0.0834 | Epsilon: 0.010\n",
            "Episode 559/1000 | Reward: 2.50 | Avg loss: 0.0713 | Epsilon: 0.010\n",
            "Episode 560/1000 | Reward: 2.50 | Avg loss: 0.0716 | Epsilon: 0.010\n",
            "Episode 561/1000 | Reward: 5.00 | Avg loss: 0.0785 | Epsilon: 0.010\n",
            "Episode 562/1000 | Reward: 1.00 | Avg loss: 0.0816 | Epsilon: 0.010\n",
            "Episode 563/1000 | Reward: 5.30 | Avg loss: 0.0828 | Epsilon: 0.010\n",
            "Episode 564/1000 | Reward: -0.80 | Avg loss: 0.0811 | Epsilon: 0.010\n",
            "Episode 565/1000 | Reward: 1.80 | Avg loss: 0.0881 | Epsilon: 0.010\n",
            "Episode 566/1000 | Reward: 0.20 | Avg loss: 0.0823 | Epsilon: 0.010\n",
            "Episode 567/1000 | Reward: 2.70 | Avg loss: 0.0869 | Epsilon: 0.010\n",
            "Episode 568/1000 | Reward: 2.10 | Avg loss: 0.0838 | Epsilon: 0.010\n",
            "Episode 569/1000 | Reward: -0.90 | Avg loss: 0.0936 | Epsilon: 0.010\n",
            "Episode 570/1000 | Reward: 3.00 | Avg loss: 0.0795 | Epsilon: 0.010\n",
            "Episode 571/1000 | Reward: -0.90 | Avg loss: 0.0766 | Epsilon: 0.010\n",
            "Episode 572/1000 | Reward: 3.00 | Avg loss: 0.0824 | Epsilon: 0.010\n",
            "Episode 573/1000 | Reward: 1.90 | Avg loss: 0.0645 | Epsilon: 0.010\n",
            "Episode 574/1000 | Reward: -0.90 | Avg loss: 0.0707 | Epsilon: 0.010\n",
            "Episode 575/1000 | Reward: -0.90 | Avg loss: 0.0812 | Epsilon: 0.010\n",
            "Episode 576/1000 | Reward: -0.90 | Avg loss: 0.0686 | Epsilon: 0.010\n",
            "Episode 577/1000 | Reward: -0.90 | Avg loss: 0.0708 | Epsilon: 0.010\n",
            "Episode 578/1000 | Reward: 2.40 | Avg loss: 0.0683 | Epsilon: 0.010\n",
            "Episode 579/1000 | Reward: 5.70 | Avg loss: 0.0728 | Epsilon: 0.010\n",
            "Episode 580/1000 | Reward: 1.80 | Avg loss: 0.0623 | Epsilon: 0.010\n",
            "Episode 581/1000 | Reward: 2.70 | Avg loss: 0.0699 | Epsilon: 0.010\n",
            "Episode 582/1000 | Reward: 0.80 | Avg loss: 0.0683 | Epsilon: 0.010\n",
            "Episode 583/1000 | Reward: 3.10 | Avg loss: 0.0698 | Epsilon: 0.010\n",
            "Episode 584/1000 | Reward: 1.50 | Avg loss: 0.0671 | Epsilon: 0.010\n",
            "Episode 585/1000 | Reward: 2.70 | Avg loss: 0.0711 | Epsilon: 0.010\n",
            "Episode 586/1000 | Reward: 11.00 | Avg loss: 0.0679 | Epsilon: 0.010\n",
            "Episode 587/1000 | Reward: 2.90 | Avg loss: 0.0590 | Epsilon: 0.010\n",
            "Episode 588/1000 | Reward: 4.40 | Avg loss: 0.0679 | Epsilon: 0.010\n",
            "Episode 589/1000 | Reward: 0.40 | Avg loss: 0.0652 | Epsilon: 0.010\n",
            "Episode 590/1000 | Reward: 2.90 | Avg loss: 0.0661 | Epsilon: 0.010\n",
            "Episode 591/1000 | Reward: 4.80 | Avg loss: 0.0727 | Epsilon: 0.010\n",
            "Episode 592/1000 | Reward: -0.90 | Avg loss: 0.0815 | Epsilon: 0.010\n",
            "Episode 593/1000 | Reward: 2.30 | Avg loss: 0.0643 | Epsilon: 0.010\n",
            "Episode 594/1000 | Reward: 0.90 | Avg loss: 0.0786 | Epsilon: 0.010\n",
            "Episode 595/1000 | Reward: 0.30 | Avg loss: 0.0870 | Epsilon: 0.010\n",
            "Episode 596/1000 | Reward: 2.80 | Avg loss: 0.0879 | Epsilon: 0.010\n",
            "Episode 597/1000 | Reward: 0.90 | Avg loss: 0.0820 | Epsilon: 0.010\n",
            "Episode 598/1000 | Reward: -0.90 | Avg loss: 0.0967 | Epsilon: 0.010\n",
            "Episode 599/1000 | Reward: 1.50 | Avg loss: 0.0697 | Epsilon: 0.010\n",
            "Episode 600/1000 | Reward: 7.00 | Avg loss: 0.0820 | Epsilon: 0.010\n",
            "Episode 601/1000 | Reward: 1.20 | Avg loss: 0.0677 | Epsilon: 0.010\n",
            "Episode 602/1000 | Reward: 1.20 | Avg loss: 0.0754 | Epsilon: 0.010\n",
            "Episode 603/1000 | Reward: 1.80 | Avg loss: 0.0869 | Epsilon: 0.010\n",
            "Episode 604/1000 | Reward: 2.00 | Avg loss: 0.0799 | Epsilon: 0.010\n",
            "Episode 605/1000 | Reward: 2.60 | Avg loss: 0.0751 | Epsilon: 0.010\n",
            "Episode 606/1000 | Reward: 1.20 | Avg loss: 0.0656 | Epsilon: 0.010\n",
            "Episode 607/1000 | Reward: 2.20 | Avg loss: 0.0680 | Epsilon: 0.010\n",
            "Episode 608/1000 | Reward: 0.30 | Avg loss: 0.0710 | Epsilon: 0.010\n",
            "Episode 609/1000 | Reward: 4.70 | Avg loss: 0.0722 | Epsilon: 0.010\n",
            "Episode 610/1000 | Reward: 0.10 | Avg loss: 0.0764 | Epsilon: 0.010\n",
            "Episode 611/1000 | Reward: 2.40 | Avg loss: 0.0813 | Epsilon: 0.010\n",
            "Episode 612/1000 | Reward: 0.30 | Avg loss: 0.0740 | Epsilon: 0.010\n",
            "Episode 613/1000 | Reward: 1.70 | Avg loss: 0.0867 | Epsilon: 0.010\n",
            "Episode 614/1000 | Reward: 1.50 | Avg loss: 0.0987 | Epsilon: 0.010\n",
            "Episode 615/1000 | Reward: 2.80 | Avg loss: 0.0901 | Epsilon: 0.010\n",
            "Episode 616/1000 | Reward: 5.80 | Avg loss: 0.0758 | Epsilon: 0.010\n",
            "Episode 617/1000 | Reward: -0.80 | Avg loss: 0.0823 | Epsilon: 0.010\n",
            "Episode 618/1000 | Reward: 3.40 | Avg loss: 0.0850 | Epsilon: 0.010\n",
            "Episode 619/1000 | Reward: 3.20 | Avg loss: 0.0950 | Epsilon: 0.010\n",
            "Episode 620/1000 | Reward: 0.30 | Avg loss: 0.0839 | Epsilon: 0.010\n",
            "Episode 621/1000 | Reward: 3.20 | Avg loss: 0.1097 | Epsilon: 0.010\n",
            "Episode 622/1000 | Reward: 2.00 | Avg loss: 0.1001 | Epsilon: 0.010\n",
            "Episode 623/1000 | Reward: 2.00 | Avg loss: 0.0851 | Epsilon: 0.010\n",
            "Episode 624/1000 | Reward: 2.10 | Avg loss: 0.0843 | Epsilon: 0.010\n",
            "Episode 625/1000 | Reward: 2.20 | Avg loss: 0.0832 | Epsilon: 0.010\n",
            "Episode 626/1000 | Reward: 5.40 | Avg loss: 0.0848 | Epsilon: 0.010\n",
            "Episode 627/1000 | Reward: 8.60 | Avg loss: 0.0821 | Epsilon: 0.010\n",
            "Episode 628/1000 | Reward: 2.20 | Avg loss: 0.0671 | Epsilon: 0.010\n",
            "Episode 629/1000 | Reward: 3.00 | Avg loss: 0.0810 | Epsilon: 0.010\n",
            "Episode 630/1000 | Reward: 5.90 | Avg loss: 0.0829 | Epsilon: 0.010\n",
            "Episode 631/1000 | Reward: 10.00 | Avg loss: 0.0704 | Epsilon: 0.010\n",
            "Episode 632/1000 | Reward: 2.10 | Avg loss: 0.0799 | Epsilon: 0.010\n",
            "Episode 633/1000 | Reward: 2.70 | Avg loss: 0.0695 | Epsilon: 0.010\n",
            "Episode 634/1000 | Reward: 0.30 | Avg loss: 0.0604 | Epsilon: 0.010\n",
            "Episode 635/1000 | Reward: -4.20 | Avg loss: 0.0753 | Epsilon: 0.010\n",
            "Episode 636/1000 | Reward: 0.30 | Avg loss: 0.0663 | Epsilon: 0.010\n",
            "Episode 637/1000 | Reward: -0.30 | Avg loss: 0.0647 | Epsilon: 0.010\n",
            "Episode 638/1000 | Reward: 7.00 | Avg loss: 0.0710 | Epsilon: 0.010\n",
            "Episode 639/1000 | Reward: 6.10 | Avg loss: 0.0693 | Epsilon: 0.010\n",
            "Episode 640/1000 | Reward: -1.20 | Avg loss: 0.0744 | Epsilon: 0.010\n",
            "Episode 641/1000 | Reward: -0.30 | Avg loss: 0.0676 | Epsilon: 0.010\n",
            "Episode 642/1000 | Reward: 2.60 | Avg loss: 0.0886 | Epsilon: 0.010\n",
            "Episode 643/1000 | Reward: 6.90 | Avg loss: 0.0894 | Epsilon: 0.010\n",
            "Episode 644/1000 | Reward: -0.80 | Avg loss: 0.0825 | Epsilon: 0.010\n",
            "Episode 645/1000 | Reward: -4.20 | Avg loss: 0.0616 | Epsilon: 0.010\n",
            "Episode 646/1000 | Reward: 2.70 | Avg loss: 0.0655 | Epsilon: 0.010\n",
            "Episode 647/1000 | Reward: 2.50 | Avg loss: 0.0775 | Epsilon: 0.010\n",
            "Episode 648/1000 | Reward: 2.50 | Avg loss: 0.0706 | Epsilon: 0.010\n",
            "Episode 649/1000 | Reward: 2.70 | Avg loss: 0.0899 | Epsilon: 0.010\n",
            "Episode 650/1000 | Reward: -0.20 | Avg loss: 0.0651 | Epsilon: 0.010\n",
            "Episode 651/1000 | Reward: 2.90 | Avg loss: 0.0755 | Epsilon: 0.010\n",
            "Episode 652/1000 | Reward: 3.10 | Avg loss: 0.0715 | Epsilon: 0.010\n",
            "Episode 653/1000 | Reward: -0.90 | Avg loss: 0.0933 | Epsilon: 0.010\n",
            "Episode 654/1000 | Reward: 2.40 | Avg loss: 0.0863 | Epsilon: 0.010\n",
            "Episode 655/1000 | Reward: 3.40 | Avg loss: 0.0932 | Epsilon: 0.010\n",
            "Episode 656/1000 | Reward: 2.90 | Avg loss: 0.0983 | Epsilon: 0.010\n",
            "Episode 657/1000 | Reward: 12.60 | Avg loss: 0.0801 | Epsilon: 0.010\n",
            "Episode 658/1000 | Reward: -0.80 | Avg loss: 0.0784 | Epsilon: 0.010\n",
            "Episode 659/1000 | Reward: 1.30 | Avg loss: 0.0866 | Epsilon: 0.010\n",
            "Episode 660/1000 | Reward: 1.70 | Avg loss: 0.0821 | Epsilon: 0.010\n",
            "Episode 661/1000 | Reward: -0.30 | Avg loss: 0.0753 | Epsilon: 0.010\n",
            "Episode 662/1000 | Reward: 1.20 | Avg loss: 0.0820 | Epsilon: 0.010\n",
            "Episode 663/1000 | Reward: 2.30 | Avg loss: 0.0739 | Epsilon: 0.010\n",
            "Episode 664/1000 | Reward: 2.20 | Avg loss: 0.0847 | Epsilon: 0.010\n",
            "Episode 665/1000 | Reward: 3.00 | Avg loss: 0.0740 | Epsilon: 0.010\n",
            "Episode 666/1000 | Reward: 3.10 | Avg loss: 0.0767 | Epsilon: 0.010\n",
            "Episode 667/1000 | Reward: 2.50 | Avg loss: 0.0726 | Epsilon: 0.010\n",
            "Episode 668/1000 | Reward: 2.60 | Avg loss: 0.0661 | Epsilon: 0.010\n",
            "Episode 669/1000 | Reward: -0.90 | Avg loss: 0.0763 | Epsilon: 0.010\n",
            "Episode 670/1000 | Reward: -1.60 | Avg loss: 0.0680 | Epsilon: 0.010\n",
            "Episode 671/1000 | Reward: 3.30 | Avg loss: 0.0812 | Epsilon: 0.010\n",
            "Episode 672/1000 | Reward: 5.30 | Avg loss: 0.0800 | Epsilon: 0.010\n",
            "Episode 673/1000 | Reward: 1.50 | Avg loss: 0.0874 | Epsilon: 0.010\n",
            "Episode 674/1000 | Reward: 14.70 | Avg loss: 0.0784 | Epsilon: 0.010\n",
            "Episode 675/1000 | Reward: 2.50 | Avg loss: 0.0797 | Epsilon: 0.010\n",
            "Episode 676/1000 | Reward: 2.80 | Avg loss: 0.0766 | Epsilon: 0.010\n",
            "Episode 677/1000 | Reward: 6.30 | Avg loss: 0.0806 | Epsilon: 0.010\n",
            "Episode 678/1000 | Reward: 2.30 | Avg loss: 0.0981 | Epsilon: 0.010\n",
            "Episode 679/1000 | Reward: 0.60 | Avg loss: 0.1028 | Epsilon: 0.010\n",
            "Episode 680/1000 | Reward: 0.90 | Avg loss: 0.0888 | Epsilon: 0.010\n",
            "Episode 681/1000 | Reward: 3.70 | Avg loss: 0.0982 | Epsilon: 0.010\n",
            "Episode 682/1000 | Reward: 0.90 | Avg loss: 0.1146 | Epsilon: 0.010\n",
            "Episode 683/1000 | Reward: 2.90 | Avg loss: 0.1118 | Epsilon: 0.010\n",
            "Episode 684/1000 | Reward: 2.20 | Avg loss: 0.1141 | Epsilon: 0.010\n",
            "Episode 685/1000 | Reward: 7.60 | Avg loss: 0.0977 | Epsilon: 0.010\n",
            "Episode 686/1000 | Reward: 2.60 | Avg loss: 0.0913 | Epsilon: 0.010\n",
            "Episode 687/1000 | Reward: 2.40 | Avg loss: 0.0732 | Epsilon: 0.010\n",
            "Episode 688/1000 | Reward: -0.90 | Avg loss: 0.0978 | Epsilon: 0.010\n",
            "Episode 689/1000 | Reward: 2.90 | Avg loss: 0.0946 | Epsilon: 0.010\n",
            "Episode 690/1000 | Reward: 2.60 | Avg loss: 0.0852 | Epsilon: 0.010\n",
            "Episode 691/1000 | Reward: -0.10 | Avg loss: 0.0961 | Epsilon: 0.010\n",
            "Episode 692/1000 | Reward: 2.80 | Avg loss: 0.0929 | Epsilon: 0.010\n",
            "Episode 693/1000 | Reward: 5.10 | Avg loss: 0.0926 | Epsilon: 0.010\n",
            "Episode 694/1000 | Reward: -0.90 | Avg loss: 0.0957 | Epsilon: 0.010\n",
            "Episode 695/1000 | Reward: 9.40 | Avg loss: 0.0969 | Epsilon: 0.010\n",
            "Episode 696/1000 | Reward: 2.60 | Avg loss: 0.0949 | Epsilon: 0.010\n",
            "Episode 697/1000 | Reward: 2.60 | Avg loss: 0.0957 | Epsilon: 0.010\n",
            "Episode 698/1000 | Reward: 1.40 | Avg loss: 0.0790 | Epsilon: 0.010\n",
            "Episode 699/1000 | Reward: -0.90 | Avg loss: 0.0827 | Epsilon: 0.010\n",
            "Episode 700/1000 | Reward: 2.90 | Avg loss: 0.1055 | Epsilon: 0.010\n",
            "Episode 701/1000 | Reward: 2.80 | Avg loss: 0.0987 | Epsilon: 0.010\n",
            "Episode 702/1000 | Reward: 2.70 | Avg loss: 0.0815 | Epsilon: 0.010\n",
            "Episode 703/1000 | Reward: 2.40 | Avg loss: 0.0755 | Epsilon: 0.010\n",
            "Episode 704/1000 | Reward: 4.90 | Avg loss: 0.0729 | Epsilon: 0.010\n",
            "Episode 705/1000 | Reward: 2.80 | Avg loss: 0.0763 | Epsilon: 0.010\n",
            "Episode 706/1000 | Reward: 5.80 | Avg loss: 0.0801 | Epsilon: 0.010\n",
            "Episode 707/1000 | Reward: 2.40 | Avg loss: 0.0758 | Epsilon: 0.010\n",
            "Episode 708/1000 | Reward: 1.30 | Avg loss: 0.0763 | Epsilon: 0.010\n",
            "Episode 709/1000 | Reward: 2.40 | Avg loss: 0.0790 | Epsilon: 0.010\n",
            "Episode 710/1000 | Reward: 2.60 | Avg loss: 0.0774 | Epsilon: 0.010\n",
            "Episode 711/1000 | Reward: 0.70 | Avg loss: 0.0669 | Epsilon: 0.010\n",
            "Episode 712/1000 | Reward: -0.90 | Avg loss: 0.0771 | Epsilon: 0.010\n",
            "Episode 713/1000 | Reward: 5.30 | Avg loss: 0.0854 | Epsilon: 0.010\n",
            "Episode 714/1000 | Reward: 2.00 | Avg loss: 0.1104 | Epsilon: 0.010\n",
            "Episode 715/1000 | Reward: 2.00 | Avg loss: 0.0930 | Epsilon: 0.010\n",
            "Episode 716/1000 | Reward: 0.90 | Avg loss: 0.0934 | Epsilon: 0.010\n",
            "Episode 717/1000 | Reward: 2.50 | Avg loss: 0.0896 | Epsilon: 0.010\n",
            "Episode 718/1000 | Reward: 1.50 | Avg loss: 0.0867 | Epsilon: 0.010\n",
            "Episode 719/1000 | Reward: 2.20 | Avg loss: 0.0867 | Epsilon: 0.010\n",
            "Episode 720/1000 | Reward: 2.10 | Avg loss: 0.0840 | Epsilon: 0.010\n",
            "Episode 721/1000 | Reward: 2.30 | Avg loss: 0.0932 | Epsilon: 0.010\n",
            "Episode 722/1000 | Reward: -0.90 | Avg loss: 0.0854 | Epsilon: 0.010\n",
            "Episode 723/1000 | Reward: 5.50 | Avg loss: 0.0979 | Epsilon: 0.010\n",
            "Episode 724/1000 | Reward: 2.70 | Avg loss: 0.0868 | Epsilon: 0.010\n",
            "Episode 725/1000 | Reward: 2.80 | Avg loss: 0.1094 | Epsilon: 0.010\n",
            "Episode 726/1000 | Reward: 2.60 | Avg loss: 0.1296 | Epsilon: 0.010\n",
            "Episode 727/1000 | Reward: 3.60 | Avg loss: 0.0927 | Epsilon: 0.010\n",
            "Episode 728/1000 | Reward: 7.00 | Avg loss: 0.0867 | Epsilon: 0.010\n",
            "Episode 729/1000 | Reward: 1.50 | Avg loss: 0.1007 | Epsilon: 0.010\n",
            "Episode 730/1000 | Reward: 2.90 | Avg loss: 0.0782 | Epsilon: 0.010\n",
            "Episode 731/1000 | Reward: -0.90 | Avg loss: 0.0772 | Epsilon: 0.010\n",
            "Episode 732/1000 | Reward: 2.50 | Avg loss: 0.0671 | Epsilon: 0.010\n",
            "Episode 733/1000 | Reward: -0.90 | Avg loss: 0.0622 | Epsilon: 0.010\n",
            "Episode 734/1000 | Reward: 5.10 | Avg loss: 0.0726 | Epsilon: 0.010\n",
            "Episode 735/1000 | Reward: -0.30 | Avg loss: 0.0723 | Epsilon: 0.010\n",
            "Episode 736/1000 | Reward: 2.80 | Avg loss: 0.0804 | Epsilon: 0.010\n",
            "Episode 737/1000 | Reward: 0.60 | Avg loss: 0.0919 | Epsilon: 0.010\n",
            "Episode 738/1000 | Reward: 3.10 | Avg loss: 0.0816 | Epsilon: 0.010\n",
            "Episode 739/1000 | Reward: 1.50 | Avg loss: 0.0803 | Epsilon: 0.010\n",
            "Episode 740/1000 | Reward: 0.20 | Avg loss: 0.0831 | Epsilon: 0.010\n",
            "Episode 741/1000 | Reward: 2.20 | Avg loss: 0.1012 | Epsilon: 0.010\n",
            "Episode 742/1000 | Reward: 0.30 | Avg loss: 0.0679 | Epsilon: 0.010\n",
            "Episode 743/1000 | Reward: 2.10 | Avg loss: 0.0748 | Epsilon: 0.010\n",
            "Episode 744/1000 | Reward: 0.90 | Avg loss: 0.0789 | Epsilon: 0.010\n",
            "Episode 745/1000 | Reward: 0.90 | Avg loss: 0.0794 | Epsilon: 0.010\n",
            "Episode 746/1000 | Reward: -0.90 | Avg loss: 0.0739 | Epsilon: 0.010\n",
            "Episode 747/1000 | Reward: 3.00 | Avg loss: 0.0718 | Epsilon: 0.010\n",
            "Episode 748/1000 | Reward: 10.20 | Avg loss: 0.0864 | Epsilon: 0.010\n",
            "Episode 749/1000 | Reward: 2.10 | Avg loss: 0.1217 | Epsilon: 0.010\n",
            "Episode 750/1000 | Reward: -0.30 | Avg loss: 0.1100 | Epsilon: 0.010\n",
            "Episode 751/1000 | Reward: 3.00 | Avg loss: 0.0876 | Epsilon: 0.010\n",
            "Episode 752/1000 | Reward: 0.30 | Avg loss: 0.1010 | Epsilon: 0.010\n",
            "Episode 753/1000 | Reward: -0.90 | Avg loss: 0.1128 | Epsilon: 0.010\n",
            "Episode 754/1000 | Reward: 5.80 | Avg loss: 0.1086 | Epsilon: 0.010\n",
            "Episode 755/1000 | Reward: -0.90 | Avg loss: 0.1270 | Epsilon: 0.010\n",
            "Episode 756/1000 | Reward: 2.10 | Avg loss: 0.1324 | Epsilon: 0.010\n",
            "Episode 757/1000 | Reward: -0.90 | Avg loss: 0.1235 | Epsilon: 0.010\n",
            "Episode 758/1000 | Reward: -0.90 | Avg loss: 0.1043 | Epsilon: 0.010\n",
            "Episode 759/1000 | Reward: 0.00 | Avg loss: 0.1208 | Epsilon: 0.010\n",
            "Episode 760/1000 | Reward: 1.60 | Avg loss: 0.1070 | Epsilon: 0.010\n",
            "Episode 761/1000 | Reward: 0.10 | Avg loss: 0.1088 | Epsilon: 0.010\n",
            "Episode 762/1000 | Reward: 3.00 | Avg loss: 0.1130 | Epsilon: 0.010\n",
            "Episode 763/1000 | Reward: 4.50 | Avg loss: 0.1115 | Epsilon: 0.010\n",
            "Episode 764/1000 | Reward: 11.60 | Avg loss: 0.0961 | Epsilon: 0.010\n",
            "Episode 765/1000 | Reward: 4.50 | Avg loss: 0.0777 | Epsilon: 0.010\n",
            "Episode 766/1000 | Reward: -0.10 | Avg loss: 0.0806 | Epsilon: 0.010\n",
            "Episode 767/1000 | Reward: 3.00 | Avg loss: 0.0670 | Epsilon: 0.010\n",
            "Episode 768/1000 | Reward: 2.50 | Avg loss: 0.0705 | Epsilon: 0.010\n",
            "Episode 769/1000 | Reward: -0.90 | Avg loss: 0.0857 | Epsilon: 0.010\n",
            "Episode 770/1000 | Reward: -0.90 | Avg loss: 0.0710 | Epsilon: 0.010\n",
            "Episode 771/1000 | Reward: -0.90 | Avg loss: 0.0731 | Epsilon: 0.010\n",
            "Episode 772/1000 | Reward: -0.90 | Avg loss: 0.1112 | Epsilon: 0.010\n",
            "Episode 773/1000 | Reward: -0.10 | Avg loss: 0.0869 | Epsilon: 0.010\n",
            "Episode 774/1000 | Reward: 0.80 | Avg loss: 0.1058 | Epsilon: 0.010\n",
            "Episode 775/1000 | Reward: -0.90 | Avg loss: 0.1314 | Epsilon: 0.010\n",
            "Episode 776/1000 | Reward: -0.90 | Avg loss: 0.1417 | Epsilon: 0.010\n",
            "Episode 777/1000 | Reward: -1.80 | Avg loss: 0.1263 | Epsilon: 0.010\n",
            "Episode 778/1000 | Reward: 2.50 | Avg loss: 0.1254 | Epsilon: 0.010\n",
            "Episode 779/1000 | Reward: 2.60 | Avg loss: 0.0824 | Epsilon: 0.010\n",
            "Episode 780/1000 | Reward: 0.40 | Avg loss: 0.0853 | Epsilon: 0.010\n",
            "Episode 781/1000 | Reward: 2.20 | Avg loss: 0.0667 | Epsilon: 0.010\n",
            "Episode 782/1000 | Reward: 10.60 | Avg loss: 0.0753 | Epsilon: 0.010\n",
            "Episode 783/1000 | Reward: 1.00 | Avg loss: 0.0872 | Epsilon: 0.010\n",
            "Episode 784/1000 | Reward: 1.50 | Avg loss: 0.0954 | Epsilon: 0.010\n",
            "Episode 785/1000 | Reward: 2.00 | Avg loss: 0.0824 | Epsilon: 0.010\n",
            "Episode 786/1000 | Reward: 3.10 | Avg loss: 0.0895 | Epsilon: 0.010\n",
            "Episode 787/1000 | Reward: 2.80 | Avg loss: 0.0767 | Epsilon: 0.010\n",
            "Episode 788/1000 | Reward: 3.00 | Avg loss: 0.0767 | Epsilon: 0.010\n",
            "Episode 789/1000 | Reward: 8.80 | Avg loss: 0.0858 | Epsilon: 0.010\n",
            "Episode 790/1000 | Reward: -0.60 | Avg loss: 0.0743 | Epsilon: 0.010\n",
            "Episode 791/1000 | Reward: 2.60 | Avg loss: 0.0989 | Epsilon: 0.010\n",
            "Episode 792/1000 | Reward: 0.90 | Avg loss: 0.0791 | Epsilon: 0.010\n",
            "Episode 793/1000 | Reward: 2.60 | Avg loss: 0.0970 | Epsilon: 0.010\n",
            "Episode 794/1000 | Reward: 5.30 | Avg loss: 0.0921 | Epsilon: 0.010\n",
            "Episode 795/1000 | Reward: 2.30 | Avg loss: 0.0976 | Epsilon: 0.010\n",
            "Episode 796/1000 | Reward: 3.70 | Avg loss: 0.1073 | Epsilon: 0.010\n",
            "Episode 797/1000 | Reward: 2.20 | Avg loss: 0.1319 | Epsilon: 0.010\n",
            "Episode 798/1000 | Reward: 15.50 | Avg loss: 0.0860 | Epsilon: 0.010\n",
            "Episode 799/1000 | Reward: 13.70 | Avg loss: 0.0893 | Epsilon: 0.010\n",
            "Episode 800/1000 | Reward: 7.40 | Avg loss: 0.0784 | Epsilon: 0.010\n",
            "Episode 801/1000 | Reward: 6.10 | Avg loss: 0.0795 | Epsilon: 0.010\n",
            "Episode 802/1000 | Reward: 4.10 | Avg loss: 0.0859 | Epsilon: 0.010\n",
            "Episode 803/1000 | Reward: -0.90 | Avg loss: 0.0910 | Epsilon: 0.010\n",
            "Episode 804/1000 | Reward: 6.50 | Avg loss: 0.0691 | Epsilon: 0.010\n",
            "Episode 805/1000 | Reward: -0.90 | Avg loss: 0.0669 | Epsilon: 0.010\n",
            "Episode 806/1000 | Reward: 1.00 | Avg loss: 0.0705 | Epsilon: 0.010\n",
            "Episode 807/1000 | Reward: 2.40 | Avg loss: 0.0858 | Epsilon: 0.010\n",
            "Episode 808/1000 | Reward: 2.90 | Avg loss: 0.0870 | Epsilon: 0.010\n",
            "Episode 809/1000 | Reward: 2.30 | Avg loss: 0.0818 | Epsilon: 0.010\n",
            "Episode 810/1000 | Reward: -1.00 | Avg loss: 0.1103 | Epsilon: 0.010\n",
            "Episode 811/1000 | Reward: 2.60 | Avg loss: 0.1007 | Epsilon: 0.010\n",
            "Episode 812/1000 | Reward: 2.30 | Avg loss: 0.1100 | Epsilon: 0.010\n",
            "Episode 813/1000 | Reward: -1.20 | Avg loss: 0.0961 | Epsilon: 0.010\n",
            "Episode 814/1000 | Reward: 1.50 | Avg loss: 0.0988 | Epsilon: 0.010\n",
            "Episode 815/1000 | Reward: 4.90 | Avg loss: 0.0950 | Epsilon: 0.010\n",
            "Episode 816/1000 | Reward: 2.70 | Avg loss: 0.1050 | Epsilon: 0.010\n",
            "Episode 817/1000 | Reward: -0.30 | Avg loss: 0.1313 | Epsilon: 0.010\n",
            "Episode 818/1000 | Reward: 1.50 | Avg loss: 0.1027 | Epsilon: 0.010\n",
            "Episode 819/1000 | Reward: 3.70 | Avg loss: 0.0891 | Epsilon: 0.010\n",
            "Episode 820/1000 | Reward: 0.70 | Avg loss: 0.0812 | Epsilon: 0.010\n",
            "Episode 821/1000 | Reward: 1.80 | Avg loss: 0.1069 | Epsilon: 0.010\n",
            "Episode 822/1000 | Reward: -0.10 | Avg loss: 0.1031 | Epsilon: 0.010\n",
            "Episode 823/1000 | Reward: 3.00 | Avg loss: 0.0935 | Epsilon: 0.010\n",
            "Episode 824/1000 | Reward: -0.90 | Avg loss: 0.1103 | Epsilon: 0.010\n",
            "Episode 825/1000 | Reward: 2.80 | Avg loss: 0.1120 | Epsilon: 0.010\n",
            "Episode 826/1000 | Reward: 12.00 | Avg loss: 0.1051 | Epsilon: 0.010\n",
            "Episode 827/1000 | Reward: 2.60 | Avg loss: 0.1129 | Epsilon: 0.010\n",
            "Episode 828/1000 | Reward: 1.70 | Avg loss: 0.1022 | Epsilon: 0.010\n",
            "Episode 829/1000 | Reward: -0.90 | Avg loss: 0.1193 | Epsilon: 0.010\n",
            "Episode 830/1000 | Reward: 7.10 | Avg loss: 0.1091 | Epsilon: 0.010\n",
            "Episode 831/1000 | Reward: 0.50 | Avg loss: 0.1076 | Epsilon: 0.010\n",
            "Episode 832/1000 | Reward: 4.70 | Avg loss: 0.1074 | Epsilon: 0.010\n",
            "Episode 833/1000 | Reward: -0.30 | Avg loss: 0.1142 | Epsilon: 0.010\n",
            "Episode 834/1000 | Reward: -0.90 | Avg loss: 0.1063 | Epsilon: 0.010\n",
            "Episode 835/1000 | Reward: -0.90 | Avg loss: 0.1008 | Epsilon: 0.010\n",
            "Episode 836/1000 | Reward: 4.40 | Avg loss: 0.0944 | Epsilon: 0.010\n",
            "Episode 837/1000 | Reward: -0.90 | Avg loss: 0.1043 | Epsilon: 0.010\n",
            "Episode 838/1000 | Reward: -0.90 | Avg loss: 0.0848 | Epsilon: 0.010\n",
            "Episode 839/1000 | Reward: 2.20 | Avg loss: 0.1127 | Epsilon: 0.010\n",
            "Episode 840/1000 | Reward: -0.70 | Avg loss: 0.1218 | Epsilon: 0.010\n",
            "Episode 841/1000 | Reward: 3.50 | Avg loss: 0.1176 | Epsilon: 0.010\n",
            "Episode 842/1000 | Reward: 2.10 | Avg loss: 0.1100 | Epsilon: 0.010\n",
            "Episode 843/1000 | Reward: 0.40 | Avg loss: 0.1111 | Epsilon: 0.010\n",
            "Episode 844/1000 | Reward: -0.90 | Avg loss: 0.1081 | Epsilon: 0.010\n",
            "Episode 845/1000 | Reward: -0.90 | Avg loss: 0.1008 | Epsilon: 0.010\n",
            "Episode 846/1000 | Reward: 1.50 | Avg loss: 0.0931 | Epsilon: 0.010\n",
            "Episode 847/1000 | Reward: 2.10 | Avg loss: 0.1061 | Epsilon: 0.010\n",
            "Episode 848/1000 | Reward: 3.60 | Avg loss: 0.0910 | Epsilon: 0.010\n",
            "Episode 849/1000 | Reward: 2.80 | Avg loss: 0.0850 | Epsilon: 0.010\n",
            "Episode 850/1000 | Reward: -0.90 | Avg loss: 0.1120 | Epsilon: 0.010\n",
            "Episode 851/1000 | Reward: 4.50 | Avg loss: 0.1107 | Epsilon: 0.010\n",
            "Episode 852/1000 | Reward: 2.20 | Avg loss: 0.1104 | Epsilon: 0.010\n",
            "Episode 853/1000 | Reward: -0.30 | Avg loss: 0.1000 | Epsilon: 0.010\n",
            "Episode 854/1000 | Reward: -0.60 | Avg loss: 0.1082 | Epsilon: 0.010\n",
            "Episode 855/1000 | Reward: 3.20 | Avg loss: 0.1201 | Epsilon: 0.010\n",
            "Episode 856/1000 | Reward: 2.40 | Avg loss: 0.0949 | Epsilon: 0.010\n",
            "Episode 857/1000 | Reward: 1.50 | Avg loss: 0.0887 | Epsilon: 0.010\n",
            "Episode 858/1000 | Reward: 5.10 | Avg loss: 0.1002 | Epsilon: 0.010\n",
            "Episode 859/1000 | Reward: 1.50 | Avg loss: 0.0921 | Epsilon: 0.010\n",
            "Episode 860/1000 | Reward: 2.70 | Avg loss: 0.0889 | Epsilon: 0.010\n",
            "Episode 861/1000 | Reward: 4.50 | Avg loss: 0.1052 | Epsilon: 0.010\n",
            "Episode 862/1000 | Reward: 2.60 | Avg loss: 0.1076 | Epsilon: 0.010\n",
            "Episode 863/1000 | Reward: 2.90 | Avg loss: 0.1046 | Epsilon: 0.010\n",
            "Episode 864/1000 | Reward: 2.30 | Avg loss: 0.0948 | Epsilon: 0.010\n",
            "Episode 865/1000 | Reward: 3.10 | Avg loss: 0.1033 | Epsilon: 0.010\n",
            "Episode 866/1000 | Reward: 0.90 | Avg loss: 0.0850 | Epsilon: 0.010\n",
            "Episode 867/1000 | Reward: 2.80 | Avg loss: 0.0975 | Epsilon: 0.010\n",
            "Episode 868/1000 | Reward: -0.90 | Avg loss: 0.0917 | Epsilon: 0.010\n",
            "Episode 869/1000 | Reward: -1.10 | Avg loss: 0.0934 | Epsilon: 0.010\n",
            "Episode 870/1000 | Reward: -0.50 | Avg loss: 0.1129 | Epsilon: 0.010\n",
            "Episode 871/1000 | Reward: 8.30 | Avg loss: 0.1071 | Epsilon: 0.010\n",
            "Episode 872/1000 | Reward: -0.60 | Avg loss: 0.1112 | Epsilon: 0.010\n",
            "Episode 873/1000 | Reward: -1.40 | Avg loss: 0.1025 | Epsilon: 0.010\n",
            "Episode 874/1000 | Reward: 4.80 | Avg loss: 0.0840 | Epsilon: 0.010\n",
            "Episode 875/1000 | Reward: 1.90 | Avg loss: 0.1210 | Epsilon: 0.010\n",
            "Episode 876/1000 | Reward: 0.30 | Avg loss: 0.1147 | Epsilon: 0.010\n",
            "Episode 877/1000 | Reward: 1.50 | Avg loss: 0.1230 | Epsilon: 0.010\n",
            "Episode 878/1000 | Reward: 0.90 | Avg loss: 0.1113 | Epsilon: 0.010\n",
            "Episode 879/1000 | Reward: 5.00 | Avg loss: 0.1486 | Epsilon: 0.010\n",
            "Episode 880/1000 | Reward: -0.60 | Avg loss: 0.1468 | Epsilon: 0.010\n",
            "Episode 881/1000 | Reward: 10.40 | Avg loss: 0.1316 | Epsilon: 0.010\n",
            "Episode 882/1000 | Reward: 2.10 | Avg loss: 0.1468 | Epsilon: 0.010\n",
            "Episode 883/1000 | Reward: 4.30 | Avg loss: 0.1199 | Epsilon: 0.010\n",
            "Episode 884/1000 | Reward: 8.10 | Avg loss: 0.1473 | Epsilon: 0.010\n",
            "Episode 885/1000 | Reward: -0.90 | Avg loss: 0.1192 | Epsilon: 0.010\n",
            "Episode 886/1000 | Reward: 3.00 | Avg loss: 0.1130 | Epsilon: 0.010\n",
            "Episode 887/1000 | Reward: 5.50 | Avg loss: 0.1004 | Epsilon: 0.010\n",
            "Episode 888/1000 | Reward: -0.90 | Avg loss: 0.1123 | Epsilon: 0.010\n",
            "Episode 889/1000 | Reward: 10.70 | Avg loss: 0.0985 | Epsilon: 0.010\n",
            "Episode 890/1000 | Reward: -0.90 | Avg loss: 0.0977 | Epsilon: 0.010\n",
            "Episode 891/1000 | Reward: 3.20 | Avg loss: 0.0889 | Epsilon: 0.010\n",
            "Episode 892/1000 | Reward: 2.10 | Avg loss: 0.1007 | Epsilon: 0.010\n",
            "Episode 893/1000 | Reward: 1.90 | Avg loss: 0.0957 | Epsilon: 0.010\n",
            "Episode 894/1000 | Reward: -3.90 | Avg loss: 0.1091 | Epsilon: 0.010\n",
            "Episode 895/1000 | Reward: 0.30 | Avg loss: 0.0942 | Epsilon: 0.010\n",
            "Episode 896/1000 | Reward: 3.00 | Avg loss: 0.1036 | Epsilon: 0.010\n",
            "Episode 897/1000 | Reward: -0.90 | Avg loss: 0.1167 | Epsilon: 0.010\n",
            "Episode 898/1000 | Reward: 1.70 | Avg loss: 0.1360 | Epsilon: 0.010\n",
            "Episode 899/1000 | Reward: 3.10 | Avg loss: 0.1198 | Epsilon: 0.010\n",
            "Episode 900/1000 | Reward: 2.00 | Avg loss: 0.0804 | Epsilon: 0.010\n",
            "Episode 901/1000 | Reward: -1.50 | Avg loss: 0.0849 | Epsilon: 0.010\n",
            "Episode 902/1000 | Reward: 1.10 | Avg loss: 0.0989 | Epsilon: 0.010\n",
            "Episode 903/1000 | Reward: 2.30 | Avg loss: 0.0906 | Epsilon: 0.010\n",
            "Episode 904/1000 | Reward: 2.80 | Avg loss: 0.0938 | Epsilon: 0.010\n",
            "Episode 905/1000 | Reward: 2.90 | Avg loss: 0.1027 | Epsilon: 0.010\n",
            "Episode 906/1000 | Reward: 1.80 | Avg loss: 0.1060 | Epsilon: 0.010\n",
            "Episode 907/1000 | Reward: -0.90 | Avg loss: 0.1036 | Epsilon: 0.010\n",
            "Episode 908/1000 | Reward: 0.90 | Avg loss: 0.1128 | Epsilon: 0.010\n",
            "Episode 909/1000 | Reward: 5.50 | Avg loss: 0.0992 | Epsilon: 0.010\n",
            "Episode 910/1000 | Reward: 3.10 | Avg loss: 0.0983 | Epsilon: 0.010\n",
            "Episode 911/1000 | Reward: 2.40 | Avg loss: 0.0895 | Epsilon: 0.010\n",
            "Episode 912/1000 | Reward: 1.10 | Avg loss: 0.0985 | Epsilon: 0.010\n",
            "Episode 913/1000 | Reward: -1.20 | Avg loss: 0.1036 | Epsilon: 0.010\n",
            "Episode 914/1000 | Reward: 5.10 | Avg loss: 0.0888 | Epsilon: 0.010\n",
            "Episode 915/1000 | Reward: 2.30 | Avg loss: 0.1092 | Epsilon: 0.010\n",
            "Episode 916/1000 | Reward: 1.60 | Avg loss: 0.1088 | Epsilon: 0.010\n",
            "Episode 917/1000 | Reward: 6.50 | Avg loss: 0.0776 | Epsilon: 0.010\n",
            "Episode 918/1000 | Reward: -0.10 | Avg loss: 0.0907 | Epsilon: 0.010\n",
            "Episode 919/1000 | Reward: 5.10 | Avg loss: 0.0876 | Epsilon: 0.010\n",
            "Episode 920/1000 | Reward: 0.80 | Avg loss: 0.0898 | Epsilon: 0.010\n",
            "Episode 921/1000 | Reward: -0.30 | Avg loss: 0.0944 | Epsilon: 0.010\n",
            "Episode 922/1000 | Reward: 8.40 | Avg loss: 0.1014 | Epsilon: 0.010\n",
            "Episode 923/1000 | Reward: 6.30 | Avg loss: 0.1054 | Epsilon: 0.010\n",
            "Episode 924/1000 | Reward: -1.50 | Avg loss: 0.1022 | Epsilon: 0.010\n",
            "Episode 925/1000 | Reward: -1.80 | Avg loss: 0.1172 | Epsilon: 0.010\n",
            "Episode 926/1000 | Reward: 2.10 | Avg loss: 0.1053 | Epsilon: 0.010\n",
            "Episode 927/1000 | Reward: 2.90 | Avg loss: 0.1109 | Epsilon: 0.010\n",
            "Episode 928/1000 | Reward: 4.50 | Avg loss: 0.1054 | Epsilon: 0.010\n",
            "Episode 929/1000 | Reward: 2.80 | Avg loss: 0.0881 | Epsilon: 0.010\n",
            "Episode 930/1000 | Reward: 1.90 | Avg loss: 0.1200 | Epsilon: 0.010\n",
            "Episode 931/1000 | Reward: 1.90 | Avg loss: 0.1213 | Epsilon: 0.010\n",
            "Episode 932/1000 | Reward: 2.60 | Avg loss: 0.1145 | Epsilon: 0.010\n",
            "Episode 933/1000 | Reward: -0.90 | Avg loss: 0.0857 | Epsilon: 0.010\n",
            "Episode 934/1000 | Reward: -0.90 | Avg loss: 0.1006 | Epsilon: 0.010\n",
            "Episode 935/1000 | Reward: 1.50 | Avg loss: 0.1243 | Epsilon: 0.010\n",
            "Episode 936/1000 | Reward: -0.90 | Avg loss: 0.1094 | Epsilon: 0.010\n",
            "Episode 937/1000 | Reward: 2.50 | Avg loss: 0.1252 | Epsilon: 0.010\n",
            "Episode 938/1000 | Reward: -1.10 | Avg loss: 0.1131 | Epsilon: 0.010\n",
            "Episode 939/1000 | Reward: 0.90 | Avg loss: 0.1104 | Epsilon: 0.010\n",
            "Episode 940/1000 | Reward: 0.20 | Avg loss: 0.1049 | Epsilon: 0.010\n",
            "Episode 941/1000 | Reward: -6.30 | Avg loss: 0.1155 | Epsilon: 0.010\n",
            "Episode 942/1000 | Reward: -6.30 | Avg loss: 0.1144 | Epsilon: 0.010\n",
            "Episode 943/1000 | Reward: 0.90 | Avg loss: 0.1291 | Epsilon: 0.010\n",
            "Episode 944/1000 | Reward: 2.70 | Avg loss: 0.1109 | Epsilon: 0.010\n",
            "Episode 945/1000 | Reward: -0.30 | Avg loss: 0.1193 | Epsilon: 0.010\n",
            "Episode 946/1000 | Reward: 2.30 | Avg loss: 0.1322 | Epsilon: 0.010\n",
            "Episode 947/1000 | Reward: 2.10 | Avg loss: 0.1253 | Epsilon: 0.010\n",
            "Episode 948/1000 | Reward: -8.10 | Avg loss: 0.1688 | Epsilon: 0.010\n",
            "Episode 949/1000 | Reward: 2.40 | Avg loss: 0.1545 | Epsilon: 0.010\n",
            "Episode 950/1000 | Reward: 3.60 | Avg loss: 0.1482 | Epsilon: 0.010\n",
            "Episode 951/1000 | Reward: 5.30 | Avg loss: 0.1287 | Epsilon: 0.010\n",
            "Episode 952/1000 | Reward: 1.60 | Avg loss: 0.1395 | Epsilon: 0.010\n",
            "Episode 953/1000 | Reward: 1.80 | Avg loss: 0.1412 | Epsilon: 0.010\n",
            "Episode 954/1000 | Reward: -6.90 | Avg loss: 0.1264 | Epsilon: 0.010\n",
            "Episode 955/1000 | Reward: -1.40 | Avg loss: 0.1344 | Epsilon: 0.010\n",
            "Episode 956/1000 | Reward: 0.90 | Avg loss: 0.1331 | Epsilon: 0.010\n",
            "Episode 957/1000 | Reward: 2.90 | Avg loss: 0.1339 | Epsilon: 0.010\n",
            "Episode 958/1000 | Reward: 5.00 | Avg loss: 0.1339 | Epsilon: 0.010\n",
            "Episode 959/1000 | Reward: 4.20 | Avg loss: 0.1581 | Epsilon: 0.010\n",
            "Episode 960/1000 | Reward: -0.30 | Avg loss: 0.1471 | Epsilon: 0.010\n",
            "Episode 961/1000 | Reward: -0.90 | Avg loss: 0.1180 | Epsilon: 0.010\n",
            "Episode 962/1000 | Reward: 6.20 | Avg loss: 0.1390 | Epsilon: 0.010\n",
            "Episode 963/1000 | Reward: 5.30 | Avg loss: 0.1270 | Epsilon: 0.010\n",
            "Episode 964/1000 | Reward: 2.50 | Avg loss: 0.1365 | Epsilon: 0.010\n",
            "Episode 965/1000 | Reward: 4.80 | Avg loss: 0.1205 | Epsilon: 0.010\n",
            "Episode 966/1000 | Reward: 1.90 | Avg loss: 0.1261 | Epsilon: 0.010\n",
            "Episode 967/1000 | Reward: -0.80 | Avg loss: 0.1160 | Epsilon: 0.010\n",
            "Episode 968/1000 | Reward: -0.30 | Avg loss: 0.1120 | Epsilon: 0.010\n",
            "Episode 969/1000 | Reward: 0.90 | Avg loss: 0.0956 | Epsilon: 0.010\n",
            "Episode 970/1000 | Reward: 9.70 | Avg loss: 0.1077 | Epsilon: 0.010\n",
            "Episode 971/1000 | Reward: 2.50 | Avg loss: 0.1223 | Epsilon: 0.010\n",
            "Episode 972/1000 | Reward: 8.40 | Avg loss: 0.1304 | Epsilon: 0.010\n",
            "Episode 973/1000 | Reward: 12.60 | Avg loss: 0.1030 | Epsilon: 0.010\n",
            "Episode 974/1000 | Reward: 1.30 | Avg loss: 0.1127 | Epsilon: 0.010\n",
            "Episode 975/1000 | Reward: 2.80 | Avg loss: 0.0933 | Epsilon: 0.010\n",
            "Episode 976/1000 | Reward: 4.30 | Avg loss: 0.1045 | Epsilon: 0.010\n",
            "Episode 977/1000 | Reward: -1.20 | Avg loss: 0.0949 | Epsilon: 0.010\n",
            "Episode 978/1000 | Reward: 1.20 | Avg loss: 0.0932 | Epsilon: 0.010\n",
            "Episode 979/1000 | Reward: 1.50 | Avg loss: 0.0870 | Epsilon: 0.010\n",
            "Episode 980/1000 | Reward: 2.60 | Avg loss: 0.0973 | Epsilon: 0.010\n",
            "Episode 981/1000 | Reward: 2.30 | Avg loss: 0.0958 | Epsilon: 0.010\n",
            "Episode 982/1000 | Reward: -0.20 | Avg loss: 0.1093 | Epsilon: 0.010\n",
            "Episode 983/1000 | Reward: -0.70 | Avg loss: 0.1198 | Epsilon: 0.010\n",
            "Episode 984/1000 | Reward: 1.50 | Avg loss: 0.1388 | Epsilon: 0.010\n",
            "Episode 985/1000 | Reward: -1.70 | Avg loss: 0.1471 | Epsilon: 0.010\n",
            "Episode 986/1000 | Reward: 1.50 | Avg loss: 0.1353 | Epsilon: 0.010\n",
            "Episode 987/1000 | Reward: 8.80 | Avg loss: 0.1647 | Epsilon: 0.010\n",
            "Episode 988/1000 | Reward: 2.30 | Avg loss: 0.1475 | Epsilon: 0.010\n",
            "Episode 989/1000 | Reward: 1.70 | Avg loss: 0.1491 | Epsilon: 0.010\n",
            "Episode 990/1000 | Reward: -0.90 | Avg loss: 0.1103 | Epsilon: 0.010\n",
            "Episode 991/1000 | Reward: 15.10 | Avg loss: 0.1056 | Epsilon: 0.010\n",
            "Episode 992/1000 | Reward: 2.50 | Avg loss: 0.1028 | Epsilon: 0.010\n",
            "Episode 993/1000 | Reward: -0.30 | Avg loss: 0.1334 | Epsilon: 0.010\n",
            "Episode 994/1000 | Reward: 0.30 | Avg loss: 0.1208 | Epsilon: 0.010\n",
            "Episode 995/1000 | Reward: 8.90 | Avg loss: 0.1252 | Epsilon: 0.010\n",
            "Episode 996/1000 | Reward: 1.50 | Avg loss: 0.1145 | Epsilon: 0.010\n",
            "Episode 997/1000 | Reward: 0.60 | Avg loss: 0.1117 | Epsilon: 0.010\n",
            "Episode 998/1000 | Reward: -0.90 | Avg loss: 0.1087 | Epsilon: 0.010\n",
            "Episode 999/1000 | Reward: 3.10 | Avg loss: 0.1096 | Epsilon: 0.010\n",
            "Episode 1000/1000 | Reward: 7.70 | Avg loss: 0.1283 | Epsilon: 0.010\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 7: TRAINING LOOP\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "def train_dqn(agent, env, num_episodes):\n",
        "    \"\"\"\n",
        "    Main training loop.\n",
        "\n",
        "    Args:\n",
        "        agent: DQNAgent instance\n",
        "        env: Gymnasium environment\n",
        "        num_episodes: Number of episodes to train\n",
        "\n",
        "    Returns:\n",
        "        rewards: List of episode rewards\n",
        "        losses: List of training losses (avg per episode)\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    losses = []\n",
        "\n",
        "    epsilon = EPSILON_START\n",
        "    global_step = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0.0\n",
        "        episode_losses = []\n",
        "\n",
        "        while not done:\n",
        "            # --- Epsilon-greedy action selection ---\n",
        "            action = agent.select_action(state, epsilon)\n",
        "\n",
        "            # --- Interact with environment ---\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # --- Store experience in replay buffer ---\n",
        "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            # --- Train on a random batch from replay buffer ---\n",
        "            loss = agent.train_step()\n",
        "            if loss is not None:\n",
        "                episode_losses.append(loss)\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "            # --- Periodically update target network ---\n",
        "            if global_step % TARGET_UPDATE == 0:\n",
        "                agent.update_target_network()\n",
        "\n",
        "            # --- Decay epsilon (exploration) ---\n",
        "            epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
        "\n",
        "        # end of episode\n",
        "        rewards.append(episode_reward)\n",
        "        if episode_losses:\n",
        "            avg_loss = np.mean(episode_losses)\n",
        "        else:\n",
        "            avg_loss = 0.0\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode + 1}/{num_episodes} | \"\n",
        "            f\"Reward: {episode_reward:.2f} | \"\n",
        "            f\"Avg loss: {avg_loss:.4f} | \"\n",
        "            f\"Epsilon: {epsilon:.3f}\"\n",
        "        )\n",
        "\n",
        "    return rewards, losses\n",
        "\n",
        "#test\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "env = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "agent = DQNAgent(state_dim, action_dim, device=device)\n",
        "\n",
        "rewards, losses = train_dqn(agent, env, NUM_EPISODES)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 8: HELPER FUNCTIONS\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "def plot_training_results(rewards, losses):\n",
        "    \"\"\"Plot reward curves and training losses.\"\"\"\n",
        "    # TODO: Implement plotting\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(rewards)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.title('Training Rewards')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Losses')\n",
        "    plt.show()\n",
        "\n",
        "def save_model(agent, filepath):\n",
        "    \"\"\"Save model weights (agent's primary Q-Network: policy_net).\"\"\"\n",
        "    torch.save(agent.policy_net.state_dict(), filepath)\n",
        "    print(f\"✅ Model saved to {filepath}\")\n",
        "\n",
        "def load_model(agent, filepath):\n",
        "    \"\"\"Load model weights into agent's primary Q-Network: policy_net.\"\"\"\n",
        "    state_dict = torch.load(filepath, map_location=agent.device)\n",
        "    agent.policy_net.load_state_dict(state_dict)\n",
        "    agent.policy_net.eval() # Set network to evaluation mode after loading\n",
        "    print(f\"✅ Model loaded from {filepath}\")\n",
        "    return agent\n",
        "\n",
        "def record_video(env, agent, filename):\n",
        "    \"\"\"Record agent gameplay video.\"\"\"\n",
        "    # Set a seed for consistent visualization\n",
        "    SEED = 42\n",
        "\n",
        "    # 1. Wrap the environment for recording.\n",
        "    video_env = RecordVideo(env, filename, episode_trigger=lambda x: True, name_prefix='eval_run')\n",
        "\n",
        "    # 2. Run an episode\n",
        "    state, _ = video_env.reset(seed=SEED)\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    step = 0\n",
        "\n",
        "    while not done:\n",
        "        # *** FIX: Using agent.select_action and epsilon=0.0 ***\n",
        "        # Action selection uses the trained agent's greedy policy\n",
        "        action = agent.select_action(state, epsilon=0.0)\n",
        "\n",
        "        next_state, reward, terminated, truncated, _ = video_env.step(action)\n",
        "        done = terminated or truncated\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        step += 1\n",
        "\n",
        "    video_env.close()\n",
        "    print(f\"✅ Video recorded in the '{filename}' directory. Total Reward: {total_reward:.2f}, Steps: {step}\")\n"
      ],
      "metadata": {
        "id": "e2waCE8JgvU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 9.5: VIDEO\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "LIDAR_OBS = True\n",
        "ENV_ID = 'FlappyBird-v0'\n",
        "\n",
        "MODEL_PATH = 'flappy_bird_dqn_v1.pth' # Must match the save path\n",
        "VIDEO_FOLDER = './dqn_v1_performance_video'\n",
        "\n",
        "# 1. Instantiate a new evaluation agent\n",
        "eval_agent = DQNAgent(state_dim, action_dim, device=device)\n",
        "\n",
        "# 2. Load the model\n",
        "eval_agent = load_model(eval_agent, MODEL_PATH)\n",
        "\n",
        "# 3. Create an evaluation environment (must be 'rgb_array' for video)\n",
        "eval_env = gym.make(\n",
        "    ENV_ID,\n",
        "    render_mode='rgb_array',\n",
        "    use_lidar=LIDAR_OBS\n",
        ")\n",
        "\n",
        "# 4. Record the video\n",
        "record_video(eval_env, eval_agent, VIDEO_FOLDER)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlUQ5PBJ7nEi",
        "outputId": "a08e21bc-eb0b-47ce-9128-31cce1ab801b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/dqn_v1_performance_video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded from flappy_bird_dqn_v1.pth\n",
            "✅ Video recorded in the './dqn_v1_performance_video' directory. Total Reward: -0.90, Steps: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 9: EVALUATION\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "def evaluate_agent(agent, env, num_episodes=10):\n",
        "    \"\"\"\n",
        "    Evaluate agent performance.\n",
        "\n",
        "    Returns:\n",
        "        avg_reward: Average reward\n",
        "        avg_steps: Average survival time\n",
        "        scores: List of scores\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Run agent without exploration\n",
        "    # TODO: Collect metrics\n",
        "    pass"
      ],
      "metadata": {
        "id": "nB9N0Bnng3D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 10: EXPERIMENTS - VERSION 1 (BASIC DQN)\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "# TODO: Train basic DQN\n",
        "# TODO: Evaluate and save results\n",
        "# TODO: Record video"
      ],
      "metadata": {
        "id": "aa-a2kIDbUro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 11: EXPERIMENTS - VERSION 2 (IMPROVEMENTS)\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "# TODO: Implement improvement (e.g., Double DQN)\n",
        "# TODO: Train and compare to V1"
      ],
      "metadata": {
        "id": "dQGk4nTEbVZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 12: EXPERIMENTS - VERSION 3 (MORE IMPROVEMENTS)\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "# TODO: Implement another improvement\n",
        "# TODO: Train and compare"
      ],
      "metadata": {
        "id": "nmI5Fo53bVdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "# SECTION 13: RESULTS & ANALYSIS\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "# TODO: Compare all versions\n",
        "# TODO: Create comparison graphs\n",
        "# TODO: Side-by-side videos"
      ],
      "metadata": {
        "id": "vgoAOT8CbVfH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}